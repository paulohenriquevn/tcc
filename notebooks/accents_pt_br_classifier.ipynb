{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accents-PT-BR — Accent Classifier Ablation (CNN vs wav2vec2)\n",
    "\n",
    "**Projeto:** Controle Explícito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Treinar e avaliar classificadores de sotaque (CNN mel-spectrogram vs wav2vec2 fine-tuned) no dataset combinado Accents-PT-BR (CORAA-MUPE + Common Voice PT). Esses classificadores servem como **avaliadores externos** para os Stages 2-3 (medir se o áudio gerado pelo LoRA carrega o sotaque-alvo).  \n",
    "**Config:** `configs/accent_classifier.yaml` (single source of truth).  \n",
    "**Dataset:** Accents-PT-BR = CORAA-MUPE (entrevistados) + Common Voice PT (accent label normalizado).  \n",
    "\n",
    "**Seções:**\n",
    "1. Setup do ambiente\n",
    "2. CORAA-MUPE manifest\n",
    "3. Common Voice PT manifest\n",
    "4. Dataset combinado Accents-PT-BR\n",
    "5. Análise de confounds (incluindo accent x source)\n",
    "6. Speaker-disjoint splits\n",
    "7. CNN accent classifier (treinamento + avaliação)\n",
    "8. wav2vec2 accent classifier (treinamento + avaliação)\n",
    "9. Cross-source evaluation (confound check)\n",
    "10. Ablation summary + report\n",
    "\n",
    "Este notebook é a **camada de orquestração**. Toda lógica está em `src/` (testável, auditável).  \n",
    "O notebook apenas: instala deps → configura ambiente → chama módulos → exibe resultados."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import os, subprocess, sys\n\n# --- Platform-aware setup: works on Colab, Lightning.ai, and local ---\n# Detection order: Lightning.ai -> Google Colab -> Local\n\n# 1. Determine repo directory\n_lightning_studio = '/teamspace/studios/this_studio'\nif os.path.exists(_lightning_studio):\n    REPO_DIR = os.path.join(_lightning_studio, 'TCC')\n    _platform = 'lightning'\nelif 'google.colab' in sys.modules or os.path.exists('/content'):\n    REPO_DIR = '/content/TCC'\n    _platform = 'colab'\nelse:\n    REPO_DIR = os.getcwd()\n    _platform = 'local'\n\n# 2. Clone repo if needed (idempotent)\nif not os.path.exists(os.path.join(REPO_DIR, '.git')):\n    subprocess.run(['rm', '-rf', REPO_DIR], check=False)\n    subprocess.run(\n        ['git', 'clone', 'https://github.com/paulohenriquevn/tcc.git', REPO_DIR],\n        check=True,\n    )\n\nos.chdir(REPO_DIR)\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\n\n# 3. Install dependencies\nsubprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt', '-q'], check=True)\n\n# 4. NumPy ABI check — Colab pre-loads numpy 2.x in memory, but\n#    requirements.txt pins 1.26.4. After pip downgrades, stale C-extensions\n#    cause binary incompatibility. Fix: restart runtime ONCE.\n_installed_np = subprocess.check_output(\n    [sys.executable, '-c', 'import numpy; print(numpy.__version__)'],\n    text=True,\n).strip()\n\ntry:\n    import numpy as _np\n    _loaded_np = _np.__version__\nexcept Exception:\n    _loaded_np = None\n\nif _loaded_np != _installed_np:\n    print(f'\\nNumPy ABI mismatch: loaded={_loaded_np}, installed={_installed_np}')\n    print('Restarting runtime... After restart, re-run this cell (no second restart).')\n    os.kill(os.getpid(), 9)\nelse:\n    print(f'\\nPlatform: {_platform}')\n    print(f'Repo: {REPO_DIR}')\n    print(f'Environment OK (numpy=={_installed_np})')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys, os, yaml, json, logging\nfrom pathlib import Path\nfrom collections import Counter\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Platform-aware persistent cache setup\nfrom src.utils.platform import detect_platform, setup_environment\n\nplatform = detect_platform()\nsetup_environment(platform)\n\n# Mount Google Drive only on Colab (Lightning.ai has persistent disk built-in)\nif platform.needs_drive_mount:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\nfrom src.utils.seed import set_global_seed\nfrom src.data.manifest import (\n    ManifestEntry, read_manifest, write_manifest,\n    normalize_cv_accent, compute_file_hash,\n)\nfrom src.data.manifest_builder import build_manifest_from_hf_dataset\nfrom src.data.cv_manifest_builder import build_manifest_from_common_voice\nfrom src.data.combined_manifest import combine_manifests, analyze_source_distribution\nfrom src.data.splits import (\n    generate_speaker_disjoint_splits,\n    save_splits,\n    assign_entries_to_splits,\n)\nfrom src.analysis.confounds import run_all_confound_checks\nfrom src.classifier import (\n    AccentCNN, AccentWav2Vec2,\n    train_classifier, evaluate_classifier,\n    TrainingConfig, TrainingResult,\n)\nfrom src.classifier.mel_dataset import MelSpectrogramDataset\nfrom src.classifier.wav2vec2_dataset import WaveformDataset\nfrom src.classifier.trainer import compute_class_weights\n\n# Load config — single source of truth for all experiment parameters\nwith open('configs/accent_classifier.yaml') as f:\n    config = yaml.safe_load(f)\n\nSEED = config['seed']['global']\ngenerator = set_global_seed(SEED)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(name)s - %(levelname)s - %(message)s',\n)\n\nprint(f'Platform: {platform.name}')\nprint(f'Config loaded: {config[\"experiment\"][\"name\"]}')\nprint(f'Seed global: {SEED}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Environment check: GPU, CUDA, PyTorch versions\nprint(f'Python: {sys.version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n    print(f'CUDA version: {torch.version.cuda}')\n    print(f'VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'\\nUsando device: {DEVICE}')\n\n# Drive cache base directory — platform-aware\nDRIVE_BASE = platform.cache_base\nDRIVE_BASE.mkdir(parents=True, exist_ok=True)\nprint(f'Cache base: {DRIVE_BASE}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORAA-MUPE Manifest\n",
    "\n",
    "Download CORAA-MUPE-ASR from HuggingFace, apply filters, and build the manifest JSONL.  \n",
    "\n",
    "**Filters:**  \n",
    "- `speaker_type='R'` (interviewees only)  \n",
    "- Duration: 3–15s  \n",
    "- `birth_state` valido → macro-regiao IBGE (N, NE, CO, SE, S)  \n",
    "- `min_speakers_per_region`: 8  \n",
    "- `min_utterances_per_speaker`: 3  \n",
    "\n",
    "**Nota:** O download inicial e ~42 GB. Runs subsequentes usam cache do Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Paths for CORAA-MUPE manifest and audio\n",
    "CORAA_AUDIO_DIR = DRIVE_BASE / 'coraa_mupe' / 'audio'\n",
    "CORAA_MANIFEST_PATH = DRIVE_BASE / 'coraa_mupe' / 'manifest.jsonl'\n",
    "\n",
    "if CORAA_MANIFEST_PATH.exists():\n",
    "    print(f'Loading CORAA-MUPE manifest from cache: {CORAA_MANIFEST_PATH}')\n",
    "    coraa_entries = read_manifest(CORAA_MANIFEST_PATH)\n",
    "    coraa_sha256 = compute_file_hash(CORAA_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(coraa_entries):,} entries (SHA-256: {coraa_sha256[:16]}...)')\n",
    "else:\n",
    "    print('Downloading CORAA-MUPE-ASR from HuggingFace...')\n",
    "    print('(~42 GB na primeira vez)')\n",
    "\n",
    "    ds = load_dataset('nilc-nlp/CORAA-MUPE-ASR')\n",
    "    print(f'Splits disponiveis: {list(ds.keys())}')\n",
    "    for split_name, split_data in ds.items():\n",
    "        print(f'  {split_name}: {len(split_data):,} rows')\n",
    "\n",
    "    # Concatenate all splits — we create our own speaker-disjoint splits\n",
    "    all_data = concatenate_datasets([ds[split] for split in ds.keys()])\n",
    "    print(f'\\nTotal concatenado: {len(all_data):,} rows')\n",
    "\n",
    "    coraa_entries, coraa_stats = build_manifest_from_hf_dataset(\n",
    "        dataset=all_data,\n",
    "        audio_output_dir=CORAA_AUDIO_DIR,\n",
    "        manifest_output_path=CORAA_MANIFEST_PATH,\n",
    "        speaker_type_filter=config['dataset']['filters'].get('speaker_type', 'R'),\n",
    "        min_duration_s=config['dataset']['filters']['min_duration_s'],\n",
    "        max_duration_s=config['dataset']['filters']['max_duration_s'],\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    coraa_sha256 = coraa_stats['manifest_sha256']\n",
    "\n",
    "    print(f'\\nCORAA-MUPE manifest: {len(coraa_entries):,} entries')\n",
    "    print(f'SHA-256: {coraa_sha256}')\n",
    "    for region, info in coraa_stats.get('regions', {}).items():\n",
    "        print(f'  {region}: {info[\"n_speakers\"]} speakers, {info[\"n_utterances\"]:,} utts')\n",
    "\n",
    "# Summary\n",
    "region_counts = Counter(e.accent for e in coraa_entries)\n",
    "print(f'\\nCORAA-MUPE: {len(coraa_entries):,} entries, regions: {dict(sorted(region_counts.items()))}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Voice PT Manifest\n",
    "\n",
    "Load Common Voice Portuguese (v17.0) from HuggingFace and build manifest.  \n",
    "Speaker IDs are prefixed with `cv_` to avoid collisions with CORAA-MUPE.  \n",
    "The `accent` field in Common Voice is user-submitted and noisy — `normalize_cv_accent()` handles the mapping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Paths for Common Voice manifest and audio\n",
    "CV_AUDIO_DIR = DRIVE_BASE / 'common_voice_pt' / 'audio'\n",
    "CV_MANIFEST_PATH = DRIVE_BASE / 'common_voice_pt' / 'manifest.jsonl'\n",
    "\n",
    "if CV_MANIFEST_PATH.exists():\n",
    "    print(f'Loading Common Voice PT manifest from cache: {CV_MANIFEST_PATH}')\n",
    "    cv_entries = read_manifest(CV_MANIFEST_PATH)\n",
    "    cv_sha256 = compute_file_hash(CV_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(cv_entries):,} entries (SHA-256: {cv_sha256[:16]}...)')\n",
    "else:\n",
    "    print('Loading Common Voice PT from HuggingFace...')\n",
    "    print('(mozilla-foundation/common_voice_17_0, lang=pt)')\n",
    "\n",
    "    cv_hf_id = config['dataset']['sources'][1]['hf_id']\n",
    "    cv_lang = config['dataset']['sources'][1]['hf_lang']\n",
    "\n",
    "    # Load the validated split (most reliable labels)\n",
    "    cv_dataset = load_dataset(cv_hf_id, cv_lang, split='validated')\n",
    "    print(f'Common Voice validated split: {len(cv_dataset):,} rows')\n",
    "    print(f'Columns: {cv_dataset.column_names}')\n",
    "\n",
    "    cv_entries, cv_stats = build_manifest_from_common_voice(\n",
    "        dataset=cv_dataset,\n",
    "        audio_output_dir=CV_AUDIO_DIR,\n",
    "        manifest_output_path=CV_MANIFEST_PATH,\n",
    "        min_duration_s=config['dataset']['filters']['min_duration_s'],\n",
    "        max_duration_s=config['dataset']['filters']['max_duration_s'],\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    cv_sha256 = cv_stats['manifest_sha256']\n",
    "\n",
    "    print(f'\\nCommon Voice PT manifest: {len(cv_entries):,} entries')\n",
    "    print(f'SHA-256: {cv_sha256}')\n",
    "    for region, info in cv_stats.get('regions', {}).items():\n",
    "        print(f'  {region}: {info[\"n_speakers\"]} speakers, {info[\"n_utterances\"]:,} utts')\n",
    "\n",
    "# Summary\n",
    "region_counts_cv = Counter(e.accent for e in cv_entries)\n",
    "print(f'\\nCommon Voice PT: {len(cv_entries):,} entries, regions: {dict(sorted(region_counts_cv.items()))}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Accents-PT-BR Dataset\n",
    "\n",
    "Merge CORAA-MUPE and Common Voice manifests into a single dataset.  \n",
    "Validates: no utt_id or speaker_id collisions across sources, speaker-accent consistency."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "COMBINED_MANIFEST_PATH = DRIVE_BASE / 'accents_pt_br' / 'manifest.jsonl'\n",
    "\n",
    "if COMBINED_MANIFEST_PATH.exists():\n",
    "    print(f'Loading combined manifest from cache: {COMBINED_MANIFEST_PATH}')\n",
    "    combined_entries = read_manifest(COMBINED_MANIFEST_PATH)\n",
    "    combined_sha256 = compute_file_hash(COMBINED_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(combined_entries):,} entries (SHA-256: {combined_sha256[:16]}...)')\n",
    "else:\n",
    "    combined_entries, combined_stats = combine_manifests(\n",
    "        manifests=[\n",
    "            (CORAA_MANIFEST_PATH, 'CORAA-MUPE'),\n",
    "            (CV_MANIFEST_PATH, 'CommonVoice-PT'),\n",
    "        ],\n",
    "        output_path=COMBINED_MANIFEST_PATH,\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    combined_sha256 = combined_stats['manifest_sha256']\n",
    "\n",
    "    print(f'Combined manifest: {len(combined_entries):,} entries')\n",
    "    print(f'SHA-256: {combined_sha256}')\n",
    "    print(f'\\nPer-source (input): {combined_stats[\"per_source_input\"]}')\n",
    "    print(f'Per-source (output): {combined_stats[\"per_source_output\"]}')\n",
    "    for region, info in combined_stats.get('regions', {}).items():\n",
    "        print(f'  {region}: {info[\"n_speakers\"]} speakers, {info[\"n_utterances\"]:,} utts')\n",
    "\n",
    "# Source distribution analysis\n",
    "source_dist = analyze_source_distribution(combined_entries)\n",
    "\n",
    "print(f'\\n=== SOURCE DISTRIBUTION ===')\n",
    "print(f'Source x Accent:')\n",
    "for src, counts in source_dist['source_x_accent'].items():\n",
    "    print(f'  {src}: {dict(sorted(counts.items()))}')\n",
    "\n",
    "print(f'\\nSource x Gender:')\n",
    "for src, counts in source_dist['source_x_gender'].items():\n",
    "    print(f'  {src}: {dict(sorted(counts.items()))}')\n",
    "\n",
    "if source_dist['warnings']:\n",
    "    print(f'\\nWARNINGS:')\n",
    "    for w in source_dist['warnings']:\n",
    "        print(f'  {w}')\n",
    "\n",
    "# Overall summary\n",
    "total_speakers = len({e.speaker_id for e in combined_entries})\n",
    "region_counts_all = Counter(e.accent for e in combined_entries)\n",
    "print(f'\\nTotal: {len(combined_entries):,} entries, {total_speakers} speakers')\n",
    "print(f'Regions: {dict(sorted(region_counts_all.items()))}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confound Analysis (including accent x source)\n",
    "\n",
    "**Sanity checks obrigatorios** (protocolo):  \n",
    "- Tabela accent x gender com chi-quadrado + Cramer's V  \n",
    "- Histograma de duracao por regiao + Kruskal-Wallis  \n",
    "- Accent x source: se uma regiao vem 80%+ de uma fonte, o classificador pode aprender fonte ao inves de sotaque  \n",
    "\n",
    "SNR check desabilitado para velocidade (`check_snr=False`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "confound_results = run_all_confound_checks(\n",
    "    combined_entries,\n",
    "    gender_blocking_threshold=config['confounds']['accent_x_gender']['threshold_blocker'],\n",
    "    duration_practical_diff_s=config['confounds']['accent_x_duration']['practical_diff_s'],\n",
    "    check_snr=False,  # Skip SNR for speed\n",
    "    source_blocking_threshold=config['confounds']['accent_x_source']['threshold_blocker'],\n",
    ")\n",
    "\n",
    "print('=== CONFOUND ANALYSIS ===')\n",
    "blocking_found = False\n",
    "for result in confound_results:\n",
    "    if result.is_blocking:\n",
    "        status = 'BLOCKING'\n",
    "        blocking_found = True\n",
    "    elif result.is_significant:\n",
    "        status = 'SIGNIFICANT'\n",
    "    else:\n",
    "        status = 'OK'\n",
    "\n",
    "    print(f'\\n{result.variable_a} x {result.variable_b}: {status}')\n",
    "    print(f'  Test: {result.test_name}')\n",
    "    print(f'  Statistic: {result.statistic:.4f}')\n",
    "    print(f'  p-value: {result.p_value:.6f}')\n",
    "    print(f'  Effect size ({result.effect_size_name}): {result.effect_size:.4f}')\n",
    "    print(f'  Interpretation: {result.interpretation}')\n",
    "\n",
    "# Cross-tabulation: accent x gender\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.gender for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x GENDER TABLE ===')\n",
    "print(gender_table)\n",
    "\n",
    "# Cross-tabulation: accent x source\n",
    "source_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.source for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x SOURCE TABLE ===')\n",
    "print(source_table)\n",
    "\n",
    "if blocking_found:\n",
    "    print('\\n*** BLOCKING CONFOUND DETECTED. Review before proceeding. ***')\n",
    "else:\n",
    "    print('\\nNo blocking confounds detected. Proceeding.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker-Disjoint Splits\n",
    "\n",
    "**Obrigatorio:** nenhum speaker aparece em mais de um split (train/val/test).  \n",
    "Splits sao estratificados por sotaque para garantir representacao em todos os splits.  \n",
    "Splits sao artefatos versionados — nao mudam dentro de um experimento."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "split_info = generate_speaker_disjoint_splits(\n",
    "    combined_entries,\n",
    "    train_ratio=config['splits']['ratios']['train'],\n",
    "    val_ratio=config['splits']['ratios']['val'],\n",
    "    test_ratio=config['splits']['ratios']['test'],\n",
    "    seed=config['splits']['seed'],\n",
    ")\n",
    "\n",
    "# Persist splits\n",
    "split_output_dir = Path(config['splits']['output_dir'])\n",
    "split_path = save_splits(split_info, split_output_dir)\n",
    "print(f'Splits saved to: {split_path}')\n",
    "print(f'Train: {len(split_info.train_speakers)} speakers, {split_info.utterances_per_split[\"train\"]:,} utts')\n",
    "print(f'Val:   {len(split_info.val_speakers)} speakers, {split_info.utterances_per_split[\"val\"]:,} utts')\n",
    "print(f'Test:  {len(split_info.test_speakers)} speakers, {split_info.utterances_per_split[\"test\"]:,} utts')\n",
    "\n",
    "# Assign entries to splits\n",
    "split_entries = assign_entries_to_splits(combined_entries, split_info)\n",
    "\n",
    "train_entries = split_entries['train']\n",
    "val_entries = split_entries['val']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Verify speaker-disjoint (hard fail if violated)\n",
    "train_spk = {e.speaker_id for e in train_entries}\n",
    "val_spk = {e.speaker_id for e in val_entries}\n",
    "test_spk = {e.speaker_id for e in test_entries}\n",
    "\n",
    "assert len(train_spk & val_spk) == 0, 'Speaker leakage train -> val'\n",
    "assert len(train_spk & test_spk) == 0, 'Speaker leakage train -> test'\n",
    "assert len(val_spk & test_spk) == 0, 'Speaker leakage val -> test'\n",
    "print('\\nSpeaker-disjoint verification: PASSED')\n",
    "\n",
    "# Distribution per split\n",
    "for split_name, entries_list in split_entries.items():\n",
    "    accent_dist = Counter(e.accent for e in entries_list)\n",
    "    print(f'  {split_name}: {dict(sorted(accent_dist.items()))}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Accent Classifier\n",
    "\n",
    "3-block CNN operating on mel-spectrograms.  \n",
    "Architecture: Conv2d -> BatchNorm -> ReLU -> MaxPool (x3) -> AdaptiveAvgPool -> Linear.  \n",
    "Early stopping on validation balanced accuracy.  \n",
    "Class-weighted CrossEntropyLoss for imbalanced accent distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build label mapping (sorted, deterministic)\nlabel_to_idx = MelSpectrogramDataset.build_label_mapping(combined_entries)\nidx_to_label = {v: k for k, v in label_to_idx.items()}\nn_classes = len(label_to_idx)\nlabel_names = [idx_to_label[i] for i in range(n_classes)]\n\nprint(f'Classes ({n_classes}): {label_names}')\nprint(f'Label mapping: {label_to_idx}')\n\n# Persist label_to_idx for reproducibility — ensures same mapping across runs\nlabel_map_path = Path(config['output']['report_dir']) / 'label_to_idx.json'\nlabel_map_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(label_map_path, 'w') as f:\n    json.dump(label_to_idx, f, indent=2)\nprint(f'Label mapping saved to: {label_map_path}')\n\n# CNN hyperparameters from config\ncnn_cfg = config['cnn']\n\n# Create datasets\ntrain_mel_ds = MelSpectrogramDataset(\n    entries=train_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\nval_mel_ds = MelSpectrogramDataset(\n    entries=val_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\ntest_mel_ds = MelSpectrogramDataset(\n    entries=test_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\n\nprint(f'\\nMel datasets: train={len(train_mel_ds)}, val={len(val_mel_ds)}, test={len(test_mel_ds)}')\n\n# Create DataLoaders with reproducible worker seeds\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(SEED)\n\ncnn_batch_size = cnn_cfg['training']['batch_size']\n\ntrain_mel_loader = torch.utils.data.DataLoader(\n    train_mel_ds, batch_size=cnn_batch_size, shuffle=True,\n    num_workers=2, worker_init_fn=seed_worker, generator=g, pin_memory=True,\n)\nval_mel_loader = torch.utils.data.DataLoader(\n    val_mel_ds, batch_size=cnn_batch_size, shuffle=False,\n    num_workers=2, pin_memory=True,\n)\ntest_mel_loader = torch.utils.data.DataLoader(\n    test_mel_ds, batch_size=cnn_batch_size, shuffle=False,\n    num_workers=2, pin_memory=True,\n)\n\n# Compute class weights for imbalanced data\ntrain_labels_cnn = [label_to_idx[e.accent] for e in train_entries]\ncnn_class_weights = compute_class_weights(train_labels_cnn, n_classes)\nprint(f'CNN class weights: {cnn_class_weights.tolist()}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train CNN\n",
    "cnn_model = AccentCNN(\n",
    "    n_classes=n_classes,\n",
    "    n_mels=cnn_cfg['n_mels'],\n",
    "    conv_channels=cnn_cfg['conv_channels'],\n",
    ")\n",
    "\n",
    "cnn_checkpoint_dir = Path(config['output']['checkpoint_dir']) / 'cnn'\n",
    "\n",
    "cnn_training_config = TrainingConfig(\n",
    "    learning_rate=cnn_cfg['training']['learning_rate'],\n",
    "    batch_size=cnn_cfg['training']['batch_size'],\n",
    "    n_epochs=cnn_cfg['training']['n_epochs'],\n",
    "    patience=cnn_cfg['training']['patience'],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    checkpoint_dir=cnn_checkpoint_dir,\n",
    "    experiment_name='accent_cnn',\n",
    "    use_amp=cnn_cfg['training']['use_amp'],\n",
    ")\n",
    "\n",
    "print(f'Training CNN: lr={cnn_training_config.learning_rate}, '\n",
    "      f'epochs={cnn_training_config.n_epochs}, '\n",
    "      f'patience={cnn_training_config.patience}')\n",
    "\n",
    "cnn_result = train_classifier(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_mel_loader,\n",
    "    val_loader=val_mel_loader,\n",
    "    config=cnn_training_config,\n",
    "    class_weights=cnn_class_weights,\n",
    ")\n",
    "\n",
    "print(f'\\nCNN training complete:')\n",
    "print(f'  Best epoch: {cnn_result.best_epoch}')\n",
    "print(f'  Best val bal_acc: {cnn_result.best_val_bal_acc:.4f}')\n",
    "print(f'  Total epochs: {cnn_result.total_epochs_run}')\n",
    "print(f'  Checkpoint: {cnn_result.best_checkpoint_path}')\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(cnn_result.train_losses, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('CNN Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(cnn_result.val_bal_accs, label='Val Balanced Accuracy', color='orange')\n",
    "ax2.axhline(y=1.0/n_classes, color='red', linestyle='--', label=f'Chance ({1.0/n_classes:.2f})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('CNN Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "figures_dir = Path(config['output']['figures_dir'])\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(figures_dir / 'cnn_training_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate CNN on test set\n# Load best checkpoint (weights_only=False: checkpoint contains config dict)\ncheckpoint = torch.load(cnn_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\ncnn_model.load_state_dict(checkpoint['model_state_dict'])\n\ncnn_eval = evaluate_classifier(\n    model=cnn_model,\n    test_loader=test_mel_loader,\n    label_names=label_names,\n    device=DEVICE,\n    n_bootstrap=config['evaluation']['bootstrap_n_samples'],\n)\n\nprint('=== CNN TEST EVALUATION ===')\nprint(f'Balanced Accuracy: {cnn_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI 95%: [{cnn_eval[\"ci_95_lower\"]:.4f}, {cnn_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'F1 Macro: {cnn_eval[\"f1_macro\"]:.4f}')\nprint(f'Chance level: {1.0/n_classes:.4f}')\nprint(f'\\nPer-class recall:')\nfor name, recall in cnn_eval['per_class_recall'].items():\n    print(f'  {name}: {recall:.4f}')\n\n# Display confusion matrix\ncm = np.array(cnn_eval['confusion_matrix'])\ncm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=label_names,\n            yticklabels=label_names, cmap='Blues', ax=ax1)\nax1.set_title('CNN Confusion Matrix (counts)')\nax1.set_xlabel('Predicted')\nax1.set_ylabel('True')\n\nsns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=label_names,\n            yticklabels=label_names, cmap='Blues', ax=ax2)\nax2.set_title('CNN Confusion Matrix (row-normalized recall)')\nax2.set_xlabel('Predicted')\nax2.set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig(figures_dir / 'cnn_confusion_matrix.png', dpi=150)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav2vec2 Accent Classifier\n",
    "\n",
    "Pre-trained wav2vec2-base with frozen CNN feature extractor + fine-tuned transformer + linear head.  \n",
    "Operates on raw waveforms (no mel-spectrogram preprocessing).  \n",
    "Smaller batch size due to VRAM constraints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# wav2vec2 hyperparameters from config\n",
    "w2v_cfg = config['wav2vec2']\n",
    "\n",
    "# Create waveform datasets\n",
    "train_wav_ds = WaveformDataset(\n",
    "    entries=train_entries,\n",
    "    label_to_idx=label_to_idx,\n",
    "    max_length_s=w2v_cfg['max_length_s'],\n",
    ")\n",
    "val_wav_ds = WaveformDataset(\n",
    "    entries=val_entries,\n",
    "    label_to_idx=label_to_idx,\n",
    "    max_length_s=w2v_cfg['max_length_s'],\n",
    ")\n",
    "test_wav_ds = WaveformDataset(\n",
    "    entries=test_entries,\n",
    "    label_to_idx=label_to_idx,\n",
    "    max_length_s=w2v_cfg['max_length_s'],\n",
    ")\n",
    "\n",
    "print(f'Waveform datasets: train={len(train_wav_ds)}, val={len(val_wav_ds)}, test={len(test_wav_ds)}')\n",
    "\n",
    "# DataLoaders (smaller batch for wav2vec2 VRAM)\n",
    "g_w2v = torch.Generator()\n",
    "g_w2v.manual_seed(SEED)\n",
    "\n",
    "w2v_batch_size = w2v_cfg['training']['batch_size']\n",
    "\n",
    "train_wav_loader = torch.utils.data.DataLoader(\n",
    "    train_wav_ds, batch_size=w2v_batch_size, shuffle=True,\n",
    "    num_workers=2, worker_init_fn=seed_worker, generator=g_w2v, pin_memory=True,\n",
    ")\n",
    "val_wav_loader = torch.utils.data.DataLoader(\n",
    "    val_wav_ds, batch_size=w2v_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=True,\n",
    ")\n",
    "test_wav_loader = torch.utils.data.DataLoader(\n",
    "    test_wav_ds, batch_size=w2v_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=True,\n",
    ")\n",
    "\n",
    "# Class weights (same distribution as CNN)\n",
    "w2v_class_weights = compute_class_weights(train_labels_cnn, n_classes)\n",
    "print(f'wav2vec2 class weights: {w2v_class_weights.tolist()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train wav2vec2\n",
    "# Free CNN memory before loading wav2vec2\n",
    "del cnn_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "w2v_model = AccentWav2Vec2(\n",
    "    n_classes=n_classes,\n",
    "    model_name=w2v_cfg['model_name'],\n",
    "    freeze_feature_extractor=w2v_cfg['freeze_feature_extractor'],\n",
    ")\n",
    "\n",
    "w2v_checkpoint_dir = Path(config['output']['checkpoint_dir']) / 'wav2vec2'\n",
    "\n",
    "w2v_training_config = TrainingConfig(\n",
    "    learning_rate=w2v_cfg['training']['learning_rate'],\n",
    "    batch_size=w2v_cfg['training']['batch_size'],\n",
    "    n_epochs=w2v_cfg['training']['n_epochs'],\n",
    "    patience=w2v_cfg['training']['patience'],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    checkpoint_dir=w2v_checkpoint_dir,\n",
    "    experiment_name='accent_wav2vec2',\n",
    "    use_amp=w2v_cfg['training']['use_amp'],\n",
    ")\n",
    "\n",
    "print(f'Training wav2vec2: lr={w2v_training_config.learning_rate}, '\n",
    "      f'epochs={w2v_training_config.n_epochs}, '\n",
    "      f'patience={w2v_training_config.patience}')\n",
    "print(f'VRAM before training: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
    "\n",
    "w2v_result = train_classifier(\n",
    "    model=w2v_model,\n",
    "    train_loader=train_wav_loader,\n",
    "    val_loader=val_wav_loader,\n",
    "    config=w2v_training_config,\n",
    "    class_weights=w2v_class_weights,\n",
    ")\n",
    "\n",
    "print(f'\\nwav2vec2 training complete:')\n",
    "print(f'  Best epoch: {w2v_result.best_epoch}')\n",
    "print(f'  Best val bal_acc: {w2v_result.best_val_bal_acc:.4f}')\n",
    "print(f'  Total epochs: {w2v_result.total_epochs_run}')\n",
    "print(f'  Checkpoint: {w2v_result.best_checkpoint_path}')\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(w2v_result.train_losses, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('wav2vec2 Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(w2v_result.val_bal_accs, label='Val Balanced Accuracy', color='orange')\n",
    "ax2.axhline(y=1.0/n_classes, color='red', linestyle='--', label=f'Chance ({1.0/n_classes:.2f})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('wav2vec2 Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'wav2vec2_training_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate wav2vec2 on test set\ncheckpoint_w2v = torch.load(w2v_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\nw2v_model.load_state_dict(checkpoint_w2v['model_state_dict'])\n\nw2v_eval = evaluate_classifier(\n    model=w2v_model,\n    test_loader=test_wav_loader,\n    label_names=label_names,\n    device=DEVICE,\n    n_bootstrap=config['evaluation']['bootstrap_n_samples'],\n)\n\nprint('=== WAV2VEC2 TEST EVALUATION ===')\nprint(f'Balanced Accuracy: {w2v_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI 95%: [{w2v_eval[\"ci_95_lower\"]:.4f}, {w2v_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'F1 Macro: {w2v_eval[\"f1_macro\"]:.4f}')\nprint(f'Chance level: {1.0/n_classes:.4f}')\nprint(f'\\nPer-class recall:')\nfor name, recall in w2v_eval['per_class_recall'].items():\n    print(f'  {name}: {recall:.4f}')\n\n# Display confusion matrix\ncm_w2v = np.array(w2v_eval['confusion_matrix'])\ncm_w2v_norm = cm_w2v.astype(float) / cm_w2v.sum(axis=1, keepdims=True)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(cm_w2v, annot=True, fmt='d', xticklabels=label_names,\n            yticklabels=label_names, cmap='Greens', ax=ax1)\nax1.set_title('wav2vec2 Confusion Matrix (counts)')\nax1.set_xlabel('Predicted')\nax1.set_ylabel('True')\n\nsns.heatmap(cm_w2v_norm, annot=True, fmt='.2f', xticklabels=label_names,\n            yticklabels=label_names, cmap='Greens', ax=ax2)\nax2.set_title('wav2vec2 Confusion Matrix (row-normalized recall)')\nax2.set_xlabel('Predicted')\nax2.set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig(figures_dir / 'wav2vec2_confusion_matrix.png', dpi=150)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Source Evaluation (Source Confound Check)\n",
    "\n",
    "**Objetivo:** verificar se o classificador aprendeu *sotaque* ou *fonte*.  \n",
    "- Treinamos na fonte A (CORAA-MUPE), testamos na fonte B (Common Voice) e vice-versa.  \n",
    "- Se ambas as direcoes ficam em chance level -> classificador aprendeu source, nao accent.  \n",
    "- Se ao menos uma direcao fica acima de chance -> sinal de accent e transferivel entre fontes.  \n",
    "\n",
    "Usamos o CNN (mais rapido) para este cross-source check."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check that primary classifier is above chance before cross-source\nchance = 1.0 / n_classes\nbest_primary = max(cnn_eval['balanced_accuracy'], w2v_eval['balanced_accuracy'])\nif best_primary <= chance + 0.05:\n    print(f'WARNING: Best primary classifier ({best_primary:.4f}) is at chance ({chance:.4f}).')\n    print('Cross-source evaluation may not be meaningful.')\n\n# Split combined entries by source\ncoraa_train = [e for e in train_entries if e.source == 'CORAA-MUPE']\ncoraa_test_split = [e for e in test_entries if e.source == 'CORAA-MUPE']\ncv_train = [e for e in train_entries if e.source == 'CommonVoice-PT']\ncv_test_split = [e for e in test_entries if e.source == 'CommonVoice-PT']\n\nprint(f'CORAA-MUPE: train={len(coraa_train)}, test={len(coraa_test_split)}')\nprint(f'CommonVoice-PT: train={len(cv_train)}, test={len(cv_test_split)}')\n\ncross_source_results = {}\n\n# Direction 1: Train on CORAA-MUPE, test on CommonVoice-PT\nif len(coraa_train) > 0 and len(cv_test_split) > 0:\n    print('\\n--- Direction 1: Train CORAA-MUPE -> Test CommonVoice-PT ---')\n\n    cs_train_ds = MelSpectrogramDataset(\n        coraa_train, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n    cs_test_ds = MelSpectrogramDataset(\n        cv_test_split, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    # Validation: use same-source val split (never train data)\n    coraa_val = [e for e in val_entries if e.source == 'CORAA-MUPE']\n    if len(coraa_val) < 10:\n        # Not enough val samples from this source — use full val split as fallback\n        # This is safe: val is speaker-disjoint from train, just mixes sources\n        coraa_val = list(val_entries)\n        print(f'  Val fallback: using full val split ({len(coraa_val)} entries, mixed source)')\n\n    cs_val_ds = MelSpectrogramDataset(\n        coraa_val, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    g_cs1 = torch.Generator()\n    g_cs1.manual_seed(SEED)\n\n    cs_train_loader = torch.utils.data.DataLoader(\n        cs_train_ds, batch_size=cnn_batch_size, shuffle=True,\n        num_workers=2, worker_init_fn=seed_worker, generator=g_cs1, pin_memory=True,\n    )\n    cs_val_loader = torch.utils.data.DataLoader(\n        cs_val_ds, batch_size=cnn_batch_size, shuffle=False, num_workers=2, pin_memory=True,\n    )\n    cs_test_loader = torch.utils.data.DataLoader(\n        cs_test_ds, batch_size=cnn_batch_size, shuffle=False, num_workers=2, pin_memory=True,\n    )\n\n    cs_labels = [label_to_idx[e.accent] for e in coraa_train]\n    cs_weights = compute_class_weights(cs_labels, n_classes)\n\n    cs_model_1 = AccentCNN(n_classes=n_classes, n_mels=cnn_cfg['n_mels'], conv_channels=cnn_cfg['conv_channels'])\n    cs_config_1 = TrainingConfig(\n        learning_rate=cnn_cfg['training']['learning_rate'],\n        batch_size=cnn_batch_size, n_epochs=cnn_cfg['training']['n_epochs'],\n        patience=cnn_cfg['training']['patience'], device=DEVICE, seed=SEED,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / 'cross_source_coraa2cv',\n        experiment_name='cross_source_coraa2cv', use_amp=cnn_cfg['training']['use_amp'],\n    )\n\n    cs_result_1 = train_classifier(cs_model_1, cs_train_loader, cs_val_loader, cs_config_1, cs_weights)\n\n    checkpoint_cs1 = torch.load(cs_result_1.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    cs_model_1.load_state_dict(checkpoint_cs1['model_state_dict'])\n\n    cs_eval_1 = evaluate_classifier(cs_model_1, cs_test_loader, label_names, DEVICE)\n    cross_source_results['coraa2cv'] = cs_eval_1\n\n    print(f'CORAA->CV bal_acc: {cs_eval_1[\"balanced_accuracy\"]:.4f} '\n          f'(CI: [{cs_eval_1[\"ci_95_lower\"]:.4f}, {cs_eval_1[\"ci_95_upper\"]:.4f}])')\n\n    del cs_model_1\n    torch.cuda.empty_cache()\nelse:\n    print('Skipping direction 1: insufficient data')\n\n# Direction 2: Train on CommonVoice-PT, test on CORAA-MUPE\nif len(cv_train) > 0 and len(coraa_test_split) > 0:\n    print('\\n--- Direction 2: Train CommonVoice-PT -> Test CORAA-MUPE ---')\n\n    cs_train_ds2 = MelSpectrogramDataset(\n        cv_train, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n    cs_test_ds2 = MelSpectrogramDataset(\n        coraa_test_split, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    # Validation: use same-source val split (never train data)\n    cv_val = [e for e in val_entries if e.source == 'CommonVoice-PT']\n    if len(cv_val) < 10:\n        cv_val = list(val_entries)\n        print(f'  Val fallback: using full val split ({len(cv_val)} entries, mixed source)')\n\n    cs_val_ds2 = MelSpectrogramDataset(\n        cv_val, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    g_cs2 = torch.Generator()\n    g_cs2.manual_seed(SEED)\n\n    cs_train_loader2 = torch.utils.data.DataLoader(\n        cs_train_ds2, batch_size=cnn_batch_size, shuffle=True,\n        num_workers=2, worker_init_fn=seed_worker, generator=g_cs2, pin_memory=True,\n    )\n    cs_val_loader2 = torch.utils.data.DataLoader(\n        cs_val_ds2, batch_size=cnn_batch_size, shuffle=False, num_workers=2, pin_memory=True,\n    )\n    cs_test_loader2 = torch.utils.data.DataLoader(\n        cs_test_ds2, batch_size=cnn_batch_size, shuffle=False, num_workers=2, pin_memory=True,\n    )\n\n    cs_labels2 = [label_to_idx[e.accent] for e in cv_train]\n    cs_weights2 = compute_class_weights(cs_labels2, n_classes)\n\n    cs_model_2 = AccentCNN(n_classes=n_classes, n_mels=cnn_cfg['n_mels'], conv_channels=cnn_cfg['conv_channels'])\n    cs_config_2 = TrainingConfig(\n        learning_rate=cnn_cfg['training']['learning_rate'],\n        batch_size=cnn_batch_size, n_epochs=cnn_cfg['training']['n_epochs'],\n        patience=cnn_cfg['training']['patience'], device=DEVICE, seed=SEED,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / 'cross_source_cv2coraa',\n        experiment_name='cross_source_cv2coraa', use_amp=cnn_cfg['training']['use_amp'],\n    )\n\n    cs_result_2 = train_classifier(cs_model_2, cs_train_loader2, cs_val_loader2, cs_config_2, cs_weights2)\n\n    checkpoint_cs2 = torch.load(cs_result_2.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    cs_model_2.load_state_dict(checkpoint_cs2['model_state_dict'])\n\n    cs_eval_2 = evaluate_classifier(cs_model_2, cs_test_loader2, label_names, DEVICE)\n    cross_source_results['cv2coraa'] = cs_eval_2\n\n    print(f'CV->CORAA bal_acc: {cs_eval_2[\"balanced_accuracy\"]:.4f} '\n          f'(CI: [{cs_eval_2[\"ci_95_lower\"]:.4f}, {cs_eval_2[\"ci_95_upper\"]:.4f}])')\n\n    del cs_model_2\n    torch.cuda.empty_cache()\nelse:\n    print('Skipping direction 2: insufficient data')\n\n# Interpretation\nprint(f'\\n=== CROSS-SOURCE SUMMARY ===')\nprint(f'Chance level: {chance:.4f}')\nfor direction, eval_result in cross_source_results.items():\n    ba = eval_result['balanced_accuracy']\n    above_chance = ba > chance + 0.05  # 5pp above chance\n    status = 'ABOVE CHANCE (accent signal transfers)' if above_chance else 'AT CHANCE (possible source confound)'\n    print(f'  {direction}: bal_acc={ba:.4f} -> {status}')\n\nif all(r['balanced_accuracy'] <= chance + 0.05 for r in cross_source_results.values()):\n    print('\\nWARNING: Both directions at chance. Classifier may have learned source, not accent.')\nelse:\n    print('\\nAt least one direction shows transfer. Accent signal appears generalizable across sources.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Summary\n",
    "\n",
    "Comparison table: CNN vs wav2vec2, with balanced accuracy, CI 95%, F1 macro, and cross-source results.  \n",
    "All metrics follow the protocol: balanced accuracy (primary), CI 95% (bootstrap, 1000 samples)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build comparison table\n",
    "chance = 1.0 / n_classes\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['CNN (mel-spectrogram)', 'wav2vec2-base'],\n",
    "    'Balanced Accuracy': [\n",
    "        f'{cnn_eval[\"balanced_accuracy\"]:.4f}',\n",
    "        f'{w2v_eval[\"balanced_accuracy\"]:.4f}',\n",
    "    ],\n",
    "    'CI 95% Lower': [\n",
    "        f'{cnn_eval[\"ci_95_lower\"]:.4f}',\n",
    "        f'{w2v_eval[\"ci_95_lower\"]:.4f}',\n",
    "    ],\n",
    "    'CI 95% Upper': [\n",
    "        f'{cnn_eval[\"ci_95_upper\"]:.4f}',\n",
    "        f'{w2v_eval[\"ci_95_upper\"]:.4f}',\n",
    "    ],\n",
    "    'F1 Macro': [\n",
    "        f'{cnn_eval[\"f1_macro\"]:.4f}',\n",
    "        f'{w2v_eval[\"f1_macro\"]:.4f}',\n",
    "    ],\n",
    "    'Best Epoch': [\n",
    "        cnn_result.best_epoch,\n",
    "        w2v_result.best_epoch,\n",
    "    ],\n",
    "    'Total Epochs': [\n",
    "        cnn_result.total_epochs_run,\n",
    "        w2v_result.total_epochs_run,\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('=== ABLATION: CNN vs wav2vec2 ===')\n",
    "print(f'Chance level: {chance:.4f}')\n",
    "print()\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Check if CIs overlap (cannot claim one is better)\n",
    "cnn_ci = (cnn_eval['ci_95_lower'], cnn_eval['ci_95_upper'])\n",
    "w2v_ci = (w2v_eval['ci_95_lower'], w2v_eval['ci_95_upper'])\n",
    "\n",
    "overlap = cnn_ci[0] <= w2v_ci[1] and w2v_ci[0] <= cnn_ci[1]\n",
    "if overlap:\n",
    "    print('\\nCIs overlap -> cannot claim one model is superior.')\n",
    "else:\n",
    "    winner = 'wav2vec2' if w2v_eval['balanced_accuracy'] > cnn_eval['balanced_accuracy'] else 'CNN'\n",
    "    print(f'\\nCIs do NOT overlap -> {winner} is significantly better.')\n",
    "\n",
    "# Cross-source results table\n",
    "if cross_source_results:\n",
    "    print('\\n=== CROSS-SOURCE EVALUATION ===')\n",
    "    for direction, result in cross_source_results.items():\n",
    "        print(f'  {direction}: bal_acc={result[\"balanced_accuracy\"]:.4f} '\n",
    "              f'(CI: [{result[\"ci_95_lower\"]:.4f}, {result[\"ci_95_upper\"]:.4f}])')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save full report as JSON with all metrics, configs, and hashes\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Git commit hash\n",
    "try:\n",
    "    commit_hash = subprocess.check_output(\n",
    "        ['git', 'rev-parse', 'HEAD'], text=True\n",
    "    ).strip()\n",
    "except Exception:\n",
    "    commit_hash = 'unknown'\n",
    "\n",
    "# SHA-256 of the combined manifest\n",
    "manifest_sha256 = compute_file_hash(COMBINED_MANIFEST_PATH) if COMBINED_MANIFEST_PATH.exists() else 'N/A'\n",
    "\n",
    "report = {\n",
    "    'experiment': config['experiment']['name'],\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'commit_hash': commit_hash,\n",
    "    'seed': SEED,\n",
    "    'environment': {\n",
    "        'python_version': sys.version,\n",
    "        'torch_version': torch.__version__,\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n",
    "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': config['dataset']['name'],\n",
    "        'combined_manifest_sha256': manifest_sha256,\n",
    "        'total_entries': len(combined_entries),\n",
    "        'total_speakers': len({e.speaker_id for e in combined_entries}),\n",
    "        'n_classes': n_classes,\n",
    "        'label_names': label_names,\n",
    "        'per_source': dict(Counter(e.source for e in combined_entries)),\n",
    "        'per_accent': dict(Counter(e.accent for e in combined_entries)),\n",
    "    },\n",
    "    'splits': {\n",
    "        'method': config['splits']['method'],\n",
    "        'ratios': config['splits']['ratios'],\n",
    "        'seed': config['splits']['seed'],\n",
    "        'train_utterances': len(train_entries),\n",
    "        'val_utterances': len(val_entries),\n",
    "        'test_utterances': len(test_entries),\n",
    "        'train_speakers': len(split_info.train_speakers),\n",
    "        'val_speakers': len(split_info.val_speakers),\n",
    "        'test_speakers': len(split_info.test_speakers),\n",
    "    },\n",
    "    'confounds': [\n",
    "        {\n",
    "            'test': r.test_name,\n",
    "            'variables': f'{r.variable_a} x {r.variable_b}',\n",
    "            'statistic': r.statistic,\n",
    "            'p_value': r.p_value,\n",
    "            'effect_size': r.effect_size,\n",
    "            'effect_size_name': r.effect_size_name,\n",
    "            'is_blocking': r.is_blocking,\n",
    "            'is_significant': r.is_significant,\n",
    "            'interpretation': r.interpretation,\n",
    "        }\n",
    "        for r in confound_results\n",
    "    ],\n",
    "    'cnn': {\n",
    "        'config': {\n",
    "            'n_mels': cnn_cfg['n_mels'],\n",
    "            'max_frames': cnn_cfg['max_frames'],\n",
    "            'conv_channels': cnn_cfg['conv_channels'],\n",
    "            'learning_rate': cnn_cfg['training']['learning_rate'],\n",
    "            'batch_size': cnn_cfg['training']['batch_size'],\n",
    "            'n_epochs': cnn_cfg['training']['n_epochs'],\n",
    "            'patience': cnn_cfg['training']['patience'],\n",
    "        },\n",
    "        'training': {\n",
    "            'best_epoch': cnn_result.best_epoch,\n",
    "            'best_val_bal_acc': cnn_result.best_val_bal_acc,\n",
    "            'total_epochs_run': cnn_result.total_epochs_run,\n",
    "            'checkpoint_path': str(cnn_result.best_checkpoint_path),\n",
    "        },\n",
    "        'evaluation': cnn_eval,\n",
    "    },\n",
    "    'wav2vec2': {\n",
    "        'config': {\n",
    "            'model_name': w2v_cfg['model_name'],\n",
    "            'freeze_feature_extractor': w2v_cfg['freeze_feature_extractor'],\n",
    "            'max_length_s': w2v_cfg['max_length_s'],\n",
    "            'learning_rate': w2v_cfg['training']['learning_rate'],\n",
    "            'batch_size': w2v_cfg['training']['batch_size'],\n",
    "            'n_epochs': w2v_cfg['training']['n_epochs'],\n",
    "            'patience': w2v_cfg['training']['patience'],\n",
    "        },\n",
    "        'training': {\n",
    "            'best_epoch': w2v_result.best_epoch,\n",
    "            'best_val_bal_acc': w2v_result.best_val_bal_acc,\n",
    "            'total_epochs_run': w2v_result.total_epochs_run,\n",
    "            'checkpoint_path': str(w2v_result.best_checkpoint_path),\n",
    "        },\n",
    "        'evaluation': w2v_eval,\n",
    "    },\n",
    "    'cross_source': cross_source_results,\n",
    "    'chance_level': chance,\n",
    "}\n",
    "\n",
    "# Save report\n",
    "report_dir = Path(config['output']['report_dir'])\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_path = Path(config['output']['report_json'])\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f'Report saved to: {report_path}')\n",
    "print(f'Combined manifest SHA-256: {manifest_sha256}')\n",
    "print(f'Commit hash: {commit_hash}')\n",
    "print(f'\\n=== EXPERIMENT COMPLETE ===')\n",
    "print(f'CNN bal_acc: {cnn_eval[\"balanced_accuracy\"]:.4f} '\n",
    "      f'(CI: [{cnn_eval[\"ci_95_lower\"]:.4f}, {cnn_eval[\"ci_95_upper\"]:.4f}])')\n",
    "print(f'wav2vec2 bal_acc: {w2v_eval[\"balanced_accuracy\"]:.4f} '\n",
    "      f'(CI: [{w2v_eval[\"ci_95_lower\"]:.4f}, {w2v_eval[\"ci_95_upper\"]:.4f}])')\n",
    "print(f'Chance level: {chance:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}