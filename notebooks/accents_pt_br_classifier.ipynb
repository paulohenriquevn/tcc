{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Accents-PT-BR — Accent Classifier Ablation (CNN vs wav2vec2)\n\n**Projeto:** Controle Explícito de Sotaque Regional em pt-BR  \n**Objetivo:** Treinar e avaliar classificadores de sotaque (CNN mel-spectrogram vs wav2vec2 fine-tuned) no dataset combinado Accents-PT-BR (CORAA-MUPE + Common Voice PT). Esses classificadores servem como **avaliadores externos** para os Stages 2-3 (medir se o áudio gerado pelo LoRA carrega o sotaque-alvo).  \n**Config:** `configs/accent_classifier.yaml` (single source of truth).  \n**Dataset:** Accents-PT-BR = CORAA-MUPE (entrevistados) + Common Voice PT (accent label normalizado).  \n\n**Seções:**\n1. Setup do ambiente\n2. Dataset pipeline (shared with dataset notebook)\n3. CNN accent classifier (treinamento + avaliação)\n4. wav2vec2 accent classifier (treinamento + avaliação)\n5. Robustness check (multiple seeds)\n6. Cross-source evaluation (confound check)\n7. Ablation summary + report\n\nEste notebook é a **camada de orquestração**. Toda lógica está em `src/` (testável, auditável).  \nO notebook apenas: instala deps → configura ambiente → chama módulos → exibe resultados."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Bootstrap: clone repo, install deps, check NumPy ABI.\n# This module uses only stdlib — safe to import before pip install.\n# On first Colab run, this cell may restart the runtime once (NumPy ABI fix).\nfrom src.utils.notebook_bootstrap import bootstrap\nbootstrap()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys, yaml, json, logging\nfrom pathlib import Path\nfrom collections import Counter\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Platform-aware persistent cache setup\nfrom src.utils.platform import detect_platform, setup_environment\n\nplatform = detect_platform()\nsetup_environment(platform)\n\n# Fix #1: use seed_worker from src (includes random.seed — the inline version missed it)\nfrom src.utils.seed import set_global_seed, seed_worker\nfrom src.utils.git import get_commit_hash\nfrom src.data.manifest import compute_file_hash\nfrom src.classifier import (\n    AccentCNN, AccentWav2Vec2,\n    train_classifier, evaluate_classifier,\n    TrainingConfig, TrainingResult,\n)\nfrom src.classifier.mel_dataset import MelSpectrogramDataset\nfrom src.classifier.wav2vec2_dataset import WaveformDataset\nfrom src.classifier.trainer import compute_class_weights\n\n# Load config — single source of truth for all experiment parameters\nwith open('configs/accent_classifier.yaml') as f:\n    config = yaml.safe_load(f)\n\nSEED = config['seed']['global']\ngenerator = set_global_seed(SEED)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(name)s - %(levelname)s - %(message)s',\n)\n\nprint(f'Platform: {platform.name}')\nprint(f'Config loaded: {config[\"experiment\"][\"name\"]}')\nprint(f'Seed global: {SEED}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Environment check: GPU, CUDA, PyTorch versions\nprint(f'Python: {sys.version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n    print(f'CUDA version: {torch.version.cuda}')\n    print(f'VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'\\nUsando device: {DEVICE}')\n\n# Drive cache base directory — platform-aware\nDRIVE_BASE = platform.cache_base\nDRIVE_BASE.mkdir(parents=True, exist_ok=True)\nprint(f'Cache base: {DRIVE_BASE}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Dataset Pipeline\n\nExecuta a pipeline completa via `src.data.pipeline.load_or_build_accents_dataset()`.  \nA mesma função usada pelo dataset notebook — elimina duplicação (DRY)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from src.data.pipeline import load_or_build_accents_dataset\n\nbundle = load_or_build_accents_dataset(config, DRIVE_BASE)\n\ncombined_entries = bundle.combined_entries\nsplit_info = bundle.split_info\nsplit_entries = bundle.split_entries\nconfound_results = bundle.confound_results\n\ntrain_entries = split_entries['train']\nval_entries = split_entries['val']\ntest_entries = split_entries['test']\n\n# Cross-tabulations for reference\ngender_table = pd.crosstab(\n    [e.accent for e in combined_entries],\n    [e.gender for e in combined_entries],\n    margins=True,\n)\nprint('\\n=== ACCENT x GENDER TABLE ===')\nprint(gender_table)\n\nsource_table = pd.crosstab(\n    [e.accent for e in combined_entries],\n    [e.source for e in combined_entries],\n    margins=True,\n)\nprint('\\n=== ACCENT x SOURCE TABLE ===')\nprint(source_table)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. CNN Accent Classifier\n\n3-block CNN operating on mel-spectrograms.  \nArchitecture: Conv2d -> BatchNorm -> ReLU -> MaxPool (x3) -> AdaptiveAvgPool -> Linear.  \nEarly stopping on validation balanced accuracy.  \nClass-weighted CrossEntropyLoss for imbalanced accent distributions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Build label mapping (sorted, deterministic)\nlabel_to_idx = MelSpectrogramDataset.build_label_mapping(combined_entries)\nidx_to_label = {v: k for k, v in label_to_idx.items()}\nn_classes = len(label_to_idx)\nlabel_names = [idx_to_label[i] for i in range(n_classes)]\n\nprint(f'Classes ({n_classes}): {label_names}')\nprint(f'Label mapping: {label_to_idx}')\n\n# Persist label_to_idx for reproducibility — ensures same mapping across runs\nlabel_map_path = Path(config['output']['report_dir']) / 'label_to_idx.json'\nlabel_map_path.parent.mkdir(parents=True, exist_ok=True)\nwith open(label_map_path, 'w') as f:\n    json.dump(label_to_idx, f, indent=2)\nprint(f'Label mapping saved to: {label_map_path}')\n\n# CNN hyperparameters from config\ncnn_cfg = config['cnn']\n\n# Create datasets\ntrain_mel_ds = MelSpectrogramDataset(\n    entries=train_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\nval_mel_ds = MelSpectrogramDataset(\n    entries=val_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\ntest_mel_ds = MelSpectrogramDataset(\n    entries=test_entries,\n    label_to_idx=label_to_idx,\n    n_mels=cnn_cfg['n_mels'],\n    max_frames=cnn_cfg['max_frames'],\n)\n\nprint(f'\\nMel datasets: train={len(train_mel_ds)}, val={len(val_mel_ds)}, test={len(test_mel_ds)}')\n\n# DataLoaders with reproducible worker seeds\n# seed_worker from src.utils.seed includes both np.random.seed AND random.seed\ng = torch.Generator()\ng.manual_seed(SEED)\n\ncnn_batch_size = cnn_cfg['training']['batch_size']\ncnn_num_workers = cnn_cfg['training']['num_workers']\n\ntrain_mel_loader = torch.utils.data.DataLoader(\n    train_mel_ds, batch_size=cnn_batch_size, shuffle=True,\n    num_workers=cnn_num_workers, worker_init_fn=seed_worker, generator=g, pin_memory=True,\n)\nval_mel_loader = torch.utils.data.DataLoader(\n    val_mel_ds, batch_size=cnn_batch_size, shuffle=False,\n    num_workers=cnn_num_workers, pin_memory=True,\n)\ntest_mel_loader = torch.utils.data.DataLoader(\n    test_mel_ds, batch_size=cnn_batch_size, shuffle=False,\n    num_workers=cnn_num_workers, pin_memory=True,\n)\n\n# Compute class weights for imbalanced data (shared across both classifiers)\ntrain_labels = [label_to_idx[e.accent] for e in train_entries]\ncnn_class_weights = compute_class_weights(train_labels, n_classes)\nprint(f'CNN class weights: {cnn_class_weights.tolist()}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train CNN\n",
    "cnn_model = AccentCNN(\n",
    "    n_classes=n_classes,\n",
    "    n_mels=cnn_cfg['n_mels'],\n",
    "    conv_channels=cnn_cfg['conv_channels'],\n",
    ")\n",
    "\n",
    "cnn_checkpoint_dir = Path(config['output']['checkpoint_dir']) / 'cnn'\n",
    "\n",
    "cnn_training_config = TrainingConfig(\n",
    "    learning_rate=cnn_cfg['training']['learning_rate'],\n",
    "    batch_size=cnn_cfg['training']['batch_size'],\n",
    "    n_epochs=cnn_cfg['training']['n_epochs'],\n",
    "    patience=cnn_cfg['training']['patience'],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    checkpoint_dir=cnn_checkpoint_dir,\n",
    "    experiment_name='accent_cnn',\n",
    "    use_amp=cnn_cfg['training']['use_amp'],\n",
    ")\n",
    "\n",
    "print(f'Training CNN: lr={cnn_training_config.learning_rate}, '\n",
    "      f'epochs={cnn_training_config.n_epochs}, '\n",
    "      f'patience={cnn_training_config.patience}')\n",
    "\n",
    "cnn_result = train_classifier(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_mel_loader,\n",
    "    val_loader=val_mel_loader,\n",
    "    config=cnn_training_config,\n",
    "    class_weights=cnn_class_weights,\n",
    ")\n",
    "\n",
    "print(f'\\nCNN training complete:')\n",
    "print(f'  Best epoch: {cnn_result.best_epoch}')\n",
    "print(f'  Best val bal_acc: {cnn_result.best_val_bal_acc:.4f}')\n",
    "print(f'  Total epochs: {cnn_result.total_epochs_run}')\n",
    "print(f'  Checkpoint: {cnn_result.best_checkpoint_path}')\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(cnn_result.train_losses, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('CNN Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(cnn_result.val_bal_accs, label='Val Balanced Accuracy', color='orange')\n",
    "ax2.axhline(y=1.0/n_classes, color='red', linestyle='--', label=f'Chance ({1.0/n_classes:.2f})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('CNN Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "figures_dir = Path(config['output']['figures_dir'])\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(figures_dir / 'cnn_training_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate CNN on test set\n# weights_only=False required: our checkpoint dict contains non-tensor metadata\n# (config, epoch, optimizer state) — safe since we saved it ourselves\ncheckpoint = torch.load(cnn_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\ncnn_model.load_state_dict(checkpoint['model_state_dict'])\n\ncnn_eval = evaluate_classifier(\n    model=cnn_model,\n    test_loader=test_mel_loader,\n    label_names=label_names,\n    device=DEVICE,\n    n_bootstrap=config['evaluation']['bootstrap_n_samples'],\n)\n\nprint('=== CNN TEST EVALUATION ===')\nprint(f'Balanced Accuracy: {cnn_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI 95%: [{cnn_eval[\"ci_95_lower\"]:.4f}, {cnn_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'F1 Macro: {cnn_eval[\"f1_macro\"]:.4f}')\nprint(f'Chance level: {1.0/n_classes:.4f}')\nprint(f'\\nPer-class recall:')\nfor name, recall in cnn_eval['per_class_recall'].items():\n    print(f'  {name}: {recall:.4f}')\n\n# Display confusion matrix\ncm = np.array(cnn_eval['confusion_matrix'])\ncm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=label_names,\n            yticklabels=label_names, cmap='Blues', ax=ax1)\nax1.set_title('CNN Confusion Matrix (counts)')\nax1.set_xlabel('Predicted')\nax1.set_ylabel('True')\n\nsns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=label_names,\n            yticklabels=label_names, cmap='Blues', ax=ax2)\nax2.set_title('CNN Confusion Matrix (row-normalized recall)')\nax2.set_xlabel('Predicted')\nax2.set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig(figures_dir / 'cnn_confusion_matrix.png', dpi=150)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. wav2vec2 Accent Classifier\n\nPre-trained wav2vec2-base with frozen CNN feature extractor + fine-tuned transformer + linear head.  \nOperates on raw waveforms (no mel-spectrogram preprocessing).  \nSmaller batch size due to VRAM constraints."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# wav2vec2 hyperparameters from config\nw2v_cfg = config['wav2vec2']\n\n# Create waveform datasets\ntrain_wav_ds = WaveformDataset(\n    entries=train_entries,\n    label_to_idx=label_to_idx,\n    max_length_s=w2v_cfg['max_length_s'],\n)\nval_wav_ds = WaveformDataset(\n    entries=val_entries,\n    label_to_idx=label_to_idx,\n    max_length_s=w2v_cfg['max_length_s'],\n)\ntest_wav_ds = WaveformDataset(\n    entries=test_entries,\n    label_to_idx=label_to_idx,\n    max_length_s=w2v_cfg['max_length_s'],\n)\n\nprint(f'Waveform datasets: train={len(train_wav_ds)}, val={len(val_wav_ds)}, test={len(test_wav_ds)}')\n\n# DataLoaders (smaller batch for wav2vec2 VRAM)\ng_w2v = torch.Generator()\ng_w2v.manual_seed(SEED)\n\nw2v_batch_size = w2v_cfg['training']['batch_size']\nw2v_num_workers = w2v_cfg['training']['num_workers']\n\ntrain_wav_loader = torch.utils.data.DataLoader(\n    train_wav_ds, batch_size=w2v_batch_size, shuffle=True,\n    num_workers=w2v_num_workers, worker_init_fn=seed_worker, generator=g_w2v, pin_memory=True,\n)\nval_wav_loader = torch.utils.data.DataLoader(\n    val_wav_ds, batch_size=w2v_batch_size, shuffle=False,\n    num_workers=w2v_num_workers, pin_memory=True,\n)\ntest_wav_loader = torch.utils.data.DataLoader(\n    test_wav_ds, batch_size=w2v_batch_size, shuffle=False,\n    num_workers=w2v_num_workers, pin_memory=True,\n)\n\n# Class weights — same train distribution, reuses train_labels computed above\nw2v_class_weights = compute_class_weights(train_labels, n_classes)\nprint(f'wav2vec2 class weights: {w2v_class_weights.tolist()}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train wav2vec2\n",
    "# Free CNN memory before loading wav2vec2\n",
    "del cnn_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "w2v_model = AccentWav2Vec2(\n",
    "    n_classes=n_classes,\n",
    "    model_name=w2v_cfg['model_name'],\n",
    "    freeze_feature_extractor=w2v_cfg['freeze_feature_extractor'],\n",
    ")\n",
    "\n",
    "w2v_checkpoint_dir = Path(config['output']['checkpoint_dir']) / 'wav2vec2'\n",
    "\n",
    "w2v_training_config = TrainingConfig(\n",
    "    learning_rate=w2v_cfg['training']['learning_rate'],\n",
    "    batch_size=w2v_cfg['training']['batch_size'],\n",
    "    n_epochs=w2v_cfg['training']['n_epochs'],\n",
    "    patience=w2v_cfg['training']['patience'],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    checkpoint_dir=w2v_checkpoint_dir,\n",
    "    experiment_name='accent_wav2vec2',\n",
    "    use_amp=w2v_cfg['training']['use_amp'],\n",
    ")\n",
    "\n",
    "print(f'Training wav2vec2: lr={w2v_training_config.learning_rate}, '\n",
    "      f'epochs={w2v_training_config.n_epochs}, '\n",
    "      f'patience={w2v_training_config.patience}')\n",
    "print(f'VRAM before training: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
    "\n",
    "w2v_result = train_classifier(\n",
    "    model=w2v_model,\n",
    "    train_loader=train_wav_loader,\n",
    "    val_loader=val_wav_loader,\n",
    "    config=w2v_training_config,\n",
    "    class_weights=w2v_class_weights,\n",
    ")\n",
    "\n",
    "print(f'\\nwav2vec2 training complete:')\n",
    "print(f'  Best epoch: {w2v_result.best_epoch}')\n",
    "print(f'  Best val bal_acc: {w2v_result.best_val_bal_acc:.4f}')\n",
    "print(f'  Total epochs: {w2v_result.total_epochs_run}')\n",
    "print(f'  Checkpoint: {w2v_result.best_checkpoint_path}')\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(w2v_result.train_losses, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('wav2vec2 Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(w2v_result.val_bal_accs, label='Val Balanced Accuracy', color='orange')\n",
    "ax2.axhline(y=1.0/n_classes, color='red', linestyle='--', label=f'Chance ({1.0/n_classes:.2f})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Balanced Accuracy')\n",
    "ax2.set_title('wav2vec2 Validation Balanced Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'wav2vec2_training_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate wav2vec2 on test set\n# weights_only=False required: our checkpoint dict contains non-tensor metadata\n# (config, epoch, optimizer state) — safe since we saved it ourselves\ncheckpoint_w2v = torch.load(w2v_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\nw2v_model.load_state_dict(checkpoint_w2v['model_state_dict'])\n\nw2v_eval = evaluate_classifier(\n    model=w2v_model,\n    test_loader=test_wav_loader,\n    label_names=label_names,\n    device=DEVICE,\n    n_bootstrap=config['evaluation']['bootstrap_n_samples'],\n)\n\nprint('=== WAV2VEC2 TEST EVALUATION ===')\nprint(f'Balanced Accuracy: {w2v_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI 95%: [{w2v_eval[\"ci_95_lower\"]:.4f}, {w2v_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'F1 Macro: {w2v_eval[\"f1_macro\"]:.4f}')\nprint(f'Chance level: {1.0/n_classes:.4f}')\nprint(f'\\nPer-class recall:')\nfor name, recall in w2v_eval['per_class_recall'].items():\n    print(f'  {name}: {recall:.4f}')\n\n# Display confusion matrix\ncm_w2v = np.array(w2v_eval['confusion_matrix'])\ncm_w2v_norm = cm_w2v.astype(float) / cm_w2v.sum(axis=1, keepdims=True)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(cm_w2v, annot=True, fmt='d', xticklabels=label_names,\n            yticklabels=label_names, cmap='Greens', ax=ax1)\nax1.set_title('wav2vec2 Confusion Matrix (counts)')\nax1.set_xlabel('Predicted')\nax1.set_ylabel('True')\n\nsns.heatmap(cm_w2v_norm, annot=True, fmt='.2f', xticklabels=label_names,\n            yticklabels=label_names, cmap='Greens', ax=ax2)\nax2.set_title('wav2vec2 Confusion Matrix (row-normalized recall)')\nax2.set_xlabel('Predicted')\nax2.set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig(figures_dir / 'wav2vec2_confusion_matrix.png', dpi=150)\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Robustness Check (Multiple Seeds)\n\nProtocolo requer mínimo 3 seeds para claims válidos (`experiment-protocol.md`).  \nRetreina ambos os classifiers com seeds [42, 1337, 7] e reporta média ± std da balanced accuracy.  \nSe diferença entre CNN e wav2vec2 for menor que o desvio, NÃO afirmar superioridade.\n\n**Nota:** esta seção retreina 2 modelos × 3 seeds = 6 treinamentos completos.  \nEstimativa: ~2-4h em GPU (CNN ~10min/run, wav2vec2 ~30-40min/run).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "ROBUSTNESS_SEEDS = config['seed']['robustness_seeds']\nprint(f'=== ROBUSTNESS CHECK (seeds: {ROBUSTNESS_SEEDS}) ===')\nprint(f'Training {len(ROBUSTNESS_SEEDS)} seeds x 2 models = {len(ROBUSTNESS_SEEDS)*2} runs\\n')\n\n# Free wav2vec2 model before robustness runs\ndel w2v_model\ntorch.cuda.empty_cache()\n\nrobustness_results = {'cnn': [], 'wav2vec2': []}\n\nfor s in ROBUSTNESS_SEEDS:\n    print(f'--- Seed {s} ---')\n    set_global_seed(s)\n\n    # CNN: retrain from scratch with this seed\n    g_rob = torch.Generator()\n    g_rob.manual_seed(s)\n\n    rob_train_loader = torch.utils.data.DataLoader(\n        train_mel_ds, batch_size=cnn_batch_size, shuffle=True,\n        num_workers=cnn_num_workers, worker_init_fn=seed_worker, generator=g_rob, pin_memory=True,\n    )\n\n    rob_cnn = AccentCNN(\n        n_classes=n_classes, n_mels=cnn_cfg['n_mels'],\n        conv_channels=cnn_cfg['conv_channels'],\n    )\n    rob_cnn_config = TrainingConfig(\n        learning_rate=cnn_cfg['training']['learning_rate'],\n        batch_size=cnn_cfg['training']['batch_size'],\n        n_epochs=cnn_cfg['training']['n_epochs'],\n        patience=cnn_cfg['training']['patience'],\n        device=DEVICE, seed=s,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / f'cnn_seed{s}',\n        experiment_name=f'accent_cnn_seed{s}',\n        use_amp=cnn_cfg['training']['use_amp'],\n    )\n    rob_cnn_result = train_classifier(rob_cnn, rob_train_loader, val_mel_loader, rob_cnn_config, cnn_class_weights)\n\n    # weights_only=False: checkpoint contains non-tensor metadata (safe, self-saved)\n    ckpt = torch.load(rob_cnn_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    rob_cnn.load_state_dict(ckpt['model_state_dict'])\n    rob_cnn_eval = evaluate_classifier(rob_cnn, test_mel_loader, label_names, DEVICE, n_bootstrap=0)\n    robustness_results['cnn'].append(rob_cnn_eval['balanced_accuracy'])\n    print(f'  CNN: bal_acc={rob_cnn_eval[\"balanced_accuracy\"]:.4f}')\n    del rob_cnn\n    torch.cuda.empty_cache()\n\n    # wav2vec2: retrain from scratch with this seed\n    g_rob_w2v = torch.Generator()\n    g_rob_w2v.manual_seed(s)\n\n    rob_wav_loader = torch.utils.data.DataLoader(\n        train_wav_ds, batch_size=w2v_batch_size, shuffle=True,\n        num_workers=w2v_num_workers, worker_init_fn=seed_worker, generator=g_rob_w2v, pin_memory=True,\n    )\n\n    rob_w2v = AccentWav2Vec2(\n        n_classes=n_classes, model_name=w2v_cfg['model_name'],\n        freeze_feature_extractor=w2v_cfg['freeze_feature_extractor'],\n    )\n    rob_w2v_config = TrainingConfig(\n        learning_rate=w2v_cfg['training']['learning_rate'],\n        batch_size=w2v_cfg['training']['batch_size'],\n        n_epochs=w2v_cfg['training']['n_epochs'],\n        patience=w2v_cfg['training']['patience'],\n        device=DEVICE, seed=s,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / f'wav2vec2_seed{s}',\n        experiment_name=f'accent_wav2vec2_seed{s}',\n        use_amp=w2v_cfg['training']['use_amp'],\n    )\n    rob_w2v_result = train_classifier(rob_w2v, rob_wav_loader, val_wav_loader, rob_w2v_config, w2v_class_weights)\n\n    # weights_only=False: checkpoint contains non-tensor metadata (safe, self-saved)\n    ckpt_w = torch.load(rob_w2v_result.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    rob_w2v.load_state_dict(ckpt_w['model_state_dict'])\n    rob_w2v_eval = evaluate_classifier(rob_w2v, test_wav_loader, label_names, DEVICE, n_bootstrap=0)\n    robustness_results['wav2vec2'].append(rob_w2v_eval['balanced_accuracy'])\n    print(f'  wav2vec2: bal_acc={rob_w2v_eval[\"balanced_accuracy\"]:.4f}')\n    del rob_w2v\n    torch.cuda.empty_cache()\n\n# Summary\nprint(f'\\n=== ROBUSTNESS SUMMARY ({len(ROBUSTNESS_SEEDS)} seeds) ===')\nfor model_name, accs in robustness_results.items():\n    mean_acc = np.mean(accs)\n    std_acc = np.std(accs)\n    print(f'  {model_name}: {mean_acc:.4f} +/- {std_acc:.4f}  '\n          f'(seeds: {dict(zip(ROBUSTNESS_SEEDS, [f\"{a:.4f}\" for a in accs]))})')\n\n# Check if difference is meaningful given the variance\ncnn_mean = np.mean(robustness_results['cnn'])\nw2v_mean = np.mean(robustness_results['wav2vec2'])\npooled_std = np.sqrt(np.std(robustness_results['cnn'])**2 + np.std(robustness_results['wav2vec2'])**2)\nif pooled_std > 0 and abs(cnn_mean - w2v_mean) < pooled_std:\n    print(f'\\nDifference ({abs(cnn_mean - w2v_mean):.4f}) < pooled std ({pooled_std:.4f})')\n    print('Cannot claim one model is superior based on seed variance.')\nelse:\n    winner = 'wav2vec2' if w2v_mean > cnn_mean else 'CNN'\n    print(f'\\n{winner} appears consistently better across seeds.')\n\n# Restore original seed\nset_global_seed(SEED)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Cross-Source Evaluation (Source Confound Check)\n\n**Objetivo:** verificar se o classificador aprendeu *sotaque* ou *fonte*.  \n- Treinamos na fonte A (CORAA-MUPE), testamos na fonte B (Common Voice) e vice-versa.  \n- Se ambas as direcoes ficam em chance level -> classificador aprendeu source, nao accent.  \n- Se ao menos uma direcao fica acima de chance -> sinal de accent e transferivel entre fontes.  \n\nUsamos o CNN (mais rapido) para este cross-source check."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cross-source threshold from config (percentage points above chance)\nabove_chance_margin = config['cross_source']['above_chance_margin_pp'] / 100\n\n# Check that primary classifier is above chance before cross-source\nchance = 1.0 / n_classes\nbest_primary = max(cnn_eval['balanced_accuracy'], w2v_eval['balanced_accuracy'])\nif best_primary <= chance + above_chance_margin:\n    print(f'WARNING: Best primary classifier ({best_primary:.4f}) is at chance ({chance:.4f}).')\n    print('Cross-source evaluation may not be meaningful.')\n\n# Split combined entries by source\ncoraa_train = [e for e in train_entries if e.source == 'CORAA-MUPE']\ncoraa_test_split = [e for e in test_entries if e.source == 'CORAA-MUPE']\ncv_train = [e for e in train_entries if e.source == 'CommonVoice-PT']\ncv_test_split = [e for e in test_entries if e.source == 'CommonVoice-PT']\n\nprint(f'CORAA-MUPE: train={len(coraa_train)}, test={len(coraa_test_split)}')\nprint(f'CommonVoice-PT: train={len(cv_train)}, test={len(cv_test_split)}')\n\ncross_source_results = {}\n\n# Direction 1: Train on CORAA-MUPE, test on CommonVoice-PT\nif len(coraa_train) > 0 and len(cv_test_split) > 0:\n    print('\\n--- Direction 1: Train CORAA-MUPE -> Test CommonVoice-PT ---')\n\n    cs_train_ds = MelSpectrogramDataset(\n        coraa_train, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n    cs_test_ds = MelSpectrogramDataset(\n        cv_test_split, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    # Validation: use same-source val split (never train data)\n    coraa_val = [e for e in val_entries if e.source == 'CORAA-MUPE']\n    if len(coraa_val) < 10:\n        # Not enough val samples from this source — use full val split as fallback\n        # This is safe: val is speaker-disjoint from train, just mixes sources\n        coraa_val = list(val_entries)\n        print(f'  Val fallback: using full val split ({len(coraa_val)} entries, mixed source)')\n\n    cs_val_ds = MelSpectrogramDataset(\n        coraa_val, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    g_cs1 = torch.Generator()\n    g_cs1.manual_seed(SEED)\n\n    cs_train_loader = torch.utils.data.DataLoader(\n        cs_train_ds, batch_size=cnn_batch_size, shuffle=True,\n        num_workers=cnn_num_workers, worker_init_fn=seed_worker, generator=g_cs1, pin_memory=True,\n    )\n    cs_val_loader = torch.utils.data.DataLoader(\n        cs_val_ds, batch_size=cnn_batch_size, shuffle=False, num_workers=cnn_num_workers, pin_memory=True,\n    )\n    cs_test_loader = torch.utils.data.DataLoader(\n        cs_test_ds, batch_size=cnn_batch_size, shuffle=False, num_workers=cnn_num_workers, pin_memory=True,\n    )\n\n    cs_labels = [label_to_idx[e.accent] for e in coraa_train]\n    cs_weights = compute_class_weights(cs_labels, n_classes)\n\n    cs_model_1 = AccentCNN(n_classes=n_classes, n_mels=cnn_cfg['n_mels'], conv_channels=cnn_cfg['conv_channels'])\n    cs_config_1 = TrainingConfig(\n        learning_rate=cnn_cfg['training']['learning_rate'],\n        batch_size=cnn_batch_size, n_epochs=cnn_cfg['training']['n_epochs'],\n        patience=cnn_cfg['training']['patience'], device=DEVICE, seed=SEED,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / 'cross_source_coraa2cv',\n        experiment_name='cross_source_coraa2cv', use_amp=cnn_cfg['training']['use_amp'],\n    )\n\n    cs_result_1 = train_classifier(cs_model_1, cs_train_loader, cs_val_loader, cs_config_1, cs_weights)\n\n    # weights_only=False: checkpoint contains non-tensor metadata (safe, self-saved)\n    checkpoint_cs1 = torch.load(cs_result_1.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    cs_model_1.load_state_dict(checkpoint_cs1['model_state_dict'])\n\n    cs_eval_1 = evaluate_classifier(cs_model_1, cs_test_loader, label_names, DEVICE)\n    cross_source_results['coraa2cv'] = cs_eval_1\n\n    print(f'CORAA->CV bal_acc: {cs_eval_1[\"balanced_accuracy\"]:.4f} '\n          f'(CI: [{cs_eval_1[\"ci_95_lower\"]:.4f}, {cs_eval_1[\"ci_95_upper\"]:.4f}])')\n\n    del cs_model_1\n    torch.cuda.empty_cache()\nelse:\n    print('Skipping direction 1: insufficient data')\n\n# Direction 2: Train on CommonVoice-PT, test on CORAA-MUPE\nif len(cv_train) > 0 and len(coraa_test_split) > 0:\n    print('\\n--- Direction 2: Train CommonVoice-PT -> Test CORAA-MUPE ---')\n\n    cs_train_ds2 = MelSpectrogramDataset(\n        cv_train, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n    cs_test_ds2 = MelSpectrogramDataset(\n        coraa_test_split, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    # Validation: use same-source val split (never train data)\n    cv_val = [e for e in val_entries if e.source == 'CommonVoice-PT']\n    if len(cv_val) < 10:\n        cv_val = list(val_entries)\n        print(f'  Val fallback: using full val split ({len(cv_val)} entries, mixed source)')\n\n    cs_val_ds2 = MelSpectrogramDataset(\n        cv_val, label_to_idx, n_mels=cnn_cfg['n_mels'], max_frames=cnn_cfg['max_frames'],\n    )\n\n    g_cs2 = torch.Generator()\n    g_cs2.manual_seed(SEED)\n\n    cs_train_loader2 = torch.utils.data.DataLoader(\n        cs_train_ds2, batch_size=cnn_batch_size, shuffle=True,\n        num_workers=cnn_num_workers, worker_init_fn=seed_worker, generator=g_cs2, pin_memory=True,\n    )\n    cs_val_loader2 = torch.utils.data.DataLoader(\n        cs_val_ds2, batch_size=cnn_batch_size, shuffle=False, num_workers=cnn_num_workers, pin_memory=True,\n    )\n    cs_test_loader2 = torch.utils.data.DataLoader(\n        cs_test_ds2, batch_size=cnn_batch_size, shuffle=False, num_workers=cnn_num_workers, pin_memory=True,\n    )\n\n    cs_labels2 = [label_to_idx[e.accent] for e in cv_train]\n    cs_weights2 = compute_class_weights(cs_labels2, n_classes)\n\n    cs_model_2 = AccentCNN(n_classes=n_classes, n_mels=cnn_cfg['n_mels'], conv_channels=cnn_cfg['conv_channels'])\n    cs_config_2 = TrainingConfig(\n        learning_rate=cnn_cfg['training']['learning_rate'],\n        batch_size=cnn_batch_size, n_epochs=cnn_cfg['training']['n_epochs'],\n        patience=cnn_cfg['training']['patience'], device=DEVICE, seed=SEED,\n        checkpoint_dir=Path(config['output']['checkpoint_dir']) / 'cross_source_cv2coraa',\n        experiment_name='cross_source_cv2coraa', use_amp=cnn_cfg['training']['use_amp'],\n    )\n\n    cs_result_2 = train_classifier(cs_model_2, cs_train_loader2, cs_val_loader2, cs_config_2, cs_weights2)\n\n    # weights_only=False: checkpoint contains non-tensor metadata (safe, self-saved)\n    checkpoint_cs2 = torch.load(cs_result_2.best_checkpoint_path, map_location=DEVICE, weights_only=False)\n    cs_model_2.load_state_dict(checkpoint_cs2['model_state_dict'])\n\n    cs_eval_2 = evaluate_classifier(cs_model_2, cs_test_loader2, label_names, DEVICE)\n    cross_source_results['cv2coraa'] = cs_eval_2\n\n    print(f'CV->CORAA bal_acc: {cs_eval_2[\"balanced_accuracy\"]:.4f} '\n          f'(CI: [{cs_eval_2[\"ci_95_lower\"]:.4f}, {cs_eval_2[\"ci_95_upper\"]:.4f}])')\n\n    del cs_model_2\n    torch.cuda.empty_cache()\nelse:\n    print('Skipping direction 2: insufficient data')\n\n# Interpretation — threshold from config\nprint(f'\\n=== CROSS-SOURCE SUMMARY ===')\nprint(f'Chance level: {chance:.4f}')\nprint(f'Above-chance margin: {config[\"cross_source\"][\"above_chance_margin_pp\"]}pp')\nfor direction, eval_result in cross_source_results.items():\n    ba = eval_result['balanced_accuracy']\n    above_chance = ba > chance + above_chance_margin\n    status = 'ABOVE CHANCE (accent signal transfers)' if above_chance else 'AT CHANCE (possible source confound)'\n    print(f'  {direction}: bal_acc={ba:.4f} -> {status}')\n\nif all(r['balanced_accuracy'] <= chance + above_chance_margin for r in cross_source_results.values()):\n    print('\\nWARNING: Both directions at chance. Classifier may have learned source, not accent.')\nelse:\n    print('\\nAt least one direction shows transfer. Accent signal appears generalizable across sources.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Ablation Summary\n\nComparison table: CNN vs wav2vec2, with balanced accuracy, CI 95%, F1 macro, and cross-source results.  \nAll metrics follow the protocol: balanced accuracy (primary), CI 95% (bootstrap, 1000 samples)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build comparison table\n",
    "chance = 1.0 / n_classes\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['CNN (mel-spectrogram)', 'wav2vec2-base'],\n",
    "    'Balanced Accuracy': [\n",
    "        f'{cnn_eval[\"balanced_accuracy\"]:.4f}',\n",
    "        f'{w2v_eval[\"balanced_accuracy\"]:.4f}',\n",
    "    ],\n",
    "    'CI 95% Lower': [\n",
    "        f'{cnn_eval[\"ci_95_lower\"]:.4f}',\n",
    "        f'{w2v_eval[\"ci_95_lower\"]:.4f}',\n",
    "    ],\n",
    "    'CI 95% Upper': [\n",
    "        f'{cnn_eval[\"ci_95_upper\"]:.4f}',\n",
    "        f'{w2v_eval[\"ci_95_upper\"]:.4f}',\n",
    "    ],\n",
    "    'F1 Macro': [\n",
    "        f'{cnn_eval[\"f1_macro\"]:.4f}',\n",
    "        f'{w2v_eval[\"f1_macro\"]:.4f}',\n",
    "    ],\n",
    "    'Best Epoch': [\n",
    "        cnn_result.best_epoch,\n",
    "        w2v_result.best_epoch,\n",
    "    ],\n",
    "    'Total Epochs': [\n",
    "        cnn_result.total_epochs_run,\n",
    "        w2v_result.total_epochs_run,\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('=== ABLATION: CNN vs wav2vec2 ===')\n",
    "print(f'Chance level: {chance:.4f}')\n",
    "print()\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Check if CIs overlap (cannot claim one is better)\n",
    "cnn_ci = (cnn_eval['ci_95_lower'], cnn_eval['ci_95_upper'])\n",
    "w2v_ci = (w2v_eval['ci_95_lower'], w2v_eval['ci_95_upper'])\n",
    "\n",
    "overlap = cnn_ci[0] <= w2v_ci[1] and w2v_ci[0] <= cnn_ci[1]\n",
    "if overlap:\n",
    "    print('\\nCIs overlap -> cannot claim one model is superior.')\n",
    "else:\n",
    "    winner = 'wav2vec2' if w2v_eval['balanced_accuracy'] > cnn_eval['balanced_accuracy'] else 'CNN'\n",
    "    print(f'\\nCIs do NOT overlap -> {winner} is significantly better.')\n",
    "\n",
    "# Cross-source results table\n",
    "if cross_source_results:\n",
    "    print('\\n=== CROSS-SOURCE EVALUATION ===')\n",
    "    for direction, result in cross_source_results.items():\n",
    "        print(f'  {direction}: bal_acc={result[\"balanced_accuracy\"]:.4f} '\n",
    "              f'(CI: [{result[\"ci_95_lower\"]:.4f}, {result[\"ci_95_upper\"]:.4f}])')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Save full report as JSON with all metrics, configs, and hashes\nfrom datetime import datetime\n\n# Provenance — uses src.utils.git (imported in cell-2)\ncommit_hash = get_commit_hash()\n\n# SHA-256 of the combined manifest\nCOMBINED_MANIFEST_PATH = DRIVE_BASE / 'accents_pt_br' / 'manifest.jsonl'\nmanifest_sha256 = compute_file_hash(COMBINED_MANIFEST_PATH) if COMBINED_MANIFEST_PATH.exists() else 'N/A'\n\nreport = {\n    'experiment': config['experiment']['name'],\n    'date': datetime.now().isoformat(),\n    'commit_hash': commit_hash,\n    'seed': SEED,\n    'environment': {\n        'python_version': sys.version,\n        'torch_version': torch.__version__,\n        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n    },\n    'dataset': {\n        'name': config['dataset']['name'],\n        'combined_manifest_sha256': manifest_sha256,\n        'total_entries': len(combined_entries),\n        'total_speakers': len({e.speaker_id for e in combined_entries}),\n        'n_classes': n_classes,\n        'label_names': label_names,\n        'per_source': dict(Counter(e.source for e in combined_entries)),\n        'per_accent': dict(Counter(e.accent for e in combined_entries)),\n    },\n    'splits': {\n        'method': config['splits']['method'],\n        'ratios': config['splits']['ratios'],\n        'seed': config['splits']['seed'],\n        'train_utterances': len(train_entries),\n        'val_utterances': len(val_entries),\n        'test_utterances': len(test_entries),\n        'train_speakers': len(split_info.train_speakers),\n        'val_speakers': len(split_info.val_speakers),\n        'test_speakers': len(split_info.test_speakers),\n    },\n    'confounds': [\n        {\n            'test': r.test_name,\n            'variables': f'{r.variable_a} x {r.variable_b}',\n            'statistic': r.statistic,\n            'p_value': r.p_value,\n            'effect_size': r.effect_size,\n            'effect_size_name': r.effect_size_name,\n            'is_blocking': r.is_blocking,\n            'is_significant': r.is_significant,\n            'interpretation': r.interpretation,\n        }\n        for r in confound_results\n    ],\n    'cnn': {\n        'config': {\n            'n_mels': cnn_cfg['n_mels'],\n            'max_frames': cnn_cfg['max_frames'],\n            'conv_channels': cnn_cfg['conv_channels'],\n            'learning_rate': cnn_cfg['training']['learning_rate'],\n            'batch_size': cnn_cfg['training']['batch_size'],\n            'n_epochs': cnn_cfg['training']['n_epochs'],\n            'patience': cnn_cfg['training']['patience'],\n        },\n        'training': {\n            'best_epoch': cnn_result.best_epoch,\n            'best_val_bal_acc': cnn_result.best_val_bal_acc,\n            'total_epochs_run': cnn_result.total_epochs_run,\n            'checkpoint_path': str(cnn_result.best_checkpoint_path),\n        },\n        'evaluation': cnn_eval,\n    },\n    'wav2vec2': {\n        'config': {\n            'model_name': w2v_cfg['model_name'],\n            'freeze_feature_extractor': w2v_cfg['freeze_feature_extractor'],\n            'max_length_s': w2v_cfg['max_length_s'],\n            'learning_rate': w2v_cfg['training']['learning_rate'],\n            'batch_size': w2v_cfg['training']['batch_size'],\n            'n_epochs': w2v_cfg['training']['n_epochs'],\n            'patience': w2v_cfg['training']['patience'],\n        },\n        'training': {\n            'best_epoch': w2v_result.best_epoch,\n            'best_val_bal_acc': w2v_result.best_val_bal_acc,\n            'total_epochs_run': w2v_result.total_epochs_run,\n            'checkpoint_path': str(w2v_result.best_checkpoint_path),\n        },\n        'evaluation': w2v_eval,\n    },\n    'robustness': robustness_results,\n    'cross_source': cross_source_results,\n    'chance_level': chance,\n}\n\n# Save report\nreport_dir = Path(config['output']['report_dir'])\nreport_dir.mkdir(parents=True, exist_ok=True)\nreport_path = Path(config['output']['report_json'])\n\nwith open(report_path, 'w') as f:\n    json.dump(report, f, indent=2, default=str)\n\nprint(f'Report saved to: {report_path}')\nprint(f'Combined manifest SHA-256: {manifest_sha256}')\nprint(f'Commit hash: {commit_hash}')\nprint(f'\\n=== EXPERIMENT COMPLETE ===')\nprint(f'CNN bal_acc: {cnn_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI: [{cnn_eval[\"ci_95_lower\"]:.4f}, {cnn_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'wav2vec2 bal_acc: {w2v_eval[\"balanced_accuracy\"]:.4f} '\n      f'(CI: [{w2v_eval[\"ci_95_lower\"]:.4f}, {w2v_eval[\"ci_95_upper\"]:.4f}])')\nprint(f'Chance level: {chance:.4f}')",
   "outputs": [],
   "execution_count": null
  }
 ]
}