{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1.5 â€” Latent Separability Audit\n",
    "\n",
    "**Projeto:** Controle ExplÃ­cito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Verificar se representaÃ§Ãµes internas do Qwen3-TTS codificam informaÃ§Ã£o suficiente de sotaque regional para classificaÃ§Ã£o acima de chance, com leakage controlado.  \n",
    "**Backbone:** Qwen3-TTS 1.7B-CustomVoice (frozen)  \n",
    "**Dataset:** CORAA-MUPE (speaker-disjoint splits)  \n",
    "\n",
    "Este notebook Ã© a **camada de orquestraÃ§Ã£o**. Toda lÃ³gica estÃ¡ em `src/` (testÃ¡vel, auditÃ¡vel).  \n",
    "O notebook apenas: instala deps â†’ configura ambiente â†’ chama mÃ³dulos â†’ exibe resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, subprocess, sys\n\nREPO_DIR = '/content/TCC'\n\n# 1. Clone repo (idempotent â€” skips if already cloned)\nif not os.path.exists(os.path.join(REPO_DIR, '.git')):\n    !rm -rf {REPO_DIR}\n    !git clone https://github.com/paulohenriquevn/tcc.git {REPO_DIR}\n\nos.chdir(REPO_DIR)\n!pip install -r requirements.txt -q\n\n# 2. NumPy ABI check â€” Colab pre-loads numpy 2.x in memory, but\n#    requirements.txt pins 1.26.4. After pip downgrades the on-disk\n#    files, stale C-extensions cause:\n#      \"numpy.dtype size changed, may indicate binary incompatibility\"\n#    Fix: restart runtime ONCE. After restart both match â†’ no loop.\n_installed_np = subprocess.check_output(\n    [sys.executable, '-c', 'import numpy; print(numpy.__version__)'],\n    text=True,\n).strip()\n\ntry:\n    import numpy as _np\n    _loaded_np = _np.__version__\nexcept Exception:\n    _loaded_np = None\n\nif _loaded_np != _installed_np:\n    print(f'\\nNumPy ABI mismatch: loaded={_loaded_np}, installed={_installed_np}')\n    print('Restarting runtime... After restart, re-run this cell (no second restart).')\n    os.kill(os.getpid(), 9)\nelse:\n    print(f'\\nEnvironment OK (numpy=={_installed_np})')"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-282210731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_global_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Seed global configurado: {SEED}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/TCC/src/utils/seed.py\u001b[0m in \u001b[0;36mset_global_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m     28\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Seeds e determinismo â€” OBRIGATÃ“RIO antes de qualquer operaÃ§Ã£o\n",
    "from src.utils.seed import set_global_seed\n",
    "\n",
    "SEED = 42\n",
    "generator = set_global_seed(SEED)\n",
    "print(f'Seed global configurado: {SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU e versÃµes\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'VRAM total: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'\\nUsando device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar config\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_PATH = Path('configs/stage1_5.yaml')\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Experiment: {config['experiment']['name']}\")\n",
    "print(f\"Seed: {config['seed']['global']}\")\n",
    "print(f\"Dataset: {config['dataset']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download e Build Manifest\n",
    "\n",
    "Baixa o CORAA-MUPE-ASR do HuggingFace, aplica filtros e constrÃ³i o manifest JSONL.  \n",
    "\n",
    "**Filtros aplicados:**  \n",
    "- `speaker_type='R'` (apenas entrevistados, nÃ£o entrevistadores)  \n",
    "- DuraÃ§Ã£o: 3â€“15s  \n",
    "- `birth_state` vÃ¡lido â†’ macro-regiÃ£o IBGE (N, NE, CO, SE, S)  \n",
    "- GÃªnero: M ou F  \n",
    "\n",
    "**Nota:** O download inicial Ã© ~42 GB. Runs subsequentes usam o cache do HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "print('Downloading CORAA-MUPE-ASR from HuggingFace...')\n",
    "print('(~42 GB na primeira vez â€” usa cache nas prÃ³ximas execuÃ§Ãµes)\\n')\n",
    "\n",
    "ds = load_dataset(\"nilc-nlp/CORAA-MUPE-ASR\")\n",
    "\n",
    "print(f'Splits disponÃ­veis: {list(ds.keys())}')\n",
    "for split_name, split_data in ds.items():\n",
    "    print(f'  {split_name}: {len(split_data):,} rows')\n",
    "\n",
    "# Concatenar todos os splits â€” criaremos nossos prÃ³prios splits speaker-disjoint\n",
    "all_data = concatenate_datasets([ds[split] for split in ds.keys()])\n",
    "print(f'\\nTotal concatenado: {len(all_data):,} rows')\n",
    "print(f'Colunas: {all_data.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.manifest_builder import build_manifest_from_hf_dataset\n",
    "\n",
    "AUDIO_DIR = Path('data/audio/')\n",
    "MANIFEST_PATH = Path(config['dataset']['manifest_path'])\n",
    "\n",
    "entries, build_stats = build_manifest_from_hf_dataset(\n",
    "    dataset=all_data,\n",
    "    audio_output_dir=AUDIO_DIR,\n",
    "    manifest_output_path=MANIFEST_PATH,\n",
    "    speaker_type_filter=config['dataset']['filters']['speaker_type'],\n",
    "    min_duration_s=config['dataset']['filters']['min_duration_s'],\n",
    "    max_duration_s=config['dataset']['filters']['max_duration_s'],\n",
    "    min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    ")\n",
    "\n",
    "print(f\"\\nManifest: {len(entries):,} entries\")\n",
    "print(f\"SHA-256: {build_stats['manifest_sha256']}\")\n",
    "print(f\"\\nFilter stats:\")\n",
    "for key, count in build_stats['filter_stats'].items():\n",
    "    print(f\"  {key}: {count:,}\")\n",
    "print(f\"\\nRegiÃµes:\")\n",
    "for region, info in build_stats['regions'].items():\n",
    "    print(f\"  {region}: {info['n_speakers']} speakers, {info['n_utterances']:,} utterances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speaker-Disjoint Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.splits import (\n",
    "    generate_speaker_disjoint_splits,\n",
    "    save_splits,\n",
    "    assign_entries_to_splits,\n",
    ")\n",
    "\n",
    "split_info = generate_speaker_disjoint_splits(\n",
    "    entries,\n",
    "    train_ratio=config['splits']['ratios']['train'],\n",
    "    val_ratio=config['splits']['ratios']['val'],\n",
    "    test_ratio=config['splits']['ratios']['test'],\n",
    "    seed=config['splits']['seed'],\n",
    ")\n",
    "\n",
    "# Persistir splits\n",
    "split_path = save_splits(split_info, Path(config['splits']['output_dir']))\n",
    "print(f\"Splits salvos em: {split_path}\")\n",
    "print(f\"Train: {len(split_info.train_speakers)} speakers, {split_info.utterances_per_split['train']} utts\")\n",
    "print(f\"Val:   {len(split_info.val_speakers)} speakers, {split_info.utterances_per_split['val']} utts\")\n",
    "print(f\"Test:  {len(split_info.test_speakers)} speakers, {split_info.utterances_per_split['test']} utts\")\n",
    "\n",
    "# Assign entries\n",
    "split_entries = assign_entries_to_splits(entries, split_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AnÃ¡lise de Confounds\n",
    "\n",
    "**Sanity checks obrigatÃ³rios** (recomendaÃ§Ã£o do mentor):  \n",
    "- Tabela accent Ã— gender com chi-quadrado + Cramer's V  \n",
    "- Histograma de duraÃ§Ã£o por regiÃ£o + Kruskal-Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.confounds import run_all_confound_checks\n",
    "import pandas as pd\n",
    "\n",
    "confound_results = run_all_confound_checks(\n",
    "    entries,\n",
    "    gender_blocking_threshold=config['evaluation']['confounds']['accent_x_gender']['threshold_blocker'],\n",
    "    duration_practical_diff_s=config['evaluation']['confounds']['accent_x_duration']['practical_diff_s'],\n",
    ")\n",
    "\n",
    "print(\"=== CONFOUND ANALYSIS ===\")\n",
    "for result in confound_results:\n",
    "    status = 'ðŸ”´ BLOCKING' if result.is_blocking else ('ðŸŸ¡ SIGNIFICANT' if result.is_significant else 'ðŸŸ¢ OK')\n",
    "    print(f\"\\n{result.variable_a} Ã— {result.variable_b}: {status}\")\n",
    "    print(f\"  Test: {result.test_name}\")\n",
    "    print(f\"  Statistic: {result.statistic:.4f}\")\n",
    "    print(f\"  p-value: {result.p_value:.6f}\")\n",
    "    print(f\"  Effect size ({result.effect_size_name}): {result.effect_size:.4f}\")\n",
    "    print(f\"  Interpretation: {result.interpretation}\")\n",
    "\n",
    "# Tabela accent x gender\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in entries],\n",
    "    [e.gender for e in entries],\n",
    "    margins=True,\n",
    ")\n",
    "print(\"\\n=== ACCENT Ã— GENDER TABLE ===\")\n",
    "print(gender_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de duraÃ§Ã£o por regiÃ£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "durations_df = pd.DataFrame([\n",
    "    {'accent': e.accent, 'duration_s': e.duration_s}\n",
    "    for e in entries\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.boxplot(data=durations_df, x='accent', y='duration_s', ax=ax,\n",
    "            order=sorted(durations_df['accent'].unique()))\n",
    "ax.set_title('Duration by Accent Region')\n",
    "ax.set_xlabel('Macro-Region (IBGE)')\n",
    "ax.set_ylabel('Duration (s)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/duration_by_accent.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "Extrai features de 4 fontes:\n",
    "1. **AcÃºsticas** (MFCC, pitch, energy, speech rate) â€” CPU\n",
    "2. **ECAPA-TDNN** (speaker embeddings, 192-dim) â€” CPU/GPU\n",
    "3. **WavLM** (SSL features, 5 camadas) â€” GPU recomendado\n",
    "4. **Qwen3-TTS** (backbone features, 8 camadas) â€” GPU obrigatÃ³rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from src.features.acoustic import extract_acoustic_features, features_to_vector\n",
    "from src.features.ecapa import extract_ecapa_embedding\n",
    "\n",
    "# 4.1 Acoustic features (CPU, fast)\n",
    "print('=== Extracting acoustic features ===')\n",
    "acoustic_vectors = {}\n",
    "for entry in tqdm(entries, desc='Acoustic'):\n",
    "    feats = extract_acoustic_features(\n",
    "        Path(entry.audio_path), entry.utt_id,\n",
    "        n_mfcc=config['features']['acoustic']['n_mfcc'],\n",
    "    )\n",
    "    acoustic_vectors[entry.utt_id] = features_to_vector(feats)\n",
    "\n",
    "print(f'Extracted {len(acoustic_vectors)} acoustic feature vectors')\n",
    "print(f'Dimension: {next(iter(acoustic_vectors.values())).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 ECAPA-TDNN speaker embeddings\n",
    "print('=== Extracting ECAPA embeddings ===')\n",
    "ecapa_embeddings = {}\n",
    "for entry in tqdm(entries, desc='ECAPA'):\n",
    "    emb = extract_ecapa_embedding(Path(entry.audio_path), device=DEVICE)\n",
    "    ecapa_embeddings[entry.utt_id] = emb\n",
    "\n",
    "print(f'Extracted {len(ecapa_embeddings)} ECAPA embeddings')\n",
    "print(f'Dimension: {next(iter(ecapa_embeddings.values())).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 WavLM SSL features (layer-wise)\n",
    "from src.features.ssl import extract_ssl_features\n",
    "\n",
    "SSL_LAYERS = config['features']['ssl']['layers']\n",
    "print(f'=== Extracting WavLM features (layers {SSL_LAYERS}) ===')\n",
    "\n",
    "ssl_features = {layer: {} for layer in SSL_LAYERS}  # {layer: {utt_id: vector}}\n",
    "for entry in tqdm(entries, desc='WavLM'):\n",
    "    layer_feats = extract_ssl_features(\n",
    "        Path(entry.audio_path),\n",
    "        layers=SSL_LAYERS,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for layer_idx, feat_vec in layer_feats.items():\n",
    "        ssl_features[layer_idx][entry.utt_id] = feat_vec\n",
    "\n",
    "print(f'WavLM extraction complete')\n",
    "for layer in SSL_LAYERS:\n",
    "    dim = next(iter(ssl_features[layer].values())).shape\n",
    "    print(f'  Layer {layer}: {len(ssl_features[layer])} vectors, dim={dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Qwen3-TTS backbone features (layer-wise) â€” GPU required\n",
    "from src.features.backbone import extract_backbone_features\n",
    "\n",
    "BACKBONE_LAYERS = config['features']['backbone']['layers']\n",
    "print(f'=== Extracting backbone features (layers {BACKBONE_LAYERS}) ===')\n",
    "\n",
    "backbone_features = {layer: {} for layer in BACKBONE_LAYERS}\n",
    "# Note: backbone needs text input. Use a fixed neutral text for all utterances\n",
    "# since we're probing the audio representation, not text-conditioned generation.\n",
    "NEUTRAL_TEXT = 'Este Ã© um texto neutro para extraÃ§Ã£o de features.'\n",
    "\n",
    "for entry in tqdm(entries, desc='Backbone'):\n",
    "    layer_feats = extract_backbone_features(\n",
    "        Path(entry.audio_path),\n",
    "        text=NEUTRAL_TEXT,\n",
    "        layers=BACKBONE_LAYERS,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for layer_idx, feat_vec in layer_feats.items():\n",
    "        backbone_features[layer_idx][entry.utt_id] = feat_vec\n",
    "\n",
    "print(f'Backbone extraction complete')\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    if backbone_features[layer]:\n",
    "        dim = next(iter(backbone_features[layer].values())).shape\n",
    "        print(f'  Layer {layer}: {len(backbone_features[layer])} vectors, dim={dim}')\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline ECAPA Speaker Similarity\n",
    "\n",
    "Mede similaridade intra-speaker (mesmo speaker, utterances diferentes) e inter-speaker no Ã¡udio real.  \n",
    "Este baseline Ã© referÃªncia obrigatÃ³ria para Stage 2 (preservaÃ§Ã£o de identidade com LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.ecapa import compute_speaker_similarity_baseline\n",
    "from src.evaluation.bootstrap_ci import bootstrap_cosine_similarity\n",
    "\n",
    "# Group embeddings by speaker\n",
    "speaker_embs = {}\n",
    "for entry in entries:\n",
    "    speaker_embs.setdefault(entry.speaker_id, []).append(\n",
    "        ecapa_embeddings[entry.utt_id]\n",
    "    )\n",
    "\n",
    "sim_baseline = compute_speaker_similarity_baseline(speaker_embs)\n",
    "\n",
    "# CI for intra and inter\n",
    "intra_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['intra']['values']), seed=SEED\n",
    ")\n",
    "inter_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['inter']['values']), seed=SEED\n",
    ")\n",
    "\n",
    "print('=== SPEAKER SIMILARITY BASELINE (ECAPA-TDNN, 192-dim) ===')\n",
    "print(f\"Intra-speaker: {sim_baseline['intra']['mean']:.4f} Â± {sim_baseline['intra']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{intra_ci.ci_lower:.4f}, {intra_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['intra']['n_pairs']}\")\n",
    "print(f\"\\nInter-speaker: {sim_baseline['inter']['mean']:.4f} Â± {sim_baseline['inter']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{inter_ci.ci_lower:.4f}, {inter_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['inter']['n_pairs']}\")\n",
    "print(f\"\\nSeparation: {sim_baseline['intra']['mean'] - sim_baseline['inter']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Probes\n",
    "\n",
    "Probe architecture: **Logistic Regression** (linear only â€” protocol requirement).  \n",
    "\n",
    "Split assignments (corrected â€” Achado 1 da auditoria):  \n",
    "- Accent probe: **speaker-disjoint** split  \n",
    "- Speaker probe: **stratified** split  \n",
    "- Leakage Aâ†’speaker: **stratified** split (same speakers in train/test)  \n",
    "- Leakage Sâ†’accent: **speaker-disjoint** split (different speakers in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.probes import train_linear_probe, evaluate_probe_against_thresholds\n",
    "from src.evaluation.confusion import plot_confusion_matrix\n",
    "\n",
    "# Helper: build X, y arrays from features dict and entries\n",
    "def build_probe_data(feature_dict, entry_list, target_field):\n",
    "    \"\"\"Build X, y arrays for probing.\"\"\"\n",
    "    X, y = [], []\n",
    "    for entry in entry_list:\n",
    "        if entry.utt_id in feature_dict:\n",
    "            X.append(feature_dict[entry.utt_id])\n",
    "            y.append(getattr(entry, target_field))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Collect all probe results\n",
    "all_probe_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Accent Probe (per layer, speaker-disjoint split)\n",
    "print('=== ACCENT PROBES ===')\n",
    "\n",
    "# Build train/test for speaker-disjoint\n",
    "train_entries = split_entries['train']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Probe each feature source\n",
    "feature_sources = {}\n",
    "\n",
    "# Acoustic\n",
    "feature_sources['acoustic'] = acoustic_vectors\n",
    "\n",
    "# ECAPA\n",
    "feature_sources['ecapa'] = ecapa_embeddings\n",
    "\n",
    "# WavLM layers\n",
    "for layer in SSL_LAYERS:\n",
    "    feature_sources[f'wavlm_layer_{layer}'] = ssl_features[layer]\n",
    "\n",
    "# Backbone layers\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    if backbone_features[layer]:\n",
    "        feature_sources[f'backbone_layer_{layer}'] = backbone_features[layer]\n",
    "\n",
    "for source_name, feat_dict in feature_sources.items():\n",
    "    X_train, y_train = build_probe_data(feat_dict, train_entries, 'accent')\n",
    "    X_test, y_test = build_probe_data(feat_dict, test_entries, 'accent')\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f'  {source_name}: SKIPPED (no data)')\n",
    "        continue\n",
    "    \n",
    "    result = train_linear_probe(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        probe_name=f'accent_{source_name}',\n",
    "        feature_source=source_name,\n",
    "        target='accent',\n",
    "        split_type='speaker_disjoint',\n",
    "        C=config['probes']['default_C'],\n",
    "        seed=SEED,\n",
    "    )\n",
    "    all_probe_results.append(result)\n",
    "    \n",
    "    decision = evaluate_probe_against_thresholds(\n",
    "        result, config['thresholds']['accent_probe']\n",
    "    )\n",
    "    print(f'  {source_name}: bal_acc={result.balanced_accuracy:.4f} '\n",
    "          f'CI=[{result.ci.ci_lower:.4f}, {result.ci.ci_upper:.4f}] '\n",
    "          f'delta={result.delta_pp:+.1f}pp â†’ {decision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Leakage Probes\n",
    "print('\\n=== LEAKAGE PROBES ===')\n",
    "\n",
    "# Leakage Aâ†’speaker: Do accent features contain speaker identity?\n",
    "# Uses STRATIFIED split (same speakers in train/test â€” we need known speakers)\n",
    "# TODO: Implement stratified split for leakage probes\n",
    "# For now, use speaker-disjoint as conservative test\n",
    "\n",
    "# Leakage Sâ†’accent: Do speaker features contain accent info?\n",
    "# Uses SPEAKER-DISJOINT split (different speakers in test â€” tests generalization)\n",
    "print('Leakage Sâ†’accent (ECAPA embeddings, speaker-disjoint split):')\n",
    "X_train, y_train = build_probe_data(ecapa_embeddings, train_entries, 'accent')\n",
    "X_test, y_test = build_probe_data(ecapa_embeddings, test_entries, 'accent')\n",
    "\n",
    "leakage_s2a = train_linear_probe(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    probe_name='leakage_s2a_ecapa',\n",
    "    feature_source='ecapa',\n",
    "    target='accent',\n",
    "    split_type='speaker_disjoint',\n",
    "    C=config['probes']['default_C'],\n",
    "    seed=SEED,\n",
    ")\n",
    "all_probe_results.append(leakage_s2a)\n",
    "\n",
    "leak_decision = evaluate_probe_against_thresholds(\n",
    "    leakage_s2a, config['thresholds']['leakage']\n",
    ")\n",
    "print(f'  bal_acc={leakage_s2a.balanced_accuracy:.4f} '\n",
    "      f'chance={leakage_s2a.chance_level:.4f} '\n",
    "      f'delta={leakage_s2a.delta_pp:+.1f}pp â†’ {leak_decision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Confusion Matrices (best accent probe)\n",
    "accent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\n",
    "if accent_results:\n",
    "    best = max(accent_results, key=lambda r: r.balanced_accuracy)\n",
    "    print(f'Best accent probe: {best.feature_source} (bal_acc={best.balanced_accuracy:.4f})')\n",
    "    \n",
    "    if best.confusion_matrix is not None:\n",
    "        Path('reports/figures').mkdir(parents=True, exist_ok=True)\n",
    "        plot_confusion_matrix(\n",
    "            best.confusion_matrix,\n",
    "            best.confusion_labels,\n",
    "            title=f'Accent Confusion Matrix ({best.feature_source})',\n",
    "            output_path=Path('reports/figures/confusion_matrix_accent.png'),\n",
    "        )\n",
    "        print('Confusion matrix saved to reports/figures/confusion_matrix_accent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness (Multiple Seeds)\n",
    "\n",
    "Repete o melhor probe com 3 seeds para reportar mÃ©dia e desvio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBUSTNESS_SEEDS = config['seed']['robustness_seeds']\n",
    "print(f'=== ROBUSTNESS CHECK (seeds: {ROBUSTNESS_SEEDS}) ===')\n",
    "\n",
    "if accent_results:\n",
    "    best_source = best.feature_source\n",
    "    best_features = feature_sources[best_source]\n",
    "    \n",
    "    seed_results = []\n",
    "    for s in ROBUSTNESS_SEEDS:\n",
    "        set_global_seed(s)\n",
    "        X_tr, y_tr = build_probe_data(best_features, train_entries, 'accent')\n",
    "        X_te, y_te = build_probe_data(best_features, test_entries, 'accent')\n",
    "        \n",
    "        r = train_linear_probe(\n",
    "            X_tr, y_tr, X_te, y_te,\n",
    "            probe_name=f'accent_{best_source}_seed{s}',\n",
    "            feature_source=best_source,\n",
    "            target='accent',\n",
    "            split_type='speaker_disjoint',\n",
    "            seed=s,\n",
    "            compute_ci=True,\n",
    "        )\n",
    "        seed_results.append(r)\n",
    "        print(f'  Seed {s}: bal_acc={r.balanced_accuracy:.4f} CI=[{r.ci.ci_lower:.4f}, {r.ci.ci_upper:.4f}]')\n",
    "    \n",
    "    accs = [r.balanced_accuracy for r in seed_results]\n",
    "    print(f'\\n  Mean: {np.mean(accs):.4f} Â± {np.std(accs):.4f}')\n",
    "    \n",
    "    # Restore original seed\n",
    "    set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gate Decision\n",
    "\n",
    "AvaliaÃ§Ã£o automÃ¡tica contra os thresholds do protocolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('STAGE 1.5 â€” GATE DECISION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Best accent probe\n",
    "if accent_results:\n",
    "    best = max(accent_results, key=lambda r: r.balanced_accuracy)\n",
    "    accent_decision = evaluate_probe_against_thresholds(\n",
    "        best, config['thresholds']['accent_probe']\n",
    "    )\n",
    "    print(f'\\nAccent probe ({best.feature_source}):')\n",
    "    print(f'  bal_acc = {best.balanced_accuracy:.4f}')\n",
    "    print(f'  CI 95% = [{best.ci.ci_lower:.4f}, {best.ci.ci_upper:.4f}]')\n",
    "    print(f'  Chance = {best.chance_level:.4f}')\n",
    "    print(f'  Decision: {accent_decision}')\n",
    "\n",
    "# Leakage\n",
    "leakage_results = [r for r in all_probe_results if 'leakage' in r.probe_name]\n",
    "leak_decisions = []\n",
    "for lr in leakage_results:\n",
    "    ld = evaluate_probe_against_thresholds(lr, config['thresholds']['leakage'])\n",
    "    leak_decisions.append(ld)\n",
    "    print(f'\\nLeakage {lr.probe_name}:')\n",
    "    print(f'  bal_acc = {lr.balanced_accuracy:.4f}')\n",
    "    print(f'  Chance = {lr.chance_level:.4f}')\n",
    "    print(f'  Delta = {lr.delta_pp:+.1f}pp')\n",
    "    print(f'  Decision: {ld}')\n",
    "\n",
    "# Overall\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "all_decisions = [accent_decision] + leak_decisions if accent_results else []\n",
    "if 'FAIL' in all_decisions:\n",
    "    overall = 'FAIL'\n",
    "elif 'GO_CONDITIONAL' in all_decisions:\n",
    "    overall = 'GO_CONDITIONAL'\n",
    "else:\n",
    "    overall = 'GO'\n",
    "print(f'OVERALL GATE DECISION: {overall}')\n",
    "print(f'{\"=\" * 60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Report\n",
    "\n",
    "Gera `stage1_5_report.json` com todos os resultados para auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get git commit hash\n",
    "try:\n",
    "    commit_hash = subprocess.check_output(\n",
    "        ['git', 'rev-parse', 'HEAD'], text=True\n",
    "    ).strip()\n",
    "except Exception:\n",
    "    commit_hash = 'unknown'\n",
    "\n",
    "report = {\n",
    "    'experiment': config['experiment']['name'],\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'commit_hash': commit_hash,\n",
    "    'seed': SEED,\n",
    "    'dataset': {\n",
    "        'name': config['dataset']['name'],\n",
    "        'manifest_sha256': build_stats.get('manifest_sha256'),\n",
    "        'total_entries': len(entries),\n",
    "        'regions': build_stats.get('regions'),\n",
    "    },\n",
    "    'splits': split_info.to_dict(),\n",
    "    'confounds': [\n",
    "        {\n",
    "            'test': r.test_name,\n",
    "            'variables': f'{r.variable_a} x {r.variable_b}',\n",
    "            'statistic': r.statistic,\n",
    "            'p_value': r.p_value,\n",
    "            'effect_size': r.effect_size,\n",
    "            'is_blocking': r.is_blocking,\n",
    "            'interpretation': r.interpretation,\n",
    "        }\n",
    "        for r in confound_results\n",
    "    ],\n",
    "    'speaker_similarity_baseline': {\n",
    "        'intra': {\n",
    "            'mean': sim_baseline['intra']['mean'],\n",
    "            'std': sim_baseline['intra']['std'],\n",
    "            'ci_lower': intra_ci.ci_lower,\n",
    "            'ci_upper': intra_ci.ci_upper,\n",
    "            'n_pairs': sim_baseline['intra']['n_pairs'],\n",
    "        },\n",
    "        'inter': {\n",
    "            'mean': sim_baseline['inter']['mean'],\n",
    "            'std': sim_baseline['inter']['std'],\n",
    "            'ci_lower': inter_ci.ci_lower,\n",
    "            'ci_upper': inter_ci.ci_upper,\n",
    "            'n_pairs': sim_baseline['inter']['n_pairs'],\n",
    "        },\n",
    "    },\n",
    "    'probes': [\n",
    "        {\n",
    "            'name': r.probe_name,\n",
    "            'feature_source': r.feature_source,\n",
    "            'target': r.target,\n",
    "            'split_type': r.split_type,\n",
    "            'balanced_accuracy': r.balanced_accuracy,\n",
    "            'f1_macro': r.f1_macro,\n",
    "            'chance_level': r.chance_level,\n",
    "            'delta_pp': r.delta_pp,\n",
    "            'ci_lower': r.ci.ci_lower if r.ci else None,\n",
    "            'ci_upper': r.ci.ci_upper if r.ci else None,\n",
    "            'n_train': r.n_train,\n",
    "            'n_test': r.n_test,\n",
    "            'n_classes': r.n_classes,\n",
    "            'C': r.regularization_C,\n",
    "        }\n",
    "        for r in all_probe_results\n",
    "    ],\n",
    "    'gate_decision': overall if 'overall' in dir() else 'NOT_EVALUATED',\n",
    "}\n",
    "\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "report_path = Path('reports/stage1_5_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f'Report saved to {report_path}')\n",
    "print(f'Total probe results: {len(all_probe_results)}')\n",
    "print(f'Gate decision: {report[\"gate_decision\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}