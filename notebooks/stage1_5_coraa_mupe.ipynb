{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1.5 â€” Latent Separability Audit\n",
    "\n",
    "**Projeto:** Controle ExplÃ­cito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Verificar se representaÃ§Ãµes internas do Qwen3-TTS codificam informaÃ§Ã£o suficiente de sotaque regional para classificaÃ§Ã£o acima de chance, com leakage controlado.  \n",
    "**Backbone:** Qwen3-TTS 1.7B-CustomVoice (frozen)  \n",
    "**Dataset:** CORAA-MUPE (speaker-disjoint splits)  \n",
    "\n",
    "Este notebook Ã© a **camada de orquestraÃ§Ã£o**. Toda lÃ³gica estÃ¡ em `src/` (testÃ¡vel, auditÃ¡vel).  \n",
    "O notebook apenas: instala deps â†’ configura ambiente â†’ chama mÃ³dulos â†’ exibe resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bootstrap: clone repo, install deps, check NumPy ABI.\n# This module uses only stdlib â€” safe to import before pip install.\n# On first Colab run, this cell may restart the runtime once (NumPy ABI fix).\nfrom src.utils.notebook_bootstrap import bootstrap\nbootstrap()"
  },
  {
   "cell_type": "code",
   "source": "# Platform-aware persistent cache setup\n# - Colab: Google Drive mount â†’ /content/drive/MyDrive/tcc-cache\n# - Lightning.ai: persistent storage â†’ /teamspace/studios/this_studio/cache\n# - Paperspace: persistent storage â†’ /storage/tcc-cache\n# - Local: ./cache (relative to repo root)\n\nfrom src.utils.platform import detect_platform, setup_environment\n\nplatform = detect_platform()\nsetup_environment(platform)\n# Note: setup_environment() already handles Drive mounting on Colab\n\nDRIVE_BASE = platform.cache_base\nDRIVE_BASE.mkdir(parents=True, exist_ok=True)\n\nprint(f'Platform: {platform.name}')\nprint(f'Cache base: {DRIVE_BASE}')\nprint(f'GPU: {platform.has_gpu}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds e determinismo â€” OBRIGATÃ“RIO antes de qualquer operaÃ§Ã£o\n",
    "from src.utils.seed import set_global_seed\n",
    "\n",
    "SEED = 42\n",
    "generator = set_global_seed(SEED)\n",
    "print(f'Seed global configurado: {SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU e versÃµes\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'\\nUsando device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Load experiment config YAML â€” single source of truth\nimport yaml\nfrom pathlib import Path\n\nwith open('configs/stage1_5.yaml') as f:\n    config = yaml.safe_load(f)\n\nprint(f'Config loaded: {config[\"experiment\"][\"name\"]}')\nprint(f'Dataset: {config[\"dataset\"][\"name\"]}')\nprint(f'Splits: {config[\"splits\"][\"method\"]} (seed={config[\"splits\"][\"seed\"]})')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Download e Build Manifest\n\nCarrega o CORAA-MUPE-ASR do HuggingFace, filtra por `speaker_type='R'` (entrevistados),\nduraÃ§Ã£o 3â€“15s e mÃ­nimo de speakers por regiÃ£o. O manifest Ã© o artefato versionado (SHA-256).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.data.cache import PipelineCache\n\ncache = PipelineCache(config, drive_base=DRIVE_BASE)\nprint(cache.report())\nprint()\n\n# Initialize variables for both code paths (cache hit vs miss)\nentries = None\nbuild_stats = None\n\nif cache.has_manifest():\n    print('Loading manifest from Drive cache...')\n    entries = cache.load_manifest()\n    print(f'Loaded {len(entries):,} entries from cache')\nelse:\n    from datasets import load_dataset, concatenate_datasets\n\n    print('Downloading CORAA-MUPE-ASR from HuggingFace...')\n    print('(~42 GB na primeira vez â€” usa cache nas prÃ³ximas execuÃ§Ãµes)\\n')\n\n    ds = load_dataset(\"nilc-nlp/CORAA-MUPE-ASR\")\n    print(f'Splits disponÃ­veis: {list(ds.keys())}')\n    for split_name, split_data in ds.items():\n        print(f'  {split_name}: {len(split_data):,} rows')\n\n    # Concatenar todos os splits â€” criaremos nossos prÃ³prios splits speaker-disjoint\n    all_data = concatenate_datasets([ds[split] for split in ds.keys()])\n    print(f'\\nTotal concatenado: {len(all_data):,} rows')\n    print(f'Colunas: {all_data.column_names}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build manifest from HF dataset (only when not loaded from cache)\nif entries is None:\n    from src.data.manifest_builder import build_manifest_from_hf_dataset\n\n    AUDIO_DIR = Path('data/audio/')\n    MANIFEST_PATH = Path(config['dataset']['manifest_path'])\n\n    entries, build_stats = build_manifest_from_hf_dataset(\n        dataset=all_data,\n        audio_output_dir=AUDIO_DIR,\n        manifest_output_path=MANIFEST_PATH,\n        speaker_type_filter=config['dataset']['filters']['speaker_type'],\n        min_duration_s=config['dataset']['filters']['min_duration_s'],\n        max_duration_s=config['dataset']['filters']['max_duration_s'],\n        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n    )\n\n    # Save to cache for next run\n    cache.save_manifest(entries)\n\n    print(f\"Manifest: {len(entries):,} entries\")\n    print(f\"SHA-256: {build_stats['manifest_sha256']}\")\n    print(f\"\\nFilter stats:\")\n    for key, count in build_stats['filter_stats'].items():\n        if key == 'dropped_regions':\n            continue\n        print(f\"  {key}: {count:,}\")\n\n    # Report dropped regions (protocol Â§4.3 fallback)\n    dropped = build_stats['filter_stats'].get('dropped_regions', [])\n    if dropped:\n        print(f\"\\nDropped regions (< {config['dataset']['filters']['min_speakers_per_region']} speakers): {dropped}\")\n        print(\"Fallback per TECHNICAL_VALIDATION_PROTOCOL.md Â§4.3\")\n\n    print(f\"\\nRegiÃµes mantidas:\")\n    for region, info in build_stats['regions'].items():\n        print(f\"  {region}: {info['n_speakers']} speakers, {info['n_utterances']:,} utterances\")\nelse:\n    print(f'Manifest already loaded from cache: {len(entries):,} entries')\n    print('Skipping HF dataset build.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speaker-Disjoint Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.data.splits import (\n    generate_speaker_disjoint_splits,\n    generate_stratified_splits,\n    save_splits,\n    save_stratified_splits,\n    assign_entries_to_splits,\n    assign_entries_to_stratified_splits,\n)\n\nsplit_info = generate_speaker_disjoint_splits(\n    entries,\n    train_ratio=config['splits']['ratios']['train'],\n    val_ratio=config['splits']['ratios']['val'],\n    test_ratio=config['splits']['ratios']['test'],\n    seed=config['splits']['seed'],\n)\n\n# Persistir splits\nsplit_path = save_splits(split_info, Path(config['splits']['output_dir']))\nprint(f\"Splits salvos em: {split_path}\")\nprint(f\"Train: {len(split_info.train_speakers)} speakers, {split_info.utterances_per_split['train']} utts\")\nprint(f\"Val:   {len(split_info.val_speakers)} speakers, {split_info.utterances_per_split['val']} utts\")\nprint(f\"Test:  {len(split_info.test_speakers)} speakers, {split_info.utterances_per_split['test']} utts\")\n\n# Assign entries (speaker-disjoint)\nsplit_entries = assign_entries_to_splits(entries, split_info)\n\n# Verify speaker-disjoint (HARD FAIL if violated â€” KB_HARD_FAIL_RULES Â§1)\ntrain_spk = {e.speaker_id for e in split_entries['train']}\nval_spk = {e.speaker_id for e in split_entries['val']}\ntest_spk = {e.speaker_id for e in split_entries['test']}\n\nassert len(train_spk & val_spk) == 0, f'Speaker leakage train->val: {train_spk & val_spk}'\nassert len(train_spk & test_spk) == 0, f'Speaker leakage train->test: {train_spk & test_spk}'\nassert len(val_spk & test_spk) == 0, f'Speaker leakage val->test: {val_spk & test_spk}'\nprint('\\nSpeaker-disjoint verification: PASSED')\n\n# Generate stratified split for leakage Aâ†’speaker probes\nstratified_split_info = generate_stratified_splits(\n    entries,\n    train_ratio=config['splits']['ratios']['train'],\n    seed=config['splits']['seed'],\n)\nstratified_split_path = save_stratified_splits(\n    stratified_split_info, Path(config['splits']['output_dir'])\n)\nstratified_entries = assign_entries_to_stratified_splits(entries, stratified_split_info)\nprint(f\"\\nStratified splits salvos em: {stratified_split_path}\")\nprint(f\"Stratified Train: {stratified_split_info.utterances_per_split['train']} utts\")\nprint(f\"Stratified Test:  {stratified_split_info.utterances_per_split['test']} utts\")\nprint(f\"Speakers in common: {stratified_split_info.speakers_in_common}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AnÃ¡lise de Confounds\n",
    "\n",
    "**Sanity checks obrigatÃ³rios** (recomendaÃ§Ã£o do mentor):  \n",
    "- Tabela accent Ã— gender com chi-quadrado + Cramer's V  \n",
    "- Histograma de duraÃ§Ã£o por regiÃ£o + Kruskal-Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.confounds import run_all_confound_checks\n",
    "import pandas as pd\n",
    "\n",
    "confound_results = run_all_confound_checks(\n",
    "    entries,\n",
    "    gender_blocking_threshold=config['evaluation']['confounds']['accent_x_gender']['threshold_blocker'],\n",
    "    duration_practical_diff_s=config['evaluation']['confounds']['accent_x_duration']['practical_diff_s'],\n",
    "    snr_practical_diff_db=config['evaluation']['confounds']['accent_x_snr']['practical_diff_db'],\n",
    ")\n",
    "\n",
    "print(\"=== CONFOUND ANALYSIS ===\")\n",
    "for result in confound_results:\n",
    "    status = 'ðŸ”´ BLOCKING' if result.is_blocking else ('ðŸŸ¡ SIGNIFICANT' if result.is_significant else 'ðŸŸ¢ OK')\n",
    "    print(f\"\\n{result.variable_a} Ã— {result.variable_b}: {status}\")\n",
    "    print(f\"  Test: {result.test_name}\")\n",
    "    print(f\"  Statistic: {result.statistic:.4f}\")\n",
    "    print(f\"  p-value: {result.p_value:.6f}\")\n",
    "    print(f\"  Effect size ({result.effect_size_name}): {result.effect_size:.4f}\")\n",
    "    print(f\"  Interpretation: {result.interpretation}\")\n",
    "\n",
    "# Tabela accent x gender\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in entries],\n",
    "    [e.gender for e in entries],\n",
    "    margins=True,\n",
    ")\n",
    "print(\"\\n=== ACCENT Ã— GENDER TABLE ===\")\n",
    "print(gender_table)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Duration histogram by region + summary stats\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndurations_by_region = {}\nfor e in entries:\n    durations_by_region.setdefault(e.accent, []).append(e.duration_s)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nregions_sorted = sorted(durations_by_region.keys())\nax.boxplot(\n    [durations_by_region[r] for r in regions_sorted],\n    labels=regions_sorted,\n    showfliers=False,\n)\nax.set_xlabel('Region (IBGE macro-region)')\nax.set_ylabel('Duration (seconds)')\nax.set_title('Duration distribution by accent region')\nplt.tight_layout()\n\nPath('reports/figures').mkdir(parents=True, exist_ok=True)\nplt.savefig('reports/figures/duration_by_region.png', dpi=150)\nplt.show()\n\nprint('\\nDuration summary:')\nfor r in regions_sorted:\n    durs = durations_by_region[r]\n    print(f'  {r}: mean={np.mean(durs):.2f}s, std={np.std(durs):.2f}s, '\n          f'median={np.median(durs):.2f}s, n={len(durs)}')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Feature Extraction\n\nQuatro fontes de features para probing:\n1. **Acoustic** (MFCC + pitch + energy) â€” baseline rÃ¡pido, CPU-only\n2. **ECAPA-TDNN** â€” embeddings de speaker (192-dim)\n3. **WavLM** â€” SSL features por camada\n4. **Qwen3-TTS backbone** â€” features internas do modelo-alvo (GPU)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom tqdm.auto import tqdm\nfrom src.features.acoustic import extract_acoustic_features, features_to_vector\nfrom src.features.ecapa import extract_ecapa_embedding\n\n# 4.1 Acoustic features (CPU, fast)\nprint('=== Acoustic features ===')\nif cache.has_features('acoustic'):\n    acoustic_vectors = cache.load_features('acoustic')\n    print(f'Loaded {len(acoustic_vectors)} vectors from cache')\nelse:\n    acoustic_vectors = {}\n    for entry in tqdm(entries, desc='Acoustic'):\n        feats = extract_acoustic_features(\n            Path(entry.audio_path), entry.utt_id,\n            n_mfcc=config['features']['acoustic']['n_mfcc'],\n        )\n        acoustic_vectors[entry.utt_id] = features_to_vector(feats)\n    cache.save_features('acoustic', acoustic_vectors)\n    print(f'Extracted and cached {len(acoustic_vectors)} vectors')\n\nprint(f'Dimension: {next(iter(acoustic_vectors.values())).shape}')\nif torch.cuda.is_available():\n    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4.2 ECAPA-TDNN speaker embeddings\nprint('=== ECAPA embeddings ===')\nif cache.has_features('ecapa'):\n    ecapa_embeddings = cache.load_features('ecapa')\n    print(f'Loaded {len(ecapa_embeddings)} embeddings from cache')\nelse:\n    ecapa_embeddings = {}\n    for entry in tqdm(entries, desc='ECAPA'):\n        emb = extract_ecapa_embedding(Path(entry.audio_path), device=DEVICE)\n        ecapa_embeddings[entry.utt_id] = emb\n    cache.save_features('ecapa', ecapa_embeddings)\n    print(f'Extracted and cached {len(ecapa_embeddings)} embeddings')\n\nprint(f'Dimension: {next(iter(ecapa_embeddings.values())).shape}')\nif torch.cuda.is_available():\n    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4.3 WavLM SSL features (layer-wise)\nfrom src.features.ssl import extract_ssl_features\n\nSSL_LAYERS = config['features']['ssl']['layers']\nprint(f'=== WavLM features (layers {SSL_LAYERS}) ===')\n\nssl_features = {layer: {} for layer in SSL_LAYERS}\n\n# Check cache per layer\nall_cached = True\nfor layer in SSL_LAYERS:\n    cache_key = f'wavlm_layer_{layer}'\n    if cache.has_features(cache_key):\n        ssl_features[layer] = cache.load_features(cache_key)\n        print(f'  Layer {layer}: loaded {len(ssl_features[layer])} vectors from cache')\n    else:\n        all_cached = False\n        break\n\nif not all_cached:\n    print('  Extracting from scratch...')\n    ssl_features = {layer: {} for layer in SSL_LAYERS}\n    for entry in tqdm(entries, desc='WavLM'):\n        layer_feats = extract_ssl_features(\n            Path(entry.audio_path),\n            layers=SSL_LAYERS,\n            device=DEVICE,\n        )\n        for layer_idx, feat_vec in layer_feats.items():\n            ssl_features[layer_idx][entry.utt_id] = feat_vec\n\n    for layer in SSL_LAYERS:\n        cache.save_features(f'wavlm_layer_{layer}', ssl_features[layer])\n\nprint(f'WavLM extraction complete')\nfor layer in SSL_LAYERS:\n    dim = next(iter(ssl_features[layer].values())).shape\n    print(f'  Layer {layer}: {len(ssl_features[layer])} vectors, dim={dim}')\nif torch.cuda.is_available():\n    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4.4 Qwen3-TTS backbone features (layer-wise) â€” GPU required\nfrom src.features.backbone import extract_backbone_features\n\nBACKBONE_LAYERS = config['features']['backbone']['layers']\nNEUTRAL_TEXT = config['features']['backbone']['neutral_text']  # from config, not hardcoded\nprint(f'=== Backbone features (layers {BACKBONE_LAYERS}) ===')\nprint(f'Neutral text: \"{NEUTRAL_TEXT}\"')\n\nbackbone_features = {layer: {} for layer in BACKBONE_LAYERS}\n\n# Check cache per layer\nall_cached = True\nfor layer in BACKBONE_LAYERS:\n    cache_key = f'backbone_layer_{layer}'\n    if cache.has_features(cache_key):\n        backbone_features[layer] = cache.load_features(cache_key)\n        print(f'  Layer {layer}: loaded {len(backbone_features[layer])} vectors from cache')\n    else:\n        all_cached = False\n        break\n\nif not all_cached:\n    print('  Extracting from scratch...')\n    backbone_features = {layer: {} for layer in BACKBONE_LAYERS}\n\n    for entry in tqdm(entries, desc='Backbone'):\n        layer_feats = extract_backbone_features(\n            Path(entry.audio_path),\n            text=NEUTRAL_TEXT,\n            layers=BACKBONE_LAYERS,\n            device=DEVICE,\n        )\n        for layer_idx, feat_vec in layer_feats.items():\n            backbone_features[layer_idx][entry.utt_id] = feat_vec\n\n    for layer in BACKBONE_LAYERS:\n        if backbone_features[layer]:\n            cache.save_features(f'backbone_layer_{layer}', backbone_features[layer])\n\nprint(f'Backbone extraction complete')\nfor layer in BACKBONE_LAYERS:\n    if backbone_features[layer]:\n        dim = next(iter(backbone_features[layer].values())).shape\n        print(f'  Layer {layer}: {len(backbone_features[layer])} vectors, dim={dim}')\n\n# Free GPU memory after heaviest extraction\nif torch.cuda.is_available():\n    print(f'VRAM before cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline ECAPA Speaker Similarity\n",
    "\n",
    "Mede similaridade intra-speaker (mesmo speaker, utterances diferentes) e inter-speaker no Ã¡udio real.  \n",
    "Este baseline Ã© referÃªncia obrigatÃ³ria para Stage 2 (preservaÃ§Ã£o de identidade com LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.ecapa import compute_speaker_similarity_baseline\n",
    "from src.evaluation.bootstrap_ci import bootstrap_cosine_similarity\n",
    "\n",
    "# Group embeddings by speaker\n",
    "speaker_embs = {}\n",
    "for entry in entries:\n",
    "    speaker_embs.setdefault(entry.speaker_id, []).append(\n",
    "        ecapa_embeddings[entry.utt_id]\n",
    "    )\n",
    "\n",
    "sim_baseline = compute_speaker_similarity_baseline(speaker_embs)\n",
    "\n",
    "# CI for intra and inter\n",
    "intra_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['intra']['values']), seed=SEED\n",
    ")\n",
    "inter_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['inter']['values']), seed=SEED\n",
    ")\n",
    "\n",
    "print('=== SPEAKER SIMILARITY BASELINE (ECAPA-TDNN, 192-dim) ===')\n",
    "print(f\"Intra-speaker: {sim_baseline['intra']['mean']:.4f} Â± {sim_baseline['intra']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{intra_ci.ci_lower:.4f}, {intra_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['intra']['n_pairs']}\")\n",
    "print(f\"\\nInter-speaker: {sim_baseline['inter']['mean']:.4f} Â± {sim_baseline['inter']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{inter_ci.ci_lower:.4f}, {inter_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['inter']['n_pairs']}\")\n",
    "print(f\"\\nSeparation: {sim_baseline['intra']['mean'] - sim_baseline['inter']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Probes\n",
    "\n",
    "Probe architecture: **Logistic Regression** (linear only â€” protocol requirement).  \n",
    "\n",
    "Split assignments (corrected â€” Achado 1 da auditoria):  \n",
    "- Accent probe: **speaker-disjoint** split  \n",
    "- Speaker probe: **stratified** split  \n",
    "- Leakage Aâ†’speaker: **stratified** split (same speakers in train/test)  \n",
    "- Leakage Sâ†’accent: **speaker-disjoint** split (different speakers in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.evaluation.probes import (\n    build_probe_data,\n    train_linear_probe,\n    evaluate_probe_against_thresholds,\n    sweep_regularization,\n    train_selectivity_control,\n)\nfrom src.evaluation.confusion import plot_confusion_matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 6.1 Accent Probe (per layer, speaker-disjoint split)\n# Initialize probe result collectors (reset on re-execution for idempotency)\nall_probe_results = []\nall_selectivity_results = []\n\nprint('=== ACCENT PROBES ===')\n\n# Build train/test for speaker-disjoint\ntrain_entries = split_entries['train']\ntest_entries = split_entries['test']\n\n# Probe each feature source\nfeature_sources = {}\n\n# Acoustic\nfeature_sources['acoustic'] = acoustic_vectors\n\n# ECAPA\nfeature_sources['ecapa'] = ecapa_embeddings\n\n# WavLM layers\nfor layer in SSL_LAYERS:\n    feature_sources[f'wavlm_layer_{layer}'] = ssl_features[layer]\n\n# Backbone layers\nfor layer in BACKBONE_LAYERS:\n    if backbone_features[layer]:\n        feature_sources[f'backbone_layer_{layer}'] = backbone_features[layer]\n\nC_values = config['probes']['regularization_C']\n\nfor source_name, feat_dict in feature_sources.items():\n    X_train, y_train = build_probe_data(feat_dict, train_entries, 'accent')\n    X_test, y_test = build_probe_data(feat_dict, test_entries, 'accent')\n    \n    if len(X_train) == 0 or len(X_test) == 0:\n        print(f'  {source_name}: SKIPPED (no data)')\n        continue\n    \n    # Sweep regularization to find best C\n    sweep_results = sweep_regularization(\n        X_train, y_train, X_test, y_test,\n        C_values=C_values,\n        probe_name=f'accent_{source_name}',\n        feature_source=source_name,\n        target='accent',\n        split_type='speaker_disjoint',\n        seed=SEED,\n    )\n    best_sweep = max(sweep_results, key=lambda r: r.balanced_accuracy)\n    best_C = best_sweep.regularization_C\n    \n    # Re-train with best C and full CI\n    result = train_linear_probe(\n        X_train, y_train, X_test, y_test,\n        probe_name=f'accent_{source_name}',\n        feature_source=source_name,\n        target='accent',\n        split_type='speaker_disjoint',\n        C=best_C,\n        seed=SEED,\n    )\n    all_probe_results.append(result)\n    \n    decision = evaluate_probe_against_thresholds(\n        result, config['thresholds']['accent_probe']\n    )\n    print(f'  {source_name}: bal_acc={result.balanced_accuracy:.4f} '\n          f'CI=[{result.ci.ci_lower:.4f}, {result.ci.ci_upper:.4f}] '\n          f'delta={result.delta_pp:+.1f}pp C={best_C} â†’ {decision}')\n\n# Selectivity control for accent probes\nprint('\\n=== SELECTIVITY CONTROL (accent probes) ===')\naccent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\nfor result in accent_results:\n    feat_dict = feature_sources[result.feature_source]\n    X_train, y_train = build_probe_data(feat_dict, train_entries, 'accent')\n    X_test, y_test = build_probe_data(feat_dict, test_entries, 'accent')\n    \n    sel = train_selectivity_control(\n        X_train, y_train, X_test, y_test,\n        real_result=result,\n        seed=SEED,\n        C=result.regularization_C,\n    )\n    sel['probe_name'] = result.probe_name\n    sel['feature_source'] = result.feature_source\n    all_selectivity_results.append(sel)\n    \n    print(f'  {result.feature_source}: real={sel[\"real_bal_acc\"]:.4f} '\n          f'permuted={sel[\"permuted_bal_acc_mean\"]:.4f}Â±{sel[\"permuted_bal_acc_std\"]:.4f} '\n          f'selectivity={sel[\"selectivity_pp\"]:+.1f}pp')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 6.2 Leakage Probes\n# Remove previous leakage results for idempotent re-execution\nall_probe_results = [r for r in all_probe_results if 'leakage' not in r.probe_name]\nall_selectivity_results = [s for s in all_selectivity_results if 'leakage' not in s.get('probe_name', '')]\n\nprint('\\n=== LEAKAGE PROBES ===')\n\n# --- Leakage Aâ†’speaker: Do accent features contain speaker identity? ---\n# Uses STRATIFIED split (same speakers in train/test â€” we need known speakers)\nprint('Leakage Aâ†’speaker (accent feature sources, stratified split):')\n\nstrat_train_entries = stratified_entries['train']\nstrat_test_entries = stratified_entries['test']\n\n# Accent feature sources: WavLM layers, backbone layers, acoustic\n# (NOT ECAPA â€” those are speaker embeddings, not accent features)\nleakage_a2s_sources = {}\nfor layer in SSL_LAYERS:\n    leakage_a2s_sources[f'wavlm_layer_{layer}'] = ssl_features[layer]\nfor layer in BACKBONE_LAYERS:\n    if backbone_features[layer]:\n        leakage_a2s_sources[f'backbone_layer_{layer}'] = backbone_features[layer]\nleakage_a2s_sources['acoustic'] = acoustic_vectors\n\nleakage_a2s_results = []\nfor source_name, feat_dict in leakage_a2s_sources.items():\n    X_train, y_train = build_probe_data(feat_dict, strat_train_entries, 'speaker_id')\n    X_test, y_test = build_probe_data(feat_dict, strat_test_entries, 'speaker_id')\n    \n    if len(X_train) == 0 or len(X_test) == 0:\n        print(f'  {source_name}: SKIPPED (no data)')\n        continue\n\n    result = train_linear_probe(\n        X_train, y_train, X_test, y_test,\n        probe_name=f'leakage_a2s_{source_name}',\n        feature_source=source_name,\n        target='speaker_id',\n        split_type='stratified',\n        C=config['probes']['default_C'],\n        seed=SEED,\n    )\n    leakage_a2s_results.append(result)\n    all_probe_results.append(result)\n\n    leak_decision = evaluate_probe_against_thresholds(\n        result, config['thresholds']['leakage']\n    )\n    print(f'  {source_name}: bal_acc={result.balanced_accuracy:.4f} '\n          f'chance={result.chance_level:.4f} '\n          f'delta={result.delta_pp:+.1f}pp â†’ {leak_decision}')\n\n# Selectivity control for Aâ†’speaker leakage\nprint('\\n=== SELECTIVITY CONTROL (leakage Aâ†’speaker) ===')\nfor result in leakage_a2s_results:\n    feat_dict = leakage_a2s_sources[result.feature_source]\n    X_train, y_train = build_probe_data(feat_dict, strat_train_entries, 'speaker_id')\n    X_test, y_test = build_probe_data(feat_dict, strat_test_entries, 'speaker_id')\n\n    sel = train_selectivity_control(\n        X_train, y_train, X_test, y_test,\n        real_result=result,\n        seed=SEED,\n        C=result.regularization_C,\n    )\n    sel['probe_name'] = result.probe_name\n    sel['feature_source'] = result.feature_source\n    all_selectivity_results.append(sel)\n\n    print(f'  {result.feature_source}: real={sel[\"real_bal_acc\"]:.4f} '\n          f'permuted={sel[\"permuted_bal_acc_mean\"]:.4f}Â±{sel[\"permuted_bal_acc_std\"]:.4f} '\n          f'selectivity={sel[\"selectivity_pp\"]:+.1f}pp')\n\n# --- Leakage Sâ†’accent: Do speaker features contain accent info? ---\n# Uses SPEAKER-DISJOINT split (different speakers in test â€” tests generalization)\nprint('\\nLeakage Sâ†’accent (ECAPA embeddings, speaker-disjoint split):')\nX_train, y_train = build_probe_data(ecapa_embeddings, train_entries, 'accent')\nX_test, y_test = build_probe_data(ecapa_embeddings, test_entries, 'accent')\n\nleakage_s2a = train_linear_probe(\n    X_train, y_train, X_test, y_test,\n    probe_name='leakage_s2a_ecapa',\n    feature_source='ecapa',\n    target='accent',\n    split_type='speaker_disjoint',\n    C=config['probes']['default_C'],\n    seed=SEED,\n)\nall_probe_results.append(leakage_s2a)\n\nleak_decision = evaluate_probe_against_thresholds(\n    leakage_s2a, config['thresholds']['leakage']\n)\nprint(f'  bal_acc={leakage_s2a.balanced_accuracy:.4f} '\n      f'chance={leakage_s2a.chance_level:.4f} '\n      f'delta={leakage_s2a.delta_pp:+.1f}pp â†’ {leak_decision}')\n\n# Selectivity control for Sâ†’accent leakage\nsel_s2a = train_selectivity_control(\n    X_train, y_train, X_test, y_test,\n    real_result=leakage_s2a,\n    seed=SEED,\n    C=leakage_s2a.regularization_C,\n)\nsel_s2a['probe_name'] = leakage_s2a.probe_name\nsel_s2a['feature_source'] = leakage_s2a.feature_source\nall_selectivity_results.append(sel_s2a)\nprint(f'  selectivity: real={sel_s2a[\"real_bal_acc\"]:.4f} '\n      f'permuted={sel_s2a[\"permuted_bal_acc_mean\"]:.4f}Â±{sel_s2a[\"permuted_bal_acc_std\"]:.4f} '\n      f'selectivity={sel_s2a[\"selectivity_pp\"]:+.1f}pp')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Confusion Matrices (best accent probe)\n",
    "accent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\n",
    "if accent_results:\n",
    "    best = max(accent_results, key=lambda r: r.balanced_accuracy)\n",
    "    print(f'Best accent probe: {best.feature_source} (bal_acc={best.balanced_accuracy:.4f})')\n",
    "    \n",
    "    if best.confusion_matrix is not None:\n",
    "        Path('reports/figures').mkdir(parents=True, exist_ok=True)\n",
    "        plot_confusion_matrix(\n",
    "            best.confusion_matrix,\n",
    "            best.confusion_labels,\n",
    "            title=f'Accent Confusion Matrix ({best.feature_source})',\n",
    "            output_path=Path('reports/figures/confusion_matrix_accent.png'),\n",
    "        )\n",
    "        print('Confusion matrix saved to reports/figures/confusion_matrix_accent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness (Multiple Seeds)\n",
    "\n",
    "Repete o melhor probe com 3 seeds para reportar mÃ©dia e desvio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBUSTNESS_SEEDS = config['seed']['robustness_seeds']\n",
    "print(f'=== ROBUSTNESS CHECK (seeds: {ROBUSTNESS_SEEDS}) ===')\n",
    "\n",
    "if accent_results:\n",
    "    best_source = best.feature_source\n",
    "    best_features = feature_sources[best_source]\n",
    "    \n",
    "    seed_results = []\n",
    "    for s in ROBUSTNESS_SEEDS:\n",
    "        set_global_seed(s)\n",
    "        X_tr, y_tr = build_probe_data(best_features, train_entries, 'accent')\n",
    "        X_te, y_te = build_probe_data(best_features, test_entries, 'accent')\n",
    "        \n",
    "        r = train_linear_probe(\n",
    "            X_tr, y_tr, X_te, y_te,\n",
    "            probe_name=f'accent_{best_source}_seed{s}',\n",
    "            feature_source=best_source,\n",
    "            target='accent',\n",
    "            split_type='speaker_disjoint',\n",
    "            seed=s,\n",
    "            compute_ci=True,\n",
    "        )\n",
    "        seed_results.append(r)\n",
    "        print(f'  Seed {s}: bal_acc={r.balanced_accuracy:.4f} CI=[{r.ci.ci_lower:.4f}, {r.ci.ci_upper:.4f}]')\n",
    "    \n",
    "    accs = [r.balanced_accuracy for r in seed_results]\n",
    "    print(f'\\n  Mean: {np.mean(accs):.4f} Â± {np.std(accs):.4f}')\n",
    "    \n",
    "    # Restore original seed\n",
    "    set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gate Decision\n",
    "\n",
    "AvaliaÃ§Ã£o automÃ¡tica contra os thresholds do protocolo."
   ]
  },
  {
   "cell_type": "code",
   "source": "from src.evaluation.probes import evaluate_probe_against_thresholds\n\n# Gate decision: evaluate all probes against protocol thresholds\nprint('=== STAGE 1.5 GATE DECISION ===\\n')\n\noverall = 'NOT_EVALUATED'  # safe default â€” overwritten below if all checks run\n\n# 1. Accent probes â€” at least one must reach GO or GO_CONDITIONAL\naccent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\naccent_decisions = []\nfor r in accent_results:\n    d = evaluate_probe_against_thresholds(r, config['thresholds']['accent_probe'])\n    accent_decisions.append((r.feature_source, r.balanced_accuracy, r.delta_pp, d))\n    print(f'  Accent {r.feature_source}: bal_acc={r.balanced_accuracy:.4f} delta={r.delta_pp:+.1f}pp â†’ {d}')\n\naccent_pass = any(d in ('GO', 'GO_CONDITIONAL') for _, _, _, d in accent_decisions)\nprint(f'\\n  Accent gate: {\"GO\" if accent_pass else \"FAIL\"} (at least one source above threshold)')\n\n# 2. Leakage probes â€” all must be GO or GO_CONDITIONAL (below threshold)\nleakage_results = [r for r in all_probe_results if 'leakage' in r.probe_name]\nleakage_decisions = []\nfor r in leakage_results:\n    d = evaluate_probe_against_thresholds(r, config['thresholds']['leakage'])\n    leakage_decisions.append((r.probe_name, r.balanced_accuracy, r.delta_pp, d))\n    print(f'  Leakage {r.probe_name}: bal_acc={r.balanced_accuracy:.4f} delta={r.delta_pp:+.1f}pp â†’ {d}')\n\nleakage_pass = all(d in ('GO', 'GO_CONDITIONAL') for _, _, _, d in leakage_decisions)\nprint(f'\\n  Leakage gate: {\"GO\" if leakage_pass else \"FAIL\"} (all probes below threshold)')\n\n# 3. Confounds â€” no blocking confound\nconfound_pass = not any(r.is_blocking for r in confound_results)\nprint(f'  Confound gate: {\"GO\" if confound_pass else \"FAIL\"} (no blocking confounds)')\n\n# 4. Overall decision\nif accent_pass and leakage_pass and confound_pass:\n    overall = 'GO'\nelif accent_pass and confound_pass:\n    overall = 'ADJUST'  # signal exists but leakage needs attention\nelse:\n    overall = 'FAIL'\n\nprint(f'\\n{\"=\"*50}')\nprint(f'  STAGE 1.5 GATE: {overall}')\nprint(f'{\"=\"*50}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom src.utils.git import get_commit_hash\n\ncommit_hash = get_commit_hash()\n\n# Manifest SHA-256 â€” from build_stats if available, or recompute from file\nmanifest_sha256 = None\nif build_stats is not None:\n    manifest_sha256 = build_stats.get('manifest_sha256')\nelif cache.has_manifest():\n    from src.data.manifest import compute_file_hash\n    manifest_sha256 = compute_file_hash(cache.get_manifest_path())\n\n# Compute region stats (fallback when build_stats is unavailable)\nif build_stats and build_stats.get('regions'):\n    region_stats = build_stats['regions']\nelse:\n    speakers_by_region = defaultdict(set)\n    utts_by_region = defaultdict(int)\n    for e in entries:\n        speakers_by_region[e.accent].add(e.speaker_id)\n        utts_by_region[e.accent] += 1\n    region_stats = {\n        region: {'n_speakers': len(speakers_by_region[region]), 'n_utterances': utts_by_region[region]}\n        for region in sorted(speakers_by_region)\n    }\n\nreport = {\n    'experiment': config['experiment']['name'],\n    'date': datetime.now().isoformat(),\n    'commit_hash': commit_hash,\n    'seed': SEED,\n    'filter_hash': cache.filter_hash,\n    'environment': {\n        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n        'cudnn_version': torch.backends.cudnn.version() if torch.cuda.is_available() else None,\n        'torch_version': torch.__version__,\n    },\n    'dataset': {\n        'name': config['dataset']['name'],\n        'manifest_sha256': manifest_sha256,\n        'total_entries': len(entries),\n        'regions': region_stats,\n    },\n    'splits': split_info.to_dict(),\n    'stratified_splits': stratified_split_info.to_dict(),\n    'confounds': [\n        {\n            'test': r.test_name,\n            'variables': f'{r.variable_a} x {r.variable_b}',\n            'statistic': r.statistic,\n            'p_value': r.p_value,\n            'effect_size': r.effect_size,\n            'is_blocking': r.is_blocking,\n            'interpretation': r.interpretation,\n        }\n        for r in confound_results\n    ],\n    'speaker_similarity_baseline': {\n        'intra': {\n            'mean': sim_baseline['intra']['mean'],\n            'std': sim_baseline['intra']['std'],\n            'ci_lower': intra_ci.ci_lower,\n            'ci_upper': intra_ci.ci_upper,\n            'n_pairs': sim_baseline['intra']['n_pairs'],\n        },\n        'inter': {\n            'mean': sim_baseline['inter']['mean'],\n            'std': sim_baseline['inter']['std'],\n            'ci_lower': inter_ci.ci_lower,\n            'ci_upper': inter_ci.ci_upper,\n            'n_pairs': sim_baseline['inter']['n_pairs'],\n        },\n    },\n    'probes': [\n        {\n            'name': r.probe_name,\n            'feature_source': r.feature_source,\n            'target': r.target,\n            'split_type': r.split_type,\n            'balanced_accuracy': r.balanced_accuracy,\n            'f1_macro': r.f1_macro,\n            'chance_level': r.chance_level,\n            'delta_pp': r.delta_pp,\n            'ci_lower': r.ci.ci_lower if r.ci else None,\n            'ci_upper': r.ci.ci_upper if r.ci else None,\n            'n_train': r.n_train,\n            'n_test': r.n_test,\n            'n_classes': r.n_classes,\n            'C': r.regularization_C,\n        }\n        for r in all_probe_results\n    ],\n    'selectivity_controls': all_selectivity_results,\n    'gate_decision': overall,\n}\n\nPath('reports').mkdir(exist_ok=True)\nreport_path = Path('reports/stage1_5_report.json')\nwith open(report_path, 'w') as f:\n    json.dump(report, f, indent=2, default=str)\n\nprint(f'Report saved to {report_path}')\nprint(f'Filter hash: {cache.filter_hash}')\nprint(f'Total probe results: {len(all_probe_results)}')\nprint(f'Total selectivity controls: {len(all_selectivity_results)}')\nprint(f'Gate decision: {report[\"gate_decision\"]}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}