{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1.5 â€” Latent Separability Audit\n",
    "\n",
    "**Projeto:** Controle ExplÃ­cito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Verificar se representaÃ§Ãµes internas do Qwen3-TTS codificam informaÃ§Ã£o suficiente de sotaque regional para classificaÃ§Ã£o acima de chance, com leakage controlado.  \n",
    "**Backbone:** Qwen3-TTS 1.7B-CustomVoice (frozen)  \n",
    "**Dataset:** CORAA-MUPE (speaker-disjoint splits)  \n",
    "\n",
    "Este notebook Ã© a **camada de orquestraÃ§Ã£o**. Toda lÃ³gica estÃ¡ em `src/` (testÃ¡vel, auditÃ¡vel).  \n",
    "O notebook apenas: instala deps â†’ configura ambiente â†’ chama mÃ³dulos â†’ exibe resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: /content/TCC\n",
      "Bootstrap OK\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap: clone repo, install deps, check NumPy ABI.\n",
    "# This code is intentionally INLINE (stdlib-only) because on Colab the repo\n",
    "# does not yet exist when this cell runs. After cloning + pip install, we can\n",
    "# import the ABI check from src/. On first Colab run, this cell may restart\n",
    "# the runtime once (NumPy ABI fix). After restart, re-run â€” it completes fast.\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Detect platform and set repo directory\n",
    "if os.path.exists(\"/teamspace/studios/this_studio\"):\n",
    "    _REPO_DIR = \"/teamspace/studios/this_studio/TCC\"\n",
    "elif os.environ.get(\"PAPERSPACE\"):\n",
    "    _REPO_DIR = \"/notebooks/TCC\"\n",
    "elif \"google.colab\" in sys.modules or os.path.exists(\"/content\"):\n",
    "    _REPO_DIR = \"/content/TCC\"\n",
    "else:\n",
    "    _REPO_DIR = os.getcwd()\n",
    "\n",
    "# Clone repo if not already present\n",
    "if not os.path.exists(os.path.join(_REPO_DIR, \".git\")):\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/paulohenriquevn/tcc.git\", _REPO_DIR],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "os.chdir(_REPO_DIR)\n",
    "if _REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, _REPO_DIR)\n",
    "\n",
    "# Install dependencies â€” capture output so errors are visible on failure\n",
    "_req = os.path.join(_REPO_DIR, \"requirements.txt\")\n",
    "if os.path.exists(_req):\n",
    "    _pip = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", _req, \"-q\"],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if _pip.returncode != 0:\n",
    "        print(\"pip install FAILED. stderr:\")\n",
    "        print(_pip.stderr)\n",
    "        print(\"stdout:\")\n",
    "        print(_pip.stdout)\n",
    "        raise RuntimeError(\n",
    "            f\"pip install -r requirements.txt failed (exit {_pip.returncode}). \"\n",
    "            \"Check the output above for the failing package.\"\n",
    "        )\n",
    "\n",
    "# NumPy ABI check â€” may restart runtime once on Colab (stale C-extensions)\n",
    "from src.utils.notebook_bootstrap import _check_numpy_abi\n",
    "_check_numpy_abi()\n",
    "\n",
    "print(f\"Repo: {_REPO_DIR}\")\n",
    "print(\"Bootstrap OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/paulohenriquevn/tcc\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!cd /content/TCC && git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Platform: colab\n",
      "Cache base: /content/drive/MyDrive/tcc-cache\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "# Platform-aware persistent cache setup\n",
    "# - Colab: Google Drive mount â†’ /content/drive/MyDrive/tcc-cache\n",
    "# - Lightning.ai: persistent storage â†’ /teamspace/studios/this_studio/cache\n",
    "# - Paperspace: persistent storage â†’ /storage/tcc-cache\n",
    "# - Local: ./cache (relative to repo root)\n",
    "\n",
    "from src.utils.platform import detect_platform, setup_environment\n",
    "\n",
    "platform = detect_platform()\n",
    "setup_environment(platform)\n",
    "# Note: setup_environment() already handles Drive mounting on Colab\n",
    "\n",
    "DRIVE_BASE = platform.cache_base\n",
    "DRIVE_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Platform: {platform.name}')\n",
    "print(f'Cache base: {DRIVE_BASE}')\n",
    "print(f'GPU: {platform.has_gpu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed global configurado: 42\n"
     ]
    }
   ],
   "source": [
    "# Seeds e determinismo â€” OBRIGATÃ“RIO antes de qualquer operaÃ§Ã£o\n",
    "from src.utils.seed import set_global_seed\n",
    "\n",
    "SEED = 42\n",
    "generator = set_global_seed(SEED)\n",
    "print(f'Seed global configurado: {SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA L4\n",
      "CUDA version: 12.4\n",
      "VRAM total: 23.7 GB\n",
      "\n",
      "Usando device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar GPU e versÃµes\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'\\nUsando device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded: stage1_5_separability_audit\n",
      "Dataset: CORAA-MUPE\n",
      "Splits: speaker_disjoint (seed=42)\n"
     ]
    }
   ],
   "source": [
    "# Load experiment config YAML â€” single source of truth\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open('configs/stage1_5.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f'Config loaded: {config[\"experiment\"][\"name\"]}')\n",
    "print(f'Dataset: {config[\"dataset\"][\"name\"]}')\n",
    "print(f'Splits: {config[\"splits\"][\"method\"]} (seed={config[\"splits\"][\"seed\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# HuggingFace authentication â€” required for gated datasets (CORAA-MUPE-ASR).\n# Store your token in Colab Secrets (sidebar â†’ Secrets â†’ HF_TOKEN).\n# Never hardcode tokens in notebooks.\nfrom huggingface_hub import login\n\ntry:\n    from google.colab import userdata\n    _hf_token = userdata.get('HF_TOKEN')\nexcept Exception:\n    _hf_token = None\n\nif _hf_token:\n    login(token=_hf_token)\n    print('HuggingFace: logged in via Colab Secret')\nelse:\n    # Fallback: use cached token from `huggingface-cli login`\n    from huggingface_hub import HfApi\n    try:\n        user = HfApi().whoami()\n        print(f'HuggingFace: logged in as {user[\"name\"]} (cached token)')\n    except Exception:\n        print('WARNING: No HF token found. CORAA-MUPE download may fail.')\n        print('Run: huggingface-cli login  OR  set HF_TOKEN in Colab Secrets')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download e Build Manifest\n",
    "\n",
    "Carrega o CORAA-MUPE-ASR do HuggingFace, filtra por `speaker_type='R'` (entrevistados),\n",
    "duraÃ§Ã£o 3â€“15s e mÃ­nimo de speakers por regiÃ£o. O manifest Ã© o artefato versionado (SHA-256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineCache (filter_hash=1671fa699e9c)\n",
      "  Base: /content/drive/MyDrive/tcc-cache/1671fa699e9c\n",
      "  Manifest: MISSING\n",
      "  Features: NONE\n",
      "  Audio: NONE\n",
      "\n",
      "Downloading CORAA-MUPE-ASR from HuggingFace...\n",
      "(~42 GB na primeira vez â€” usa cache nas prÃ³ximas execuÃ§Ãµes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec42322507346d38012fc8799a788c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131be1669a11424884ca29f869e83242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85db0de400404d49b6a7e48785dbbfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d18cf851bee4c3496c43b8ed70bdbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00003.parquet:   0%|          | 0.00/484M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cab89b5bb84113b9ba36a9eb553672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00001-of-00003.parquet:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e29338d711242708f17643287067730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00002-of-00003.parquet:   0%|          | 0.00/505M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17c3c1b0eb6477a92eaa8a0aee3d666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00008.parquet:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7357290865f542169389c9c9bea8f789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00008.parquet:   0%|          | 0.00/560M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7aab22e5b64d858b601dce4a3f158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00002-of-00008.parquet:   0%|          | 0.00/544M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef946aa2fc9f418bb7beb0b5bb9022b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00003-of-00008.parquet:   0%|          | 0.00/463M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ac7880e79e4018bd86114fdf544e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00004-of-00008.parquet:   0%|          | 0.00/425M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3412bb571c45b5946e51fc0d302596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00005-of-00008.parquet:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad3c1935a734b3ebd3ac9ea662af64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00006-of-00008.parquet:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a5ffc6ec944dd7978a569535b50577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00007-of-00008.parquet:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb82d3a22d64f17b6c3a370ab4315a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/74 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ff9047eed14965821f8dfdad5af698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00074.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287a40db0a241d180d95d98c9ee928c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00074.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab08bef496494beab3329a3729073ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00074.parquet:   0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2884a6bb5ed4735bb74566cd9fd0453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00074.parquet:   0%|          | 0.00/525M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b225aee45f4ef2800b0ebaa0f0d45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00074.parquet:   0%|          | 0.00/684M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7456a08d1c4b7bb525a42525e8a10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00074.parquet:   0%|          | 0.00/464M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92e22416b2240df8dc54aa203a1e896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00074.parquet:   0%|          | 0.00/515M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d755584ded43158095bbb2fd309f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00074.parquet:   0%|          | 0.00/512M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18923a3e6da848b684014a2af885400f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00074.parquet:   0%|          | 0.00/554M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27331a1f39764c4595a2f98a714c649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00074.parquet:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cb0054787346c49b97071a3203df52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00074.parquet:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110df5563e664bfebe4825d496513ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00011-of-00074.parquet:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b6bc29829b465cb261b5b2cbee32a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00074.parquet:   0%|          | 0.00/484M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174ab1257ad34eea81051748cba03ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00074.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144e35e25f0d403dbdc6b5f0a351fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00074.parquet:   0%|          | 0.00/514M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b33d7ca3c04c33af26f672b826ab65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00074.parquet:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d398cd56504ed5a28bb798005425e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00016-of-00074.parquet:   0%|          | 0.00/395M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0daa31aca94a472ebf5ec3a0cd713e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00017-of-00074.parquet:   0%|          | 0.00/557M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8567f4cce3cc4d818d554837beb05700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00018-of-00074.parquet:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bff86d04704485becc389bf5f3fdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00019-of-00074.parquet:   0%|          | 0.00/592M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871bb4d308f4b199cabd3ba732d2fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00020-of-00074.parquet:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcdbc1ab19b4c0a8824f4c9e6c7aad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00021-of-00074.parquet:   0%|          | 0.00/549M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae61b8ffcfaa40cba72b52c27d141f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00022-of-00074.parquet:   0%|          | 0.00/524M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c83bbf7f934fe1863e91cc02410e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00023-of-00074.parquet:   0%|          | 0.00/455M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94cf34612214188ae78ec074e077ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00024-of-00074.parquet:   0%|          | 0.00/444M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7cb72e031d46e4ae705c0155a24901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00025-of-00074.parquet:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7f8a851c6e47aca4421ceb6fe0a9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00026-of-00074.parquet:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a02457bff44f91aab0aa887bf1dd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00027-of-00074.parquet:   0%|          | 0.00/545M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1636c1d84334742b7bd176ac0b4da1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00028-of-00074.parquet:   0%|          | 0.00/503M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ce18b8cb7349ca83801f24c68dda4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00029-of-00074.parquet:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bf031d3b774b159fdd921f98f9a251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00030-of-00074.parquet:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b2fdca494e4deb9a98b02ecafe002c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00031-of-00074.parquet:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4006e61da8884bec8935e6cf8b5c518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00032-of-00074.parquet:   0%|          | 0.00/552M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a670a99a0954892886efd9fd6c2125a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00033-of-00074.parquet:   0%|          | 0.00/404M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001885f94ed4c35aeceb02d5d70f352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00034-of-00074.parquet:   0%|          | 0.00/402M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a812e5c0154a779925c06007595752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00035-of-00074.parquet:   0%|          | 0.00/519M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc969794e59464fb7d3944dd11b8040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00036-of-00074.parquet:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575bf5fbbf004d4abf334db8a1136871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00037-of-00074.parquet:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cb0b79abe842ba8176997b1e23b8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00038-of-00074.parquet:   0%|          | 0.00/455M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbd285a988b4565ad28b686d685c29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00039-of-00074.parquet:   0%|          | 0.00/355M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed48a851bac465a9402f79e74e05f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00040-of-00074.parquet:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31982aa2718844aeb0725b1e3ec39212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00041-of-00074.parquet:   0%|          | 0.00/483M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2776e27ed5427db4395842088a7a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00042-of-00074.parquet:   0%|          | 0.00/545M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ef098dc70a4d4383514e5269e32d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00043-of-00074.parquet:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c36dbb1e7d642378054841452a9d61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00044-of-00074.parquet:   0%|          | 0.00/518M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22edac117699440fbfd292e964017f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00045-of-00074.parquet:   0%|          | 0.00/598M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0715cac72aed49bc8cef8957563d3ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00046-of-00074.parquet:   0%|          | 0.00/574M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e301a85c0743fe9a569eccd76c6608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00047-of-00074.parquet:   0%|          | 0.00/555M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e003c72744d6da81e0c1c9c6cef8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00048-of-00074.parquet:   0%|          | 0.00/489M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadb6cdb76cb4768bd1c9a61350dcaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00049-of-00074.parquet:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0af67702a5d4fa58a3472154755aab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00050-of-00074.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00daa4ad1d5042f5921fd22f77368e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00051-of-00074.parquet:   0%|          | 0.00/449M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b70315639a418ba3fc21f2ebee9b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00052-of-00074.parquet:   0%|          | 0.00/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a40d97ac94542d78494ec096a96c52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00053-of-00074.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044b368f28a849d6928ba6f5da1addec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00054-of-00074.parquet:   0%|          | 0.00/504M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4ea94958374896afe4109516a94f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00055-of-00074.parquet:   0%|          | 0.00/397M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a614d11b8a0a43c48a6e222ce92ed4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00056-of-00074.parquet:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.data.cache import PipelineCache\n",
    "\n",
    "cache = PipelineCache(config, drive_base=DRIVE_BASE)\n",
    "print(cache.report())\n",
    "print()\n",
    "\n",
    "# Initialize variables for both code paths (cache hit vs miss)\n",
    "entries = None\n",
    "build_stats = None\n",
    "\n",
    "if cache.has_manifest():\n",
    "    print('Loading manifest from Drive cache...')\n",
    "    entries = cache.load_manifest()\n",
    "    print(f'Loaded {len(entries):,} entries from cache')\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "    print('Downloading CORAA-MUPE-ASR from HuggingFace...')\n",
    "    print('(~42 GB na primeira vez â€” usa cache nas prÃ³ximas execuÃ§Ãµes)\\n')\n",
    "\n",
    "    ds = load_dataset(\"nilc-nlp/CORAA-MUPE-ASR\", token=True)\n",
    "    print(f'Splits disponÃ­veis: {list(ds.keys())}')\n",
    "    for split_name, split_data in ds.items():\n",
    "        print(f'  {split_name}: {len(split_data):,} rows')\n",
    "\n",
    "    # Concatenar todos os splits â€” criaremos nossos prÃ³prios splits speaker-disjoint\n",
    "    all_data = concatenate_datasets([ds[split] for split in ds.keys()])\n",
    "    print(f'\\nTotal concatenado: {len(all_data):,} rows')\n",
    "    print(f'Colunas: {all_data.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build manifest from HF dataset (only when not loaded from cache)\nif entries is None:\n    from src.data.manifest_builder import build_manifest_from_hf_dataset\n\n    AUDIO_DIR = Path('data/audio/')\n    MANIFEST_PATH = Path(config['dataset']['manifest_path'])\n\n    entries, build_stats = build_manifest_from_hf_dataset(\n        dataset=all_data,\n        audio_output_dir=AUDIO_DIR,\n        manifest_output_path=MANIFEST_PATH,\n        speaker_type_filter=config['dataset']['filters']['speaker_type'],\n        min_duration_s=config['dataset']['filters']['min_duration_s'],\n        max_duration_s=config['dataset']['filters']['max_duration_s'],\n        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n    )\n\n    # Save to cache for next run\n    cache.save_manifest(entries)\n\n    print(f\"Manifest: {len(entries):,} entries\")\n    print(f\"SHA-256: {build_stats['manifest_sha256']}\")\n    print(f\"\\nFilter stats:\")\n    for key, value in build_stats['filter_stats'].items():\n        if isinstance(value, (list, dict)):\n            continue  # skip non-numeric entries (dropped_regions, dropped_speakers)\n        print(f\"  {key}: {value:,}\")\n\n    # Report dropped regions (protocol Â§4.3 fallback)\n    dropped = build_stats['filter_stats'].get('dropped_regions', [])\n    if dropped:\n        print(f\"\\nDropped regions (< {config['dataset']['filters']['min_speakers_per_region']} speakers): {dropped}\")\n        print(\"Fallback per TECHNICAL_VALIDATION_PROTOCOL.md Â§4.3\")\n\n    print(f\"\\nRegiÃµes mantidas:\")\n    for region, info in build_stats['regions'].items():\n        print(f\"  {region}: {info['n_speakers']} speakers, {info['n_utterances']:,} utterances\")\nelse:\n    print(f'Manifest already loaded from cache: {len(entries):,} entries')\n    print('Skipping HF dataset build.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speaker-Disjoint Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.splits import (\n",
    "    generate_speaker_disjoint_splits,\n",
    "    generate_stratified_splits,\n",
    "    save_splits,\n",
    "    save_stratified_splits,\n",
    "    assign_entries_to_splits,\n",
    "    assign_entries_to_stratified_splits,\n",
    ")\n",
    "\n",
    "split_info = generate_speaker_disjoint_splits(\n",
    "    entries,\n",
    "    train_ratio=config['splits']['ratios']['train'],\n",
    "    val_ratio=config['splits']['ratios']['val'],\n",
    "    test_ratio=config['splits']['ratios']['test'],\n",
    "    seed=config['splits']['seed'],\n",
    ")\n",
    "\n",
    "# Persistir splits\n",
    "split_path = save_splits(split_info, Path(config['splits']['output_dir']))\n",
    "print(f\"Splits salvos em: {split_path}\")\n",
    "print(f\"Train: {len(split_info.train_speakers)} speakers, {split_info.utterances_per_split['train']} utts\")\n",
    "print(f\"Val:   {len(split_info.val_speakers)} speakers, {split_info.utterances_per_split['val']} utts\")\n",
    "print(f\"Test:  {len(split_info.test_speakers)} speakers, {split_info.utterances_per_split['test']} utts\")\n",
    "\n",
    "# Assign entries (speaker-disjoint)\n",
    "split_entries = assign_entries_to_splits(entries, split_info)\n",
    "\n",
    "# Verify speaker-disjoint (HARD FAIL if violated â€” KB_HARD_FAIL_RULES Â§1)\n",
    "train_spk = {e.speaker_id for e in split_entries['train']}\n",
    "val_spk = {e.speaker_id for e in split_entries['val']}\n",
    "test_spk = {e.speaker_id for e in split_entries['test']}\n",
    "\n",
    "assert len(train_spk & val_spk) == 0, f'Speaker leakage train->val: {train_spk & val_spk}'\n",
    "assert len(train_spk & test_spk) == 0, f'Speaker leakage train->test: {train_spk & test_spk}'\n",
    "assert len(val_spk & test_spk) == 0, f'Speaker leakage val->test: {val_spk & test_spk}'\n",
    "print('\\nSpeaker-disjoint verification: PASSED')\n",
    "\n",
    "# Generate stratified split for leakage Aâ†’speaker probes\n",
    "stratified_split_info = generate_stratified_splits(\n",
    "    entries,\n",
    "    train_ratio=config['splits']['ratios']['train'],\n",
    "    seed=config['splits']['seed'],\n",
    ")\n",
    "stratified_split_path = save_stratified_splits(\n",
    "    stratified_split_info, Path(config['splits']['output_dir'])\n",
    ")\n",
    "stratified_entries = assign_entries_to_stratified_splits(entries, stratified_split_info)\n",
    "print(f\"\\nStratified splits salvos em: {stratified_split_path}\")\n",
    "print(f\"Stratified Train: {stratified_split_info.utterances_per_split['train']} utts\")\n",
    "print(f\"Stratified Test:  {stratified_split_info.utterances_per_split['test']} utts\")\n",
    "print(f\"Speakers in common: {stratified_split_info.speakers_in_common}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AnÃ¡lise de Confounds\n",
    "\n",
    "**Sanity checks obrigatÃ³rios** (recomendaÃ§Ã£o do mentor):  \n",
    "- Tabela accent Ã— gender com chi-quadrado + Cramer's V  \n",
    "- Histograma de duraÃ§Ã£o por regiÃ£o + Kruskal-Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.confounds import run_all_confound_checks\n",
    "import pandas as pd\n",
    "\n",
    "confound_results = run_all_confound_checks(\n",
    "    entries,\n",
    "    gender_blocking_threshold=config['evaluation']['confounds']['accent_x_gender']['threshold_blocker'],\n",
    "    duration_practical_diff_s=config['evaluation']['confounds']['accent_x_duration']['practical_diff_s'],\n",
    "    snr_practical_diff_db=config['evaluation']['confounds']['accent_x_snr']['practical_diff_db'],\n",
    ")\n",
    "\n",
    "print(\"=== CONFOUND ANALYSIS ===\")\n",
    "for result in confound_results:\n",
    "    status = 'ðŸ”´ BLOCKING' if result.is_blocking else ('ðŸŸ¡ SIGNIFICANT' if result.is_significant else 'ðŸŸ¢ OK')\n",
    "    print(f\"\\n{result.variable_a} Ã— {result.variable_b}: {status}\")\n",
    "    print(f\"  Test: {result.test_name}\")\n",
    "    print(f\"  Statistic: {result.statistic:.4f}\")\n",
    "    print(f\"  p-value: {result.p_value:.6f}\")\n",
    "    print(f\"  Effect size ({result.effect_size_name}): {result.effect_size:.4f}\")\n",
    "    print(f\"  Interpretation: {result.interpretation}\")\n",
    "\n",
    "# Tabela accent x gender\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in entries],\n",
    "    [e.gender for e in entries],\n",
    "    margins=True,\n",
    ")\n",
    "print(\"\\n=== ACCENT Ã— GENDER TABLE ===\")\n",
    "print(gender_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration histogram by region + summary stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "durations_by_region = {}\n",
    "for e in entries:\n",
    "    durations_by_region.setdefault(e.accent, []).append(e.duration_s)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "regions_sorted = sorted(durations_by_region.keys())\n",
    "ax.boxplot(\n",
    "    [durations_by_region[r] for r in regions_sorted],\n",
    "    labels=regions_sorted,\n",
    "    showfliers=False,\n",
    ")\n",
    "ax.set_xlabel('Region (IBGE macro-region)')\n",
    "ax.set_ylabel('Duration (seconds)')\n",
    "ax.set_title('Duration distribution by accent region')\n",
    "plt.tight_layout()\n",
    "\n",
    "Path('reports/figures').mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig('reports/figures/duration_by_region.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\nDuration summary:')\n",
    "for r in regions_sorted:\n",
    "    durs = durations_by_region[r]\n",
    "    print(f'  {r}: mean={np.mean(durs):.2f}s, std={np.std(durs):.2f}s, '\n",
    "          f'median={np.median(durs):.2f}s, n={len(durs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "Quatro fontes de features para probing:\n",
    "1. **Acoustic** (MFCC + pitch + energy) â€” baseline rÃ¡pido, CPU-only\n",
    "2. **ECAPA-TDNN** â€” embeddings de speaker (192-dim)\n",
    "3. **WavLM** â€” SSL features por camada\n",
    "4. **Qwen3-TTS backbone** â€” features internas do modelo-alvo (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from src.features.acoustic import extract_acoustic_features, features_to_vector\n",
    "from src.features.ecapa import extract_ecapa_embedding\n",
    "\n",
    "CHECKPOINT_INTERVAL = 10_000  # Save to Drive cache every N items (crash protection)\n",
    "\n",
    "# 4.1 Acoustic features (CPU, fast with yin pitch tracker)\n",
    "print('=== Acoustic features ===')\n",
    "if cache.has_features('acoustic'):\n",
    "    acoustic_vectors = cache.load_features('acoustic')\n",
    "    if len(acoustic_vectors) >= len(entries):\n",
    "        print(f'Loaded {len(acoustic_vectors)} vectors from cache (complete)')\n",
    "    else:\n",
    "        print(f'Partial cache: {len(acoustic_vectors)}/{len(entries)} vectors. Resuming...')\n",
    "        for entry in tqdm(entries, desc='Acoustic (resume)'):\n",
    "            if entry.utt_id in acoustic_vectors:\n",
    "                continue\n",
    "            feats = extract_acoustic_features(\n",
    "                Path(entry.audio_path), entry.utt_id,\n",
    "                n_mfcc=config['features']['acoustic']['n_mfcc'],\n",
    "            )\n",
    "            acoustic_vectors[entry.utt_id] = features_to_vector(feats)\n",
    "            if len(acoustic_vectors) % CHECKPOINT_INTERVAL == 0:\n",
    "                cache.save_features('acoustic', acoustic_vectors)\n",
    "        cache.save_features('acoustic', acoustic_vectors)\n",
    "        print(f'Completed and cached {len(acoustic_vectors)} vectors')\n",
    "else:\n",
    "    acoustic_vectors = {}\n",
    "    for i, entry in enumerate(tqdm(entries, desc='Acoustic')):\n",
    "        feats = extract_acoustic_features(\n",
    "            Path(entry.audio_path), entry.utt_id,\n",
    "            n_mfcc=config['features']['acoustic']['n_mfcc'],\n",
    "        )\n",
    "        acoustic_vectors[entry.utt_id] = features_to_vector(feats)\n",
    "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            cache.save_features('acoustic', acoustic_vectors)\n",
    "    cache.save_features('acoustic', acoustic_vectors)\n",
    "    print(f'Extracted and cached {len(acoustic_vectors)} vectors')\n",
    "\n",
    "print(f'Dimension: {next(iter(acoustic_vectors.values())).shape}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n",
    "          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 ECAPA-TDNN speaker embeddings (GPU)\n",
    "print('=== ECAPA embeddings ===')\n",
    "if cache.has_features('ecapa'):\n",
    "    ecapa_embeddings = cache.load_features('ecapa')\n",
    "    if len(ecapa_embeddings) >= len(entries):\n",
    "        print(f'Loaded {len(ecapa_embeddings)} embeddings from cache (complete)')\n",
    "    else:\n",
    "        print(f'Partial cache: {len(ecapa_embeddings)}/{len(entries)}. Resuming...')\n",
    "        for entry in tqdm(entries, desc='ECAPA (resume)'):\n",
    "            if entry.utt_id in ecapa_embeddings:\n",
    "                continue\n",
    "            emb = extract_ecapa_embedding(Path(entry.audio_path), device=DEVICE)\n",
    "            ecapa_embeddings[entry.utt_id] = emb\n",
    "            if len(ecapa_embeddings) % CHECKPOINT_INTERVAL == 0:\n",
    "                cache.save_features('ecapa', ecapa_embeddings)\n",
    "        cache.save_features('ecapa', ecapa_embeddings)\n",
    "        print(f'Completed and cached {len(ecapa_embeddings)} embeddings')\n",
    "else:\n",
    "    ecapa_embeddings = {}\n",
    "    for i, entry in enumerate(tqdm(entries, desc='ECAPA')):\n",
    "        emb = extract_ecapa_embedding(Path(entry.audio_path), device=DEVICE)\n",
    "        ecapa_embeddings[entry.utt_id] = emb\n",
    "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            cache.save_features('ecapa', ecapa_embeddings)\n",
    "    cache.save_features('ecapa', ecapa_embeddings)\n",
    "    print(f'Extracted and cached {len(ecapa_embeddings)} embeddings')\n",
    "\n",
    "print(f'Dimension: {next(iter(ecapa_embeddings.values())).shape}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n",
    "          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 WavLM SSL features (layer-wise, with checkpointing + resume)\n",
    "from src.features.ssl import extract_ssl_features\n",
    "\n",
    "SSL_LAYERS = config['features']['ssl']['layers']\n",
    "print(f'=== WavLM features (layers {SSL_LAYERS}) ===')\n",
    "\n",
    "ssl_features = {}\n",
    "\n",
    "# Load whatever is already cached (per layer)\n",
    "layers_complete = []\n",
    "layers_partial = []\n",
    "layers_missing = []\n",
    "\n",
    "for layer in SSL_LAYERS:\n",
    "    cache_key = f'wavlm_layer_{layer}'\n",
    "    if cache.has_features(cache_key):\n",
    "        ssl_features[layer] = cache.load_features(cache_key)\n",
    "        if len(ssl_features[layer]) >= len(entries):\n",
    "            layers_complete.append(layer)\n",
    "        else:\n",
    "            layers_partial.append(layer)\n",
    "        print(f'  Layer {layer}: {len(ssl_features[layer])}/{len(entries)} vectors from cache')\n",
    "    else:\n",
    "        ssl_features[layer] = {}\n",
    "        layers_missing.append(layer)\n",
    "        print(f'  Layer {layer}: no cache')\n",
    "\n",
    "layers_to_extract = layers_partial + layers_missing\n",
    "if not layers_to_extract:\n",
    "    print('All WavLM layers fully cached.')\n",
    "else:\n",
    "    print(f'  Extracting layers: {layers_to_extract} ({len(entries)} utterances)')\n",
    "    extracted_count = 0\n",
    "    for i, entry in enumerate(tqdm(entries, desc='WavLM')):\n",
    "        # Skip if all target layers already have this utterance\n",
    "        if all(entry.utt_id in ssl_features[l] for l in layers_to_extract):\n",
    "            continue\n",
    "\n",
    "        layer_feats = extract_ssl_features(\n",
    "            Path(entry.audio_path),\n",
    "            layers=layers_to_extract,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        for layer_idx, feat_vec in layer_feats.items():\n",
    "            ssl_features[layer_idx][entry.utt_id] = feat_vec\n",
    "\n",
    "        extracted_count += 1\n",
    "        if extracted_count % CHECKPOINT_INTERVAL == 0:\n",
    "            print(f'  Checkpoint at {extracted_count} new extractions...')\n",
    "            for layer in layers_to_extract:\n",
    "                cache.save_features(f'wavlm_layer_{layer}', ssl_features[layer])\n",
    "\n",
    "    # Final save\n",
    "    for layer in layers_to_extract:\n",
    "        cache.save_features(f'wavlm_layer_{layer}', ssl_features[layer])\n",
    "    print(f'  Extracted {extracted_count} new utterances')\n",
    "\n",
    "print(f'WavLM extraction complete')\n",
    "for layer in SSL_LAYERS:\n",
    "    if ssl_features[layer]:\n",
    "        dim = next(iter(ssl_features[layer].values())).shape\n",
    "        print(f'  Layer {layer}: {len(ssl_features[layer])} vectors, dim={dim}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n",
    "          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Qwen3-TTS backbone features (layer-wise, with checkpointing + resume) â€” GPU required\n",
    "from src.features.backbone import extract_backbone_features\n",
    "\n",
    "BACKBONE_LAYERS = config['features']['backbone']['layers']\n",
    "NEUTRAL_TEXT = config['features']['backbone']['neutral_text']  # from config, not hardcoded\n",
    "print(f'=== Backbone features (layers {BACKBONE_LAYERS}) ===')\n",
    "print(f'Neutral text: \"{NEUTRAL_TEXT}\"')\n",
    "\n",
    "backbone_features = {}\n",
    "\n",
    "# Load whatever is already cached (per layer)\n",
    "layers_complete = []\n",
    "layers_partial = []\n",
    "layers_missing = []\n",
    "\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    cache_key = f'backbone_layer_{layer}'\n",
    "    if cache.has_features(cache_key):\n",
    "        backbone_features[layer] = cache.load_features(cache_key)\n",
    "        if len(backbone_features[layer]) >= len(entries):\n",
    "            layers_complete.append(layer)\n",
    "        else:\n",
    "            layers_partial.append(layer)\n",
    "        print(f'  Layer {layer}: {len(backbone_features[layer])}/{len(entries)} vectors from cache')\n",
    "    else:\n",
    "        backbone_features[layer] = {}\n",
    "        layers_missing.append(layer)\n",
    "        print(f'  Layer {layer}: no cache')\n",
    "\n",
    "layers_to_extract = layers_partial + layers_missing\n",
    "if not layers_to_extract:\n",
    "    print('All backbone layers fully cached.')\n",
    "else:\n",
    "    print(f'  Extracting layers: {layers_to_extract} ({len(entries)} utterances)')\n",
    "    extracted_count = 0\n",
    "    for i, entry in enumerate(tqdm(entries, desc='Backbone')):\n",
    "        # Skip if all target layers already have this utterance\n",
    "        if all(entry.utt_id in backbone_features[l] for l in layers_to_extract):\n",
    "            continue\n",
    "\n",
    "        layer_feats = extract_backbone_features(\n",
    "            Path(entry.audio_path),\n",
    "            text=NEUTRAL_TEXT,\n",
    "            layers=layers_to_extract,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        for layer_idx, feat_vec in layer_feats.items():\n",
    "            backbone_features[layer_idx][entry.utt_id] = feat_vec\n",
    "\n",
    "        extracted_count += 1\n",
    "        if extracted_count % CHECKPOINT_INTERVAL == 0:\n",
    "            print(f'  Checkpoint at {extracted_count} new extractions...')\n",
    "            for layer in layers_to_extract:\n",
    "                if backbone_features[layer]:\n",
    "                    cache.save_features(f'backbone_layer_{layer}', backbone_features[layer])\n",
    "\n",
    "    # Final save\n",
    "    for layer in layers_to_extract:\n",
    "        if backbone_features[layer]:\n",
    "            cache.save_features(f'backbone_layer_{layer}', backbone_features[layer])\n",
    "    print(f'  Extracted {extracted_count} new utterances')\n",
    "\n",
    "print(f'Backbone extraction complete')\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    if backbone_features[layer]:\n",
    "        dim = next(iter(backbone_features[layer].values())).shape\n",
    "        print(f'  Layer {layer}: {len(backbone_features[layer])} vectors, dim={dim}')\n",
    "\n",
    "# Free GPU memory after heaviest extraction\n",
    "if torch.cuda.is_available():\n",
    "    print(f'VRAM before cleanup: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated, '\n",
    "          f'{torch.cuda.max_memory_allocated()/1e9:.2f} GB peak')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline ECAPA Speaker Similarity\n",
    "\n",
    "Mede similaridade intra-speaker (mesmo speaker, utterances diferentes) e inter-speaker no Ã¡udio real.  \n",
    "Este baseline Ã© referÃªncia obrigatÃ³ria para Stage 2 (preservaÃ§Ã£o de identidade com LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.ecapa import compute_speaker_similarity_baseline\n",
    "from src.evaluation.bootstrap_ci import bootstrap_cosine_similarity\n",
    "\n",
    "# Group embeddings by speaker\n",
    "speaker_embs = {}\n",
    "for entry in entries:\n",
    "    speaker_embs.setdefault(entry.speaker_id, []).append(\n",
    "        ecapa_embeddings[entry.utt_id]\n",
    "    )\n",
    "\n",
    "sim_baseline = compute_speaker_similarity_baseline(speaker_embs)\n",
    "\n",
    "# CI for intra and inter\n",
    "intra_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['intra']['values']), seed=SEED\n",
    ")\n",
    "inter_ci = bootstrap_cosine_similarity(\n",
    "    np.array(sim_baseline['inter']['values']), seed=SEED\n",
    ")\n",
    "\n",
    "print('=== SPEAKER SIMILARITY BASELINE (ECAPA-TDNN, 192-dim) ===')\n",
    "print(f\"Intra-speaker: {sim_baseline['intra']['mean']:.4f} Â± {sim_baseline['intra']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{intra_ci.ci_lower:.4f}, {intra_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['intra']['n_pairs']}\")\n",
    "print(f\"\\nInter-speaker: {sim_baseline['inter']['mean']:.4f} Â± {sim_baseline['inter']['std']:.4f}\")\n",
    "print(f\"  CI 95%: [{inter_ci.ci_lower:.4f}, {inter_ci.ci_upper:.4f}]\")\n",
    "print(f\"  N pairs: {sim_baseline['inter']['n_pairs']}\")\n",
    "print(f\"\\nSeparation: {sim_baseline['intra']['mean'] - sim_baseline['inter']['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Probes\n",
    "\n",
    "Probe architecture: **Logistic Regression** (linear only â€” protocol requirement).  \n",
    "\n",
    "Split assignments (corrected â€” Achado 1 da auditoria):  \n",
    "- Accent probe: **speaker-disjoint** split  \n",
    "- Speaker probe: **stratified** split  \n",
    "- Leakage Aâ†’speaker: **stratified** split (same speakers in train/test)  \n",
    "- Leakage Sâ†’accent: **speaker-disjoint** split (different speakers in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.probes import (\n",
    "    build_probe_data,\n",
    "    train_linear_probe,\n",
    "    evaluate_probe_against_thresholds,\n",
    "    sweep_regularization,\n",
    "    train_selectivity_control,\n",
    ")\n",
    "from src.evaluation.confusion import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Accent Probe (per layer, speaker-disjoint split)\n",
    "# Initialize probe result collectors (reset on re-execution for idempotency)\n",
    "all_probe_results = []\n",
    "all_selectivity_results = []\n",
    "\n",
    "print('=== ACCENT PROBES ===')\n",
    "\n",
    "# Build train/test for speaker-disjoint\n",
    "train_entries = split_entries['train']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Probe each feature source\n",
    "feature_sources = {}\n",
    "\n",
    "# Acoustic\n",
    "feature_sources['acoustic'] = acoustic_vectors\n",
    "\n",
    "# ECAPA\n",
    "feature_sources['ecapa'] = ecapa_embeddings\n",
    "\n",
    "# WavLM layers\n",
    "for layer in SSL_LAYERS:\n",
    "    feature_sources[f'wavlm_layer_{layer}'] = ssl_features[layer]\n",
    "\n",
    "# Backbone layers\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    if backbone_features[layer]:\n",
    "        feature_sources[f'backbone_layer_{layer}'] = backbone_features[layer]\n",
    "\n",
    "C_values = config['probes']['regularization_C']\n",
    "\n",
    "for source_name, feat_dict in feature_sources.items():\n",
    "    X_train, y_train = build_probe_data(feat_dict, train_entries, 'accent')\n",
    "    X_test, y_test = build_probe_data(feat_dict, test_entries, 'accent')\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f'  {source_name}: SKIPPED (no data)')\n",
    "        continue\n",
    "    \n",
    "    # Sweep regularization to find best C\n",
    "    sweep_results = sweep_regularization(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        C_values=C_values,\n",
    "        probe_name=f'accent_{source_name}',\n",
    "        feature_source=source_name,\n",
    "        target='accent',\n",
    "        split_type='speaker_disjoint',\n",
    "        seed=SEED,\n",
    "    )\n",
    "    best_sweep = max(sweep_results, key=lambda r: r.balanced_accuracy)\n",
    "    best_C = best_sweep.regularization_C\n",
    "    \n",
    "    # Re-train with best C and full CI\n",
    "    result = train_linear_probe(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        probe_name=f'accent_{source_name}',\n",
    "        feature_source=source_name,\n",
    "        target='accent',\n",
    "        split_type='speaker_disjoint',\n",
    "        C=best_C,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    all_probe_results.append(result)\n",
    "    \n",
    "    decision = evaluate_probe_against_thresholds(\n",
    "        result, config['thresholds']['accent_probe']\n",
    "    )\n",
    "    print(f'  {source_name}: bal_acc={result.balanced_accuracy:.4f} '\n",
    "          f'CI=[{result.ci.ci_lower:.4f}, {result.ci.ci_upper:.4f}] '\n",
    "          f'delta={result.delta_pp:+.1f}pp C={best_C} â†’ {decision}')\n",
    "\n",
    "# Selectivity control for accent probes\n",
    "print('\\n=== SELECTIVITY CONTROL (accent probes) ===')\n",
    "accent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\n",
    "for result in accent_results:\n",
    "    feat_dict = feature_sources[result.feature_source]\n",
    "    X_train, y_train = build_probe_data(feat_dict, train_entries, 'accent')\n",
    "    X_test, y_test = build_probe_data(feat_dict, test_entries, 'accent')\n",
    "    \n",
    "    sel = train_selectivity_control(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        real_result=result,\n",
    "        seed=SEED,\n",
    "        C=result.regularization_C,\n",
    "    )\n",
    "    sel['probe_name'] = result.probe_name\n",
    "    sel['feature_source'] = result.feature_source\n",
    "    all_selectivity_results.append(sel)\n",
    "    \n",
    "    print(f'  {result.feature_source}: real={sel[\"real_bal_acc\"]:.4f} '\n",
    "          f'permuted={sel[\"permuted_bal_acc_mean\"]:.4f}Â±{sel[\"permuted_bal_acc_std\"]:.4f} '\n",
    "          f'selectivity={sel[\"selectivity_pp\"]:+.1f}pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Leakage Probes\n",
    "# Remove previous leakage results for idempotent re-execution\n",
    "all_probe_results = [r for r in all_probe_results if 'leakage' not in r.probe_name]\n",
    "all_selectivity_results = [s for s in all_selectivity_results if 'leakage' not in s.get('probe_name', '')]\n",
    "\n",
    "print('\\n=== LEAKAGE PROBES ===')\n",
    "\n",
    "# --- Leakage Aâ†’speaker: Do accent features contain speaker identity? ---\n",
    "# Uses STRATIFIED split (same speakers in train/test â€” we need known speakers)\n",
    "print('Leakage Aâ†’speaker (accent feature sources, stratified split):')\n",
    "\n",
    "strat_train_entries = stratified_entries['train']\n",
    "strat_test_entries = stratified_entries['test']\n",
    "\n",
    "# Accent feature sources: WavLM layers, backbone layers, acoustic\n",
    "# (NOT ECAPA â€” those are speaker embeddings, not accent features)\n",
    "leakage_a2s_sources = {}\n",
    "for layer in SSL_LAYERS:\n",
    "    leakage_a2s_sources[f'wavlm_layer_{layer}'] = ssl_features[layer]\n",
    "for layer in BACKBONE_LAYERS:\n",
    "    if backbone_features[layer]:\n",
    "        leakage_a2s_sources[f'backbone_layer_{layer}'] = backbone_features[layer]\n",
    "leakage_a2s_sources['acoustic'] = acoustic_vectors\n",
    "\n",
    "leakage_a2s_results = []\n",
    "for source_name, feat_dict in leakage_a2s_sources.items():\n",
    "    X_train, y_train = build_probe_data(feat_dict, strat_train_entries, 'speaker_id')\n",
    "    X_test, y_test = build_probe_data(feat_dict, strat_test_entries, 'speaker_id')\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f'  {source_name}: SKIPPED (no data)')\n",
    "        continue\n",
    "\n",
    "    result = train_linear_probe(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        probe_name=f'leakage_a2s_{source_name}',\n",
    "        feature_source=source_name,\n",
    "        target='speaker_id',\n",
    "        split_type='stratified',\n",
    "        C=config['probes']['default_C'],\n",
    "        seed=SEED,\n",
    "    )\n",
    "    leakage_a2s_results.append(result)\n",
    "    all_probe_results.append(result)\n",
    "\n",
    "    leak_decision = evaluate_probe_against_thresholds(\n",
    "        result, config['thresholds']['leakage']\n",
    "    )\n",
    "    print(f'  {source_name}: bal_acc={result.balanced_accuracy:.4f} '\n",
    "          f'chance={result.chance_level:.4f} '\n",
    "          f'delta={result.delta_pp:+.1f}pp â†’ {leak_decision}')\n",
    "\n",
    "# Selectivity control for Aâ†’speaker leakage\n",
    "print('\\n=== SELECTIVITY CONTROL (leakage Aâ†’speaker) ===')\n",
    "for result in leakage_a2s_results:\n",
    "    feat_dict = leakage_a2s_sources[result.feature_source]\n",
    "    X_train, y_train = build_probe_data(feat_dict, strat_train_entries, 'speaker_id')\n",
    "    X_test, y_test = build_probe_data(feat_dict, strat_test_entries, 'speaker_id')\n",
    "\n",
    "    sel = train_selectivity_control(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        real_result=result,\n",
    "        seed=SEED,\n",
    "        C=result.regularization_C,\n",
    "    )\n",
    "    sel['probe_name'] = result.probe_name\n",
    "    sel['feature_source'] = result.feature_source\n",
    "    all_selectivity_results.append(sel)\n",
    "\n",
    "    print(f'  {result.feature_source}: real={sel[\"real_bal_acc\"]:.4f} '\n",
    "          f'permuted={sel[\"permuted_bal_acc_mean\"]:.4f}Â±{sel[\"permuted_bal_acc_std\"]:.4f} '\n",
    "          f'selectivity={sel[\"selectivity_pp\"]:+.1f}pp')\n",
    "\n",
    "# --- Leakage Sâ†’accent: Do speaker features contain accent info? ---\n",
    "# Uses SPEAKER-DISJOINT split (different speakers in test â€” tests generalization)\n",
    "print('\\nLeakage Sâ†’accent (ECAPA embeddings, speaker-disjoint split):')\n",
    "X_train, y_train = build_probe_data(ecapa_embeddings, train_entries, 'accent')\n",
    "X_test, y_test = build_probe_data(ecapa_embeddings, test_entries, 'accent')\n",
    "\n",
    "leakage_s2a = train_linear_probe(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    probe_name='leakage_s2a_ecapa',\n",
    "    feature_source='ecapa',\n",
    "    target='accent',\n",
    "    split_type='speaker_disjoint',\n",
    "    C=config['probes']['default_C'],\n",
    "    seed=SEED,\n",
    ")\n",
    "all_probe_results.append(leakage_s2a)\n",
    "\n",
    "leak_decision = evaluate_probe_against_thresholds(\n",
    "    leakage_s2a, config['thresholds']['leakage']\n",
    ")\n",
    "print(f'  bal_acc={leakage_s2a.balanced_accuracy:.4f} '\n",
    "      f'chance={leakage_s2a.chance_level:.4f} '\n",
    "      f'delta={leakage_s2a.delta_pp:+.1f}pp â†’ {leak_decision}')\n",
    "\n",
    "# Selectivity control for Sâ†’accent leakage\n",
    "sel_s2a = train_selectivity_control(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    real_result=leakage_s2a,\n",
    "    seed=SEED,\n",
    "    C=leakage_s2a.regularization_C,\n",
    ")\n",
    "sel_s2a['probe_name'] = leakage_s2a.probe_name\n",
    "sel_s2a['feature_source'] = leakage_s2a.feature_source\n",
    "all_selectivity_results.append(sel_s2a)\n",
    "print(f'  selectivity: real={sel_s2a[\"real_bal_acc\"]:.4f} '\n",
    "      f'permuted={sel_s2a[\"permuted_bal_acc_mean\"]:.4f}Â±{sel_s2a[\"permuted_bal_acc_std\"]:.4f} '\n",
    "      f'selectivity={sel_s2a[\"selectivity_pp\"]:+.1f}pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Confusion Matrices (best accent probe)\n",
    "accent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\n",
    "if accent_results:\n",
    "    best = max(accent_results, key=lambda r: r.balanced_accuracy)\n",
    "    print(f'Best accent probe: {best.feature_source} (bal_acc={best.balanced_accuracy:.4f})')\n",
    "    \n",
    "    if best.confusion_matrix is not None:\n",
    "        Path('reports/figures').mkdir(parents=True, exist_ok=True)\n",
    "        plot_confusion_matrix(\n",
    "            best.confusion_matrix,\n",
    "            best.confusion_labels,\n",
    "            title=f'Accent Confusion Matrix ({best.feature_source})',\n",
    "            output_path=Path('reports/figures/confusion_matrix_accent.png'),\n",
    "        )\n",
    "        print('Confusion matrix saved to reports/figures/confusion_matrix_accent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness (Multiple Seeds)\n",
    "\n",
    "Repete o melhor probe com 3 seeds para reportar mÃ©dia e desvio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBUSTNESS_SEEDS = config['seed']['robustness_seeds']\n",
    "print(f'=== ROBUSTNESS CHECK (seeds: {ROBUSTNESS_SEEDS}) ===')\n",
    "\n",
    "if accent_results:\n",
    "    best_source = best.feature_source\n",
    "    best_features = feature_sources[best_source]\n",
    "    \n",
    "    seed_results = []\n",
    "    for s in ROBUSTNESS_SEEDS:\n",
    "        set_global_seed(s)\n",
    "        X_tr, y_tr = build_probe_data(best_features, train_entries, 'accent')\n",
    "        X_te, y_te = build_probe_data(best_features, test_entries, 'accent')\n",
    "        \n",
    "        r = train_linear_probe(\n",
    "            X_tr, y_tr, X_te, y_te,\n",
    "            probe_name=f'accent_{best_source}_seed{s}',\n",
    "            feature_source=best_source,\n",
    "            target='accent',\n",
    "            split_type='speaker_disjoint',\n",
    "            seed=s,\n",
    "            compute_ci=True,\n",
    "        )\n",
    "        seed_results.append(r)\n",
    "        print(f'  Seed {s}: bal_acc={r.balanced_accuracy:.4f} CI=[{r.ci.ci_lower:.4f}, {r.ci.ci_upper:.4f}]')\n",
    "    \n",
    "    accs = [r.balanced_accuracy for r in seed_results]\n",
    "    print(f'\\n  Mean: {np.mean(accs):.4f} Â± {np.std(accs):.4f}')\n",
    "    \n",
    "    # Restore original seed\n",
    "    set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gate Decision\n",
    "\n",
    "AvaliaÃ§Ã£o automÃ¡tica contra os thresholds do protocolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.probes import evaluate_probe_against_thresholds\n",
    "\n",
    "# Gate decision: evaluate all probes against protocol thresholds\n",
    "print('=== STAGE 1.5 GATE DECISION ===\\n')\n",
    "\n",
    "overall = 'NOT_EVALUATED'  # safe default â€” overwritten below if all checks run\n",
    "\n",
    "# 1. Accent probes â€” at least one must reach GO or GO_CONDITIONAL\n",
    "accent_results = [r for r in all_probe_results if r.target == 'accent' and 'leakage' not in r.probe_name]\n",
    "accent_decisions = []\n",
    "for r in accent_results:\n",
    "    d = evaluate_probe_against_thresholds(r, config['thresholds']['accent_probe'])\n",
    "    accent_decisions.append((r.feature_source, r.balanced_accuracy, r.delta_pp, d))\n",
    "    print(f'  Accent {r.feature_source}: bal_acc={r.balanced_accuracy:.4f} delta={r.delta_pp:+.1f}pp â†’ {d}')\n",
    "\n",
    "accent_pass = any(d in ('GO', 'GO_CONDITIONAL') for _, _, _, d in accent_decisions)\n",
    "print(f'\\n  Accent gate: {\"GO\" if accent_pass else \"FAIL\"} (at least one source above threshold)')\n",
    "\n",
    "# 2. Leakage probes â€” all must be GO or GO_CONDITIONAL (below threshold)\n",
    "leakage_results = [r for r in all_probe_results if 'leakage' in r.probe_name]\n",
    "leakage_decisions = []\n",
    "for r in leakage_results:\n",
    "    d = evaluate_probe_against_thresholds(r, config['thresholds']['leakage'])\n",
    "    leakage_decisions.append((r.probe_name, r.balanced_accuracy, r.delta_pp, d))\n",
    "    print(f'  Leakage {r.probe_name}: bal_acc={r.balanced_accuracy:.4f} delta={r.delta_pp:+.1f}pp â†’ {d}')\n",
    "\n",
    "leakage_pass = all(d in ('GO', 'GO_CONDITIONAL') for _, _, _, d in leakage_decisions)\n",
    "print(f'\\n  Leakage gate: {\"GO\" if leakage_pass else \"FAIL\"} (all probes below threshold)')\n",
    "\n",
    "# 3. Confounds â€” no blocking confound\n",
    "confound_pass = not any(r.is_blocking for r in confound_results)\n",
    "print(f'  Confound gate: {\"GO\" if confound_pass else \"FAIL\"} (no blocking confounds)')\n",
    "\n",
    "# 4. Overall decision\n",
    "if accent_pass and leakage_pass and confound_pass:\n",
    "    overall = 'GO'\n",
    "elif accent_pass and confound_pass:\n",
    "    overall = 'ADJUST'  # signal exists but leakage needs attention\n",
    "else:\n",
    "    overall = 'FAIL'\n",
    "\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'  STAGE 1.5 GATE: {overall}')\n",
    "print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from src.utils.git import get_commit_hash\n",
    "\n",
    "commit_hash = get_commit_hash()\n",
    "\n",
    "# Manifest SHA-256 â€” from build_stats if available, or recompute from file\n",
    "manifest_sha256 = None\n",
    "if build_stats is not None:\n",
    "    manifest_sha256 = build_stats.get('manifest_sha256')\n",
    "elif cache.has_manifest():\n",
    "    from src.data.manifest import compute_file_hash\n",
    "    manifest_sha256 = compute_file_hash(cache.get_manifest_path())\n",
    "\n",
    "# Compute region stats (fallback when build_stats is unavailable)\n",
    "if build_stats and build_stats.get('regions'):\n",
    "    region_stats = build_stats['regions']\n",
    "else:\n",
    "    speakers_by_region = defaultdict(set)\n",
    "    utts_by_region = defaultdict(int)\n",
    "    for e in entries:\n",
    "        speakers_by_region[e.accent].add(e.speaker_id)\n",
    "        utts_by_region[e.accent] += 1\n",
    "    region_stats = {\n",
    "        region: {'n_speakers': len(speakers_by_region[region]), 'n_utterances': utts_by_region[region]}\n",
    "        for region in sorted(speakers_by_region)\n",
    "    }\n",
    "\n",
    "report = {\n",
    "    'experiment': config['experiment']['name'],\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'commit_hash': commit_hash,\n",
    "    'seed': SEED,\n",
    "    'filter_hash': cache.filter_hash,\n",
    "    'environment': {\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n",
    "        'cudnn_version': torch.backends.cudnn.version() if torch.cuda.is_available() else None,\n",
    "        'torch_version': torch.__version__,\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': config['dataset']['name'],\n",
    "        'manifest_sha256': manifest_sha256,\n",
    "        'total_entries': len(entries),\n",
    "        'regions': region_stats,\n",
    "    },\n",
    "    'splits': split_info.to_dict(),\n",
    "    'stratified_splits': stratified_split_info.to_dict(),\n",
    "    'confounds': [\n",
    "        {\n",
    "            'test': r.test_name,\n",
    "            'variables': f'{r.variable_a} x {r.variable_b}',\n",
    "            'statistic': r.statistic,\n",
    "            'p_value': r.p_value,\n",
    "            'effect_size': r.effect_size,\n",
    "            'is_blocking': r.is_blocking,\n",
    "            'interpretation': r.interpretation,\n",
    "        }\n",
    "        for r in confound_results\n",
    "    ],\n",
    "    'speaker_similarity_baseline': {\n",
    "        'intra': {\n",
    "            'mean': sim_baseline['intra']['mean'],\n",
    "            'std': sim_baseline['intra']['std'],\n",
    "            'ci_lower': intra_ci.ci_lower,\n",
    "            'ci_upper': intra_ci.ci_upper,\n",
    "            'n_pairs': sim_baseline['intra']['n_pairs'],\n",
    "        },\n",
    "        'inter': {\n",
    "            'mean': sim_baseline['inter']['mean'],\n",
    "            'std': sim_baseline['inter']['std'],\n",
    "            'ci_lower': inter_ci.ci_lower,\n",
    "            'ci_upper': inter_ci.ci_upper,\n",
    "            'n_pairs': sim_baseline['inter']['n_pairs'],\n",
    "        },\n",
    "    },\n",
    "    'probes': [\n",
    "        {\n",
    "            'name': r.probe_name,\n",
    "            'feature_source': r.feature_source,\n",
    "            'target': r.target,\n",
    "            'split_type': r.split_type,\n",
    "            'balanced_accuracy': r.balanced_accuracy,\n",
    "            'f1_macro': r.f1_macro,\n",
    "            'chance_level': r.chance_level,\n",
    "            'delta_pp': r.delta_pp,\n",
    "            'ci_lower': r.ci.ci_lower if r.ci else None,\n",
    "            'ci_upper': r.ci.ci_upper if r.ci else None,\n",
    "            'n_train': r.n_train,\n",
    "            'n_test': r.n_test,\n",
    "            'n_classes': r.n_classes,\n",
    "            'C': r.regularization_C,\n",
    "        }\n",
    "        for r in all_probe_results\n",
    "    ],\n",
    "    'selectivity_controls': all_selectivity_results,\n",
    "    'gate_decision': overall,\n",
    "}\n",
    "\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "report_path = Path('reports/stage1_5_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f'Report saved to {report_path}')\n",
    "print(f'Filter hash: {cache.filter_hash}')\n",
    "print(f'Total probe results: {len(all_probe_results)}')\n",
    "print(f'Total selectivity controls: {len(all_selectivity_results)}')\n",
    "print(f'Gate decision: {report[\"gate_decision\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}