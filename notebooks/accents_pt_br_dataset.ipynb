{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accents-PT-BR — Dataset Pipeline + HuggingFace Publication\n",
    "\n",
    "**Projeto:** Controle Explícito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Construir o dataset derivado **Accents-PT-BR** (CORAA-MUPE + Common Voice PT),  \n",
    "executar toda a pipeline de validação (confounds, splits) e publicar no HuggingFace Hub.  \n",
    "**Config:** `configs/accent_classifier.yaml` (single source of truth).  \n",
    "\n",
    "**Seções:**\n",
    "1. Setup do ambiente\n",
    "2. CORAA-MUPE manifest\n",
    "3. Common Voice PT manifest\n",
    "4. Dataset combinado Accents-PT-BR\n",
    "5. Análise de confounds (accent × gender, duration, source)\n",
    "6. Speaker-disjoint splits\n",
    "7. Construção do HuggingFace Dataset\n",
    "8. Dataset card + publicação no HuggingFace Hub\n",
    "\n",
    "Este notebook é a **camada de orquestração**. Toda lógica está em `src/` (testável, auditável).  \n",
    "O notebook apenas: instala deps → configura ambiente → chama módulos → publica resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, subprocess, sys\n\n# --- Platform-aware setup: works on Colab, Lightning.ai, and local ---\n# Detection order: Lightning.ai -> Google Colab -> Local\n\n# 1. Determine repo directory\n_lightning_studio = '/teamspace/studios/this_studio'\nif os.path.exists(_lightning_studio):\n    REPO_DIR = os.path.join(_lightning_studio, 'TCC')\n    _platform = 'lightning'\nelif 'google.colab' in sys.modules or os.path.exists('/content'):\n    REPO_DIR = '/content/TCC'\n    _platform = 'colab'\nelse:\n    REPO_DIR = os.getcwd()\n    _platform = 'local'\n\n# 2. Clone repo if needed (idempotent)\nif not os.path.exists(os.path.join(REPO_DIR, '.git')):\n    subprocess.run(['rm', '-rf', REPO_DIR], check=False)\n    subprocess.run(\n        ['git', 'clone', 'https://github.com/paulohenriquevn/tcc.git', REPO_DIR],\n        check=True,\n    )\n\nos.chdir(REPO_DIR)\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\n\n# 3. Install dependencies\nsubprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt', '-q'], check=True)\nsubprocess.run([sys.executable, '-m', 'pip', 'install', 'huggingface_hub', '-q'], check=True)\n\n# 4. NumPy ABI check — Colab pre-loads numpy 2.x in memory, but\n#    requirements.txt pins 1.26.4. After pip downgrades, stale C-extensions\n#    cause binary incompatibility. Fix: restart runtime ONCE.\n_installed_np = subprocess.check_output(\n    [sys.executable, '-c', 'import numpy; print(numpy.__version__)'],\n    text=True,\n).strip()\n\ntry:\n    import numpy as _np\n    _loaded_np = _np.__version__\nexcept Exception:\n    _loaded_np = None\n\nif _loaded_np != _installed_np:\n    print(f'\\nNumPy ABI mismatch: loaded={_loaded_np}, installed={_installed_np}')\n    print('Restarting runtime... After restart, re-run this cell (no second restart).')\n    os.kill(os.getpid(), 9)\nelse:\n    print(f'\\nPlatform: {_platform}')\n    print(f'Repo: {REPO_DIR}')\n    print(f'Environment OK (numpy=={_installed_np})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, os, yaml, json, logging, hashlib\nfrom pathlib import Path\nfrom collections import Counter\nfrom datetime import datetime\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\n# Platform-aware persistent cache setup\nfrom src.utils.platform import detect_platform, setup_environment\n\nplatform = detect_platform()\nsetup_environment(platform)\n\n# Mount Google Drive only on Colab (Lightning.ai has persistent disk built-in)\nif platform.needs_drive_mount:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\nfrom src.utils.seed import set_global_seed\nfrom src.data.manifest import (\n    ManifestEntry, read_manifest, write_manifest,\n    normalize_cv_accent, compute_file_hash,\n)\nfrom src.data.manifest_builder import build_manifest_from_hf_dataset\nfrom src.data.cv_manifest_builder import build_manifest_from_common_voice\nfrom src.data.combined_manifest import combine_manifests, analyze_source_distribution\nfrom src.data.splits import (\n    generate_speaker_disjoint_splits,\n    save_splits,\n    assign_entries_to_splits,\n)\nfrom src.analysis.confounds import run_all_confound_checks\n\n# Load config — single source of truth for all experiment parameters\nwith open('configs/accent_classifier.yaml') as f:\n    config = yaml.safe_load(f)\n\nSEED = config['seed']['global']\ngenerator = set_global_seed(SEED)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(name)s - %(levelname)s - %(message)s',\n)\n\n# Drive cache base directory — platform-aware\nDRIVE_BASE = platform.cache_base\nDRIVE_BASE.mkdir(parents=True, exist_ok=True)\n\nprint(f'Platform: {platform.name}')\nprint(f'Config: {config[\"experiment\"][\"name\"]}')\nprint(f'Seed: {SEED}')\nprint(f'Cache: {DRIVE_BASE}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CORAA-MUPE Manifest\n",
    "\n",
    "Download CORAA-MUPE-ASR from HuggingFace, apply filters, build manifest JSONL.  \n",
    "**Filtros:** `speaker_type='R'`, duração 3–15s, `birth_state` → macro-região IBGE.  \n",
    "Runs subsequentes usam cache do Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "CORAA_AUDIO_DIR = DRIVE_BASE / 'coraa_mupe' / 'audio'\n",
    "CORAA_MANIFEST_PATH = DRIVE_BASE / 'coraa_mupe' / 'manifest.jsonl'\n",
    "\n",
    "if CORAA_MANIFEST_PATH.exists():\n",
    "    print(f'Loading CORAA-MUPE manifest from cache: {CORAA_MANIFEST_PATH}')\n",
    "    coraa_entries = read_manifest(CORAA_MANIFEST_PATH)\n",
    "    coraa_sha256 = compute_file_hash(CORAA_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(coraa_entries):,} entries (SHA-256: {coraa_sha256[:16]}...)')\n",
    "else:\n",
    "    print('Downloading CORAA-MUPE-ASR from HuggingFace...')\n",
    "    print('(~42 GB na primeira vez)')\n",
    "\n",
    "    ds = load_dataset('nilc-nlp/CORAA-MUPE-ASR')\n",
    "    print(f'Splits: {list(ds.keys())}')\n",
    "\n",
    "    # Concatenate all splits — we create our own speaker-disjoint splits\n",
    "    all_data = concatenate_datasets([ds[split] for split in ds.keys()])\n",
    "    print(f'Total concatenado: {len(all_data):,} rows')\n",
    "\n",
    "    coraa_entries, coraa_stats = build_manifest_from_hf_dataset(\n",
    "        dataset=all_data,\n",
    "        audio_output_dir=CORAA_AUDIO_DIR,\n",
    "        manifest_output_path=CORAA_MANIFEST_PATH,\n",
    "        speaker_type_filter=config['dataset']['filters'].get('speaker_type', 'R'),\n",
    "        min_duration_s=config['dataset']['filters']['min_duration_s'],\n",
    "        max_duration_s=config['dataset']['filters']['max_duration_s'],\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    coraa_sha256 = coraa_stats['manifest_sha256']\n",
    "    print(f'CORAA-MUPE: {len(coraa_entries):,} entries, SHA-256: {coraa_sha256}')\n",
    "\n",
    "region_counts = Counter(e.accent for e in coraa_entries)\n",
    "print(f'\\nCORAA-MUPE: {len(coraa_entries):,} entries')\n",
    "print(f'Regions: {dict(sorted(region_counts.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common Voice PT Manifest\n",
    "\n",
    "Common Voice Portuguese (v17.0): campo `accent` user-submitted, normalizado via `normalize_cv_accent()`.  \n",
    "IDs prefixados com `cv_` para evitar colisões com CORAA-MUPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUDIO_DIR = DRIVE_BASE / 'common_voice_pt' / 'audio'\n",
    "CV_MANIFEST_PATH = DRIVE_BASE / 'common_voice_pt' / 'manifest.jsonl'\n",
    "\n",
    "if CV_MANIFEST_PATH.exists():\n",
    "    print(f'Loading Common Voice PT manifest from cache: {CV_MANIFEST_PATH}')\n",
    "    cv_entries = read_manifest(CV_MANIFEST_PATH)\n",
    "    cv_sha256 = compute_file_hash(CV_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(cv_entries):,} entries (SHA-256: {cv_sha256[:16]}...)')\n",
    "else:\n",
    "    print('Loading Common Voice PT from HuggingFace...')\n",
    "\n",
    "    cv_hf_id = config['dataset']['sources'][1]['hf_id']\n",
    "    cv_lang = config['dataset']['sources'][1]['hf_lang']\n",
    "\n",
    "    cv_dataset = load_dataset(cv_hf_id, cv_lang, split='validated')\n",
    "    print(f'Common Voice validated: {len(cv_dataset):,} rows')\n",
    "\n",
    "    cv_entries, cv_stats = build_manifest_from_common_voice(\n",
    "        dataset=cv_dataset,\n",
    "        audio_output_dir=CV_AUDIO_DIR,\n",
    "        manifest_output_path=CV_MANIFEST_PATH,\n",
    "        min_duration_s=config['dataset']['filters']['min_duration_s'],\n",
    "        max_duration_s=config['dataset']['filters']['max_duration_s'],\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    cv_sha256 = cv_stats['manifest_sha256']\n",
    "    print(f'Common Voice PT: {len(cv_entries):,} entries, SHA-256: {cv_sha256}')\n",
    "\n",
    "region_counts_cv = Counter(e.accent for e in cv_entries)\n",
    "print(f'\\nCommon Voice PT: {len(cv_entries):,} entries')\n",
    "print(f'Regions: {dict(sorted(region_counts_cv.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Combinado Accents-PT-BR\n",
    "\n",
    "Merge CORAA-MUPE + Common Voice: validação de colisões, consistência speaker→accent, filtros de região."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_MANIFEST_PATH = DRIVE_BASE / 'accents_pt_br' / 'manifest.jsonl'\n",
    "\n",
    "if COMBINED_MANIFEST_PATH.exists():\n",
    "    print(f'Loading combined manifest from cache: {COMBINED_MANIFEST_PATH}')\n",
    "    combined_entries = read_manifest(COMBINED_MANIFEST_PATH)\n",
    "    combined_sha256 = compute_file_hash(COMBINED_MANIFEST_PATH)\n",
    "    print(f'Loaded {len(combined_entries):,} entries (SHA-256: {combined_sha256[:16]}...)')\n",
    "else:\n",
    "    combined_entries, combined_stats = combine_manifests(\n",
    "        manifests=[\n",
    "            (CORAA_MANIFEST_PATH, 'CORAA-MUPE'),\n",
    "            (CV_MANIFEST_PATH, 'CommonVoice-PT'),\n",
    "        ],\n",
    "        output_path=COMBINED_MANIFEST_PATH,\n",
    "        min_speakers_per_region=config['dataset']['filters']['min_speakers_per_region'],\n",
    "        min_utterances_per_speaker=config['dataset']['filters'].get('min_utterances_per_speaker', 3),\n",
    "    )\n",
    "    combined_sha256 = combined_stats['manifest_sha256']\n",
    "    print(f'Combined: {len(combined_entries):,} entries, SHA-256: {combined_sha256}')\n",
    "\n",
    "# Source distribution analysis\n",
    "source_dist = analyze_source_distribution(combined_entries)\n",
    "\n",
    "print(f'\\n=== SOURCE DISTRIBUTION ===')\n",
    "for src, counts in source_dist['source_x_accent'].items():\n",
    "    print(f'  {src}: {dict(sorted(counts.items()))}')\n",
    "\n",
    "if source_dist['warnings']:\n",
    "    print(f'\\nWARNINGS:')\n",
    "    for w in source_dist['warnings']:\n",
    "        print(f'  {w}')\n",
    "\n",
    "total_speakers = len({e.speaker_id for e in combined_entries})\n",
    "region_counts_all = Counter(e.accent for e in combined_entries)\n",
    "print(f'\\nTotal: {len(combined_entries):,} entries, {total_speakers} speakers')\n",
    "print(f'Regions: {dict(sorted(region_counts_all.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análise de Confounds\n",
    "\n",
    "Checks obrigatórios: accent × gender (chi² + Cramer’s V), accent × duration (Kruskal-Wallis), accent × source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_results = run_all_confound_checks(\n",
    "    combined_entries,\n",
    "    gender_blocking_threshold=config['confounds']['accent_x_gender']['threshold_blocker'],\n",
    "    duration_practical_diff_s=config['confounds']['accent_x_duration']['practical_diff_s'],\n",
    "    check_snr=False,\n",
    "    source_blocking_threshold=config['confounds']['accent_x_source']['threshold_blocker'],\n",
    ")\n",
    "\n",
    "print('=== CONFOUND ANALYSIS ===')\n",
    "blocking_found = False\n",
    "confound_summary = []\n",
    "for result in confound_results:\n",
    "    status = 'BLOCKING' if result.is_blocking else ('SIGNIFICANT' if result.is_significant else 'OK')\n",
    "    if result.is_blocking:\n",
    "        blocking_found = True\n",
    "    print(f'\\n{result.variable_a} x {result.variable_b}: {status}')\n",
    "    print(f'  {result.test_name}: stat={result.statistic:.4f}, p={result.p_value:.6f}')\n",
    "    print(f'  {result.effect_size_name}={result.effect_size:.4f}')\n",
    "    confound_summary.append({\n",
    "        'test': result.test_name,\n",
    "        'variables': f'{result.variable_a} x {result.variable_b}',\n",
    "        'statistic': result.statistic,\n",
    "        'p_value': result.p_value,\n",
    "        'effect_size': result.effect_size,\n",
    "        'effect_size_name': result.effect_size_name,\n",
    "        'is_blocking': result.is_blocking,\n",
    "    })\n",
    "\n",
    "# Cross-tabulations\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.gender for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x GENDER ===')\n",
    "print(gender_table)\n",
    "\n",
    "source_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.source for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x SOURCE ===')\n",
    "print(source_table)\n",
    "\n",
    "if blocking_found:\n",
    "    print('\\n*** BLOCKING CONFOUND DETECTED. Review before proceeding. ***')\n",
    "else:\n",
    "    print('\\nNo blocking confounds. Proceeding.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speaker-Disjoint Splits\n",
    "\n",
    "**Obrigatório:** nenhum speaker aparece em mais de um split.  \n",
    "Splits estratificados por sotaque para representação em todos os splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_info = generate_speaker_disjoint_splits(\n",
    "    combined_entries,\n",
    "    train_ratio=config['splits']['ratios']['train'],\n",
    "    val_ratio=config['splits']['ratios']['val'],\n",
    "    test_ratio=config['splits']['ratios']['test'],\n",
    "    seed=config['splits']['seed'],\n",
    ")\n",
    "\n",
    "# Persist splits\n",
    "split_output_dir = Path(config['splits']['output_dir'])\n",
    "split_path = save_splits(split_info, split_output_dir)\n",
    "print(f'Splits saved: {split_path}')\n",
    "print(f'Train: {len(split_info.train_speakers)} speakers, {split_info.utterances_per_split[\"train\"]:,} utts')\n",
    "print(f'Val:   {len(split_info.val_speakers)} speakers, {split_info.utterances_per_split[\"val\"]:,} utts')\n",
    "print(f'Test:  {len(split_info.test_speakers)} speakers, {split_info.utterances_per_split[\"test\"]:,} utts')\n",
    "\n",
    "# Assign entries to splits\n",
    "split_entries = assign_entries_to_splits(combined_entries, split_info)\n",
    "\n",
    "train_entries = split_entries['train']\n",
    "val_entries = split_entries['val']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Speaker-disjoint verification (HARD FAIL if violated)\n",
    "train_spk = {e.speaker_id for e in train_entries}\n",
    "val_spk = {e.speaker_id for e in val_entries}\n",
    "test_spk = {e.speaker_id for e in test_entries}\n",
    "\n",
    "assert len(train_spk & val_spk) == 0, 'Speaker leakage train -> val'\n",
    "assert len(train_spk & test_spk) == 0, 'Speaker leakage train -> test'\n",
    "assert len(val_spk & test_spk) == 0, 'Speaker leakage val -> test'\n",
    "print('\\nSpeaker-disjoint verification: PASSED')\n",
    "\n",
    "for split_name, entries_list in split_entries.items():\n",
    "    accent_dist = Counter(e.accent for e in entries_list)\n",
    "    print(f'  {split_name}: {dict(sorted(accent_dist.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Construção do HuggingFace Dataset\n",
    "\n",
    "Converte as entries do manifest em um `datasets.DatasetDict` com:\n",
    "- `Audio()` feature (decode automático, 16kHz)\n",
    "- Metadados: `speaker_id`, `accent`, `gender`, `duration_s`, `source`, `birth_state`, `utt_id`\n",
    "- Splits: `train`, `validation`, `test` (speaker-disjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Audio, Features, Value, ClassLabel\n",
    "\n",
    "# Build ordered label lists for ClassLabel features\n",
    "accent_labels = sorted({e.accent for e in combined_entries})\n",
    "gender_labels = sorted({e.gender for e in combined_entries})\n",
    "source_labels = sorted({e.source for e in combined_entries})\n",
    "\n",
    "print(f'Accent classes: {accent_labels}')\n",
    "print(f'Gender classes: {gender_labels}')\n",
    "print(f'Source classes: {source_labels}')\n",
    "\n",
    "\n",
    "def entries_to_hf_dict(entries: list) -> dict:\n",
    "    \"\"\"Convert ManifestEntry list to dict-of-lists for HF Dataset.\"\"\"\n",
    "    return {\n",
    "        'audio': [e.audio_path for e in entries],\n",
    "        'utt_id': [e.utt_id for e in entries],\n",
    "        'speaker_id': [e.speaker_id for e in entries],\n",
    "        'accent': [e.accent for e in entries],\n",
    "        'gender': [e.gender for e in entries],\n",
    "        'duration_s': [e.duration_s for e in entries],\n",
    "        'source': [e.source for e in entries],\n",
    "        'birth_state': [e.birth_state for e in entries],\n",
    "        'text_id': [e.text_id or '' for e in entries],\n",
    "    }\n",
    "\n",
    "\n",
    "# Define features schema\n",
    "features = Features({\n",
    "    'audio': Audio(sampling_rate=16_000),\n",
    "    'utt_id': Value('string'),\n",
    "    'speaker_id': Value('string'),\n",
    "    'accent': ClassLabel(names=accent_labels),\n",
    "    'gender': ClassLabel(names=gender_labels),\n",
    "    'duration_s': Value('float32'),\n",
    "    'source': ClassLabel(names=source_labels),\n",
    "    'birth_state': Value('string'),\n",
    "    'text_id': Value('string'),\n",
    "})\n",
    "\n",
    "# Build DatasetDict with speaker-disjoint splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict(\n",
    "        entries_to_hf_dict(train_entries),\n",
    "        features=features,\n",
    "    ),\n",
    "    'validation': Dataset.from_dict(\n",
    "        entries_to_hf_dict(val_entries),\n",
    "        features=features,\n",
    "    ),\n",
    "    'test': Dataset.from_dict(\n",
    "        entries_to_hf_dict(test_entries),\n",
    "        features=features,\n",
    "    ),\n",
    "})\n",
    "\n",
    "print(f'\\nDatasetDict criado:')\n",
    "print(dataset_dict)\n",
    "for split_name, ds in dataset_dict.items():\n",
    "    print(f'  {split_name}: {len(ds)} rows, columns={ds.column_names}')\n",
    "\n",
    "# Verify a sample\n",
    "sample = dataset_dict['train'][0]\n",
    "print(f'\\nSample (train[0]):')\n",
    "print(f'  utt_id: {sample[\"utt_id\"]}')\n",
    "print(f'  speaker_id: {sample[\"speaker_id\"]}')\n",
    "print(f'  accent: {sample[\"accent\"]}')\n",
    "print(f'  gender: {sample[\"gender\"]}')\n",
    "print(f'  duration_s: {sample[\"duration_s\"]:.2f}')\n",
    "print(f'  source: {sample[\"source\"]}')\n",
    "print(f'  audio sample_rate: {sample[\"audio\"][\"sampling_rate\"]}')\n",
    "print(f'  audio array shape: {np.array(sample[\"audio\"][\"array\"]).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Publicação no HuggingFace Hub\n",
    "\n",
    "Autentica no HuggingFace, gera dataset card com estatísticas e publica.  \n",
    "\n",
    "**IMPORTANTE:** O token precisa de permissão `write` no HuggingFace Hub.  \n",
    "Gere um token em https://huggingface.co/settings/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login — will prompt for token if not already cached\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build dataset card content ---\n",
    "\n",
    "# Compute statistics for the card\n",
    "total_entries = len(combined_entries)\n",
    "total_speakers_count = len({e.speaker_id for e in combined_entries})\n",
    "total_duration_h = sum(e.duration_s for e in combined_entries) / 3600\n",
    "\n",
    "accent_stats = Counter(e.accent for e in combined_entries)\n",
    "gender_stats = Counter(e.gender for e in combined_entries)\n",
    "source_stats = Counter(e.source for e in combined_entries)\n",
    "\n",
    "# Per-split stats\n",
    "split_stats = {}\n",
    "for name, entries in split_entries.items():\n",
    "    split_stats[name] = {\n",
    "        'utterances': len(entries),\n",
    "        'speakers': len({e.speaker_id for e in entries}),\n",
    "        'duration_h': sum(e.duration_s for e in entries) / 3600,\n",
    "        'accents': dict(sorted(Counter(e.accent for e in entries).items())),\n",
    "    }\n",
    "\n",
    "# Confound summary text\n",
    "confound_lines = []\n",
    "for cs in confound_summary:\n",
    "    status = 'BLOCKING' if cs['is_blocking'] else 'OK'\n",
    "    confound_lines.append(\n",
    "        f\"| {cs['variables']} | {cs['test']} | {cs['statistic']:.4f} | \"\n",
    "        f\"{cs['p_value']:.6f} | {cs['effect_size_name']}={cs['effect_size']:.4f} | {status} |\"\n",
    "    )\n",
    "confound_table = '\\n'.join(confound_lines)\n",
    "\n",
    "# Accent distribution table\n",
    "accent_lines = []\n",
    "for acc in sorted(accent_stats.keys()):\n",
    "    n = accent_stats[acc]\n",
    "    pct = n / total_entries * 100\n",
    "    spk_count = len({e.speaker_id for e in combined_entries if e.accent == acc})\n",
    "    accent_lines.append(f'| {acc} | {n:,} | {pct:.1f}% | {spk_count} |')\n",
    "accent_table = '\\n'.join(accent_lines)\n",
    "\n",
    "# Get commit hash\n",
    "try:\n",
    "    commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], text=True).strip()\n",
    "except Exception:\n",
    "    commit_hash = 'unknown'\n",
    "\n",
    "# Get manifest hash\n",
    "manifest_sha = compute_file_hash(COMBINED_MANIFEST_PATH) if COMBINED_MANIFEST_PATH.exists() else 'N/A'\n",
    "\n",
    "print(f'Total: {total_entries:,} utterances, {total_speakers_count} speakers, {total_duration_h:.1f}h')\n",
    "print(f'Manifest SHA-256: {manifest_sha[:16]}...')\n",
    "print(f'Commit: {commit_hash[:8]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Card (README.md) ---\n",
    "\n",
    "DATASET_CARD = f\"\"\"---\n",
    "language:\n",
    "  - pt\n",
    "license: cc-by-4.0\n",
    "task_categories:\n",
    "  - audio-classification\n",
    "tags:\n",
    "  - accent-classification\n",
    "  - brazilian-portuguese\n",
    "  - speech\n",
    "  - regional-accent\n",
    "  - ibge-macro-regions\n",
    "  - tts-evaluation\n",
    "size_categories:\n",
    "  - 1K<n<10K\n",
    "---\n",
    "\n",
    "# Accents-PT-BR\n",
    "\n",
    "A curated, multi-source dataset of Brazilian Portuguese speech annotated with IBGE macro-region\n",
    "accent labels. Designed for training and evaluating accent classifiers used as external evaluators\n",
    "in accent-controllable TTS research.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Accents-PT-BR** combines two complementary sources of Brazilian Portuguese speech:\n",
    "\n",
    "| Source | Type | Accent label origin |\n",
    "|--------|------|--------------------|\n",
    "| [CORAA-MUPE-ASR](https://huggingface.co/datasets/nilc-nlp/CORAA-MUPE-ASR) | Professional interviews | `birth_state` field (verified) |\n",
    "| [Common Voice PT](https://commonvoice.mozilla.org/pt) (v17.0) | Crowd-sourced read speech | User-submitted `accent` field (noisy) |\n",
    "\n",
    "Accent labels are normalized to **IBGE macro-regions**: N (Norte), NE (Nordeste), CO (Centro-Oeste),\n",
    "SE (Sudeste), S (Sul).\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Speaker-disjoint splits**: No speaker appears in more than one split (train/validation/test).\n",
    "  This is critical for fair evaluation of accent classifiers.\n",
    "- **Source-prefixed IDs**: CORAA-MUPE and Common Voice entries use distinct ID namespaces\n",
    "  (`cv_` prefix for Common Voice) to prevent collisions.\n",
    "- **Multi-source**: Enables cross-source evaluation to detect source confounds\n",
    "  (classifier learning recording conditions instead of accent).\n",
    "- **All audio at 16 kHz mono WAV**.\n",
    "\n",
    "## Dataset Statistics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total utterances | {total_entries:,} |\n",
    "| Total speakers | {total_speakers_count} |\n",
    "| Total duration | {total_duration_h:.1f} hours |\n",
    "| Accent classes | {len(accent_stats)} (IBGE macro-regions) |\n",
    "| Audio format | 16 kHz mono WAV |\n",
    "\n",
    "### Accent Distribution\n",
    "\n",
    "| Region | Utterances | % | Speakers |\n",
    "|--------|-----------|---|----------|\n",
    "{accent_table}\n",
    "\n",
    "### Source Distribution\n",
    "\n",
    "| Source | Utterances |\n",
    "|--------|------------|\n",
    "| CORAA-MUPE | {source_stats.get('CORAA-MUPE', 0):,} |\n",
    "| CommonVoice-PT | {source_stats.get('CommonVoice-PT', 0):,} |\n",
    "\n",
    "### Splits (Speaker-Disjoint)\n",
    "\n",
    "| Split | Utterances | Speakers | Duration |\n",
    "|-------|-----------|----------|----------|\n",
    "| train | {split_stats['train']['utterances']:,} | {split_stats['train']['speakers']} | {split_stats['train']['duration_h']:.1f}h |\n",
    "| validation | {split_stats['validation']['utterances']:,} | {split_stats['validation']['speakers']} | {split_stats['validation']['duration_h']:.1f}h |\n",
    "| test | {split_stats['test']['utterances']:,} | {split_stats['test']['speakers']} | {split_stats['test']['duration_h']:.1f}h |\n",
    "\n",
    "## Confound Analysis\n",
    "\n",
    "Mandatory confound checks were run before publication:\n",
    "\n",
    "| Variables | Test | Statistic | p-value | Effect size | Status |\n",
    "|-----------|------|-----------|---------|-------------|--------|\n",
    "{confound_table}\n",
    "\n",
    "## Dataset Fields\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `audio` | Audio (16kHz) | Audio waveform |\n",
    "| `utt_id` | string | Unique utterance identifier |\n",
    "| `speaker_id` | string | Speaker identifier (unique per person, `cv_` prefix for Common Voice) |\n",
    "| `accent` | ClassLabel | IBGE macro-region: {accent_labels} |\n",
    "| `gender` | ClassLabel | Speaker gender: {gender_labels} |\n",
    "| `duration_s` | float32 | Duration in seconds |\n",
    "| `source` | ClassLabel | Source dataset: {source_labels} |\n",
    "| `birth_state` | string | Original birth state / accent label from source |\n",
    "| `text_id` | string | Transcription ID (if available) |\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"paulohenriquevn/accents-pt-br\")\n",
    "\n",
    "# Access a sample\n",
    "sample = ds['train'][0]\n",
    "print(sample['accent'])      # e.g., 'SE'\n",
    "print(sample['speaker_id'])  # e.g., 'coraa_spk123'\n",
    "print(sample['audio'])       # {{'array': array([...]), 'sampling_rate': 16000}}\n",
    "\n",
    "# Filter by accent\n",
    "nordeste = ds['train'].filter(lambda x: x['accent'] == 'NE')\n",
    "```\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This dataset is designed for:\n",
    "1. **Training accent classifiers** for evaluating accent-controllable TTS systems.\n",
    "2. **Cross-source generalization studies** (train on one source, test on another).\n",
    "3. **Research on Brazilian Portuguese regional accent variation.**\n",
    "\n",
    "It is NOT intended for:\n",
    "- Speaker identification or re-identification.\n",
    "- Commercial voice profiling.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- **Accent as proxy**: IBGE macro-regions are coarse. Intra-regional variation exists.\n",
    "- **Common Voice labels are noisy**: User-submitted, not verified.\n",
    "- **Source confound risk**: Different recording conditions between sources.\n",
    "  Cross-source evaluation is recommended.\n",
    "- **Class imbalance**: Some regions (N, CO) may have fewer speakers.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this dataset, please cite the underlying sources:\n",
    "\n",
    "- **CORAA-MUPE**: Candido Jr. et al. (2023). CORAA: a large corpus of spontaneous and prepared speech.\n",
    "- **Common Voice**: Ardila et al. (2020). Common Voice: A Massively-Multilingual Speech Corpus.\n",
    "\n",
    "## Provenance\n",
    "\n",
    "- **Manifest SHA-256**: `{manifest_sha}`\n",
    "- **Pipeline commit**: `{commit_hash}`\n",
    "- **Build date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Seed**: {SEED}\n",
    "- **Config**: `configs/accent_classifier.yaml`\n",
    "\"\"\"\n",
    "\n",
    "print('Dataset card generated.')\n",
    "print(f'Card length: {len(DATASET_CARD)} chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Publish to HuggingFace Hub ---\n",
    "\n",
    "HF_REPO_ID = 'paulohenriquevn/accents-pt-br'\n",
    "\n",
    "print(f'Publishing to: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "print(f'Splits: {list(dataset_dict.keys())}')\n",
    "print(f'Total rows: {sum(len(ds) for ds in dataset_dict.values()):,}')\n",
    "print()\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    HF_REPO_ID,\n",
    "    private=False,\n",
    ")\n",
    "\n",
    "print(f'\\nDataset uploaded successfully!')\n",
    "print(f'URL: https://huggingface.co/datasets/{HF_REPO_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Upload dataset card ---\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Write card to temp file and upload\n",
    "card_path = Path('/tmp/accents_pt_br_README.md')\n",
    "card_path.write_text(DATASET_CARD, encoding='utf-8')\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=str(card_path),\n",
    "    path_in_repo='README.md',\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "print(f'Dataset card uploaded to {HF_REPO_ID}')\n",
    "print(f'\\n=== PUBLICATION COMPLETE ===')\n",
    "print(f'Dataset: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "print(f'Utterances: {total_entries:,}')\n",
    "print(f'Speakers: {total_speakers_count}')\n",
    "print(f'Duration: {total_duration_h:.1f}h')\n",
    "print(f'Manifest SHA-256: {manifest_sha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification: load back from Hub ---\n",
    "\n",
    "print(f'Verifying: loading {HF_REPO_ID} from Hub...')\n",
    "ds_verify = load_dataset(HF_REPO_ID)\n",
    "\n",
    "print(f'\\nLoaded successfully:')\n",
    "print(ds_verify)\n",
    "\n",
    "for split_name, split_ds in ds_verify.items():\n",
    "    print(f'  {split_name}: {len(split_ds)} rows')\n",
    "\n",
    "# Verify accent distribution matches\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    local_count = len(split_entries[split_name.replace('validation', 'val')])\n",
    "    remote_count = len(ds_verify[split_name])\n",
    "    match = 'OK' if local_count == remote_count else 'MISMATCH'\n",
    "    print(f'  {split_name}: local={local_count}, remote={remote_count} [{match}]')\n",
    "\n",
    "# Verify a sample decodes correctly\n",
    "sample = ds_verify['train'][0]\n",
    "assert sample['audio']['sampling_rate'] == 16000, 'Sampling rate mismatch'\n",
    "assert len(sample['audio']['array']) > 0, 'Empty audio'\n",
    "print(f'\\nSample verification PASSED (sr={sample[\"audio\"][\"sampling_rate\"]}, '\n",
    "      f'len={len(sample[\"audio\"][\"array\"])})')\n",
    "\n",
    "print(f'\\n=== ALL VERIFICATIONS PASSED ===')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}