{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accents-PT-BR — Dataset Pipeline + HuggingFace Publication\n",
    "\n",
    "**Projeto:** Controle Explícito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Construir o dataset derivado **Accents-PT-BR** (CORAA-MUPE + Common Voice PT),  \n",
    "executar toda a pipeline de validação (confounds, splits) e publicar no HuggingFace Hub.  \n",
    "**Config:** `configs/accent_classifier.yaml` (single source of truth).  \n",
    "\n",
    "**Seções:**\n",
    "1. Setup do ambiente\n",
    "2. Dataset pipeline (CORAA-MUPE + Common Voice + confounds + splits)\n",
    "3. Detailed distribution analysis\n",
    "4. Construção do HuggingFace Dataset\n",
    "5. Publicação no HuggingFace Hub\n",
    "6. Verification\n",
    "\n",
    "Este notebook é a **camada de orquestração**. Toda lógica está em `src/` (testável, auditável).  \n",
    "O notebook apenas: instala deps → configura ambiente → chama módulos → publica resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo: /content/TCC\n",
      "Deps: requirements-data.txt\n",
      "Bootstrap OK\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap: clone repo, install deps, check NumPy ABI.\n",
    "# This code is intentionally INLINE (stdlib-only) because on Colab the repo\n",
    "# does not yet exist when this cell runs. After cloning + pip install, we can\n",
    "# import the ABI check from src/. On first Colab run, this cell may restart\n",
    "# the runtime once (NumPy ABI fix). After restart, re-run — it completes fast.\n",
    "import os, subprocess, sys\n",
    "\n",
    "# Detect platform and set repo directory\n",
    "if os.path.exists(\"/teamspace/studios/this_studio\"):\n",
    "    _REPO_DIR = \"/teamspace/studios/this_studio/TCC\"\n",
    "elif os.environ.get(\"PAPERSPACE\"):\n",
    "    _REPO_DIR = \"/notebooks/TCC\"\n",
    "elif \"google.colab\" in sys.modules or os.path.exists(\"/content\"):\n",
    "    _REPO_DIR = \"/content/TCC\"\n",
    "else:\n",
    "    _REPO_DIR = os.getcwd()\n",
    "\n",
    "# Clone repo if not already present\n",
    "if not os.path.exists(os.path.join(_REPO_DIR, \".git\")):\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/paulohenriquevn/tcc.git\", _REPO_DIR],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "os.chdir(_REPO_DIR)\n",
    "if _REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, _REPO_DIR)\n",
    "\n",
    "# Install data-only deps (lightweight — no torch/speechbrain/qwen-tts needed).\n",
    "# torch/torchaudio are pre-installed on Colab and only used here via\n",
    "# src.utils.seed (torch.manual_seed) and lazy cv_manifest_builder imports.\n",
    "_req = os.path.join(_REPO_DIR, \"requirements-data.txt\")\n",
    "if not os.path.exists(_req):\n",
    "    _req = os.path.join(_REPO_DIR, \"requirements.txt\")\n",
    "if os.path.exists(_req):\n",
    "    _pip = subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", _req, \"-q\"],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if _pip.returncode != 0:\n",
    "        print(\"pip install FAILED. stderr:\")\n",
    "        print(_pip.stderr)\n",
    "        print(\"stdout:\")\n",
    "        print(_pip.stdout)\n",
    "        raise RuntimeError(\n",
    "            f\"pip install -r {os.path.basename(_req)} failed (exit {_pip.returncode}). \"\n",
    "            \"Check the output above for the failing package.\"\n",
    "        )\n",
    "\n",
    "# NumPy ABI check — may restart runtime once on Colab (stale C-extensions)\n",
    "from src.utils.notebook_bootstrap import _check_numpy_abi\n",
    "_check_numpy_abi()\n",
    "\n",
    "print(f\"Repo: {_REPO_DIR}\")\n",
    "print(f\"Deps: {os.path.basename(_req)}\")\n",
    "print(\"Bootstrap OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Platform: colab\n",
      "Config: accent_classifier_ablation\n",
      "Seed: 42\n",
      "Cache: /content/drive/MyDrive/tcc-cache\n"
     ]
    }
   ],
   "source": [
    "import yaml, json, logging\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Platform-aware persistent cache setup\n",
    "from src.utils.platform import detect_platform, setup_environment\n",
    "from src.utils.seed import set_global_seed\n",
    "from src.utils.git import get_commit_hash\n",
    "from src.data.manifest import compute_file_hash\n",
    "\n",
    "# Load config — single source of truth for all experiment parameters\n",
    "with open('configs/accent_classifier.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "platform = detect_platform()\n",
    "setup_environment(platform)\n",
    "\n",
    "SEED = config['seed']['global']\n",
    "generator = set_global_seed(SEED)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Drive cache base directory — platform-aware\n",
    "DRIVE_BASE = platform.cache_base\n",
    "DRIVE_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Platform: {platform.name}')\n",
    "print(f'Config: {config[\"experiment\"][\"name\"]}')\n",
    "print(f'Seed: {SEED}')\n",
    "print(f'Cache: {DRIVE_BASE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace authentication — MUST run before pipeline.\n",
    "# Common Voice data was removed from mozilla-foundation Oct 2025.\n",
    "# We use fsicoli/common_voice_17_0 (CC-0 community mirror, loading script).\n",
    "# CORAA-MUPE is public and does not require auth.\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "login(token=userdata.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: paulohenriquevn\n"
     ]
    }
   ],
   "source": [
    "# Auth check — login already happened in Section 1 (Setup).\n",
    "# This cell just verifies the token is available for push_to_hub.\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "user_info = api.whoami()\n",
    "print(f\"Authenticated as: {user_info[\"name\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: paulohenriquevn\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "info = api.whoami()\n",
    "print(f\"Logged in as: {info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick load test — Common Voice PT via community mirror.\n",
    "# mozilla-foundation/common_voice_17_0 was emptied Oct 2025 (data moved to Mozilla Data Collective).\n",
    "# fsicoli/common_voice_17_0 is a CC-0 mirror with loading script → trust_remote_code=True required.\n",
    "# Mirror uses standard splits (train/validation/test) — no aggregate \"validated\" split.\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "_splits = []\n",
    "for _name in (\"train\", \"validation\", \"test\"):\n",
    "    _s = load_dataset(\"fsicoli/common_voice_17_0\", \"pt\", split=_name, trust_remote_code=True)\n",
    "    print(f\"  {_name}: {len(_s):,}\")\n",
    "    _splits.append(_s)\n",
    "ds = concatenate_datasets(_splits)\n",
    "print(f\"Total validated: {len(ds):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Pipeline (CORAA-MUPE + Common Voice + Confounds + Splits)\n",
    "\n",
    "Executa a pipeline completa via `src.data.pipeline.load_or_build_accents_dataset()`:\n",
    "1. Load/build CORAA-MUPE manifest (cache-aware)\n",
    "2. Load/build Common Voice PT manifest (cache-aware)\n",
    "3. Combine manifests (validação de colisões, consistência speaker→accent)\n",
    "4. Análise de confounds (accent × gender, duration, source)\n",
    "5. Speaker-disjoint splits (verificação automática)\n",
    "\n",
    "A mesma função é usada pelo classifier notebook — DRY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORAA-MUPE: 136,926 entries (cached, SHA: 8f2c42f64914591f...)\n",
      "Loading Common Voice PT from HuggingFace (mozilla-foundation/common_voice_17_0)...\n"
     ]
    },
    {
     "ename": "EmptyDatasetError",
     "evalue": "The directory at hf://datasets/mozilla-foundation/common_voice_17_0@11dc88355e899d1bf2df74f01b904a8544a17b33 doesn't contain any data files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDatasetError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-5966/2018376183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_or_build_accents_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_or_build_accents_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDRIVE_BASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcombined_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/TCC/src/data/pipeline.py\u001b[0m in \u001b[0;36mload_or_build_accents_dataset\u001b[0;34m(config, drive_base)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading Common Voice PT from HuggingFace ({cv_hf_id})...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mcv_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_hf_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Common Voice validated: {len(cv_dataset):,} rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1694\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m                     \u001b[0muse_exported_dataset_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_exported_dataset_infos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1697\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mGatedRepoError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Dataset '{path}' is a gated dataset on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_files\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m             \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         data_files = DataFilesDict.from_patterns(\n\u001b[1;32m   1068\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns\u001b[0;34m(base_path, download_config)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_data_files_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The directory at {base_path} doesn't contain any data files\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmptyDatasetError\u001b[0m: The directory at hf://datasets/mozilla-foundation/common_voice_17_0@11dc88355e899d1bf2df74f01b904a8544a17b33 doesn't contain any data files"
     ]
    }
   ],
   "source": [
    "from src.data.pipeline import load_or_build_accents_dataset\n",
    "\n",
    "bundle = load_or_build_accents_dataset(config, DRIVE_BASE)\n",
    "\n",
    "combined_entries = bundle.combined_entries\n",
    "split_info = bundle.split_info\n",
    "split_entries = bundle.split_entries\n",
    "confound_results = bundle.confound_results\n",
    "combined_sha256 = bundle.combined_sha256\n",
    "\n",
    "train_entries = split_entries['train']\n",
    "val_entries = split_entries['val']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Accent distribution per split\n",
    "for split_name, entries_list in split_entries.items():\n",
    "    accent_dist = Counter(e.accent for e in entries_list)\n",
    "    print(f'  {split_name}: {dict(sorted(accent_dist.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Distribution Analysis\n",
    "\n",
    "Cross-tabulations and source distribution for documentation and dataset card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulations for dataset card\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.gender for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('=== ACCENT x GENDER ===')\n",
    "print(gender_table)\n",
    "\n",
    "source_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.source for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x SOURCE ===')\n",
    "print(source_table)\n",
    "\n",
    "# Source distribution details\n",
    "source_dist = bundle.source_distribution\n",
    "print('\\n=== SOURCE x ACCENT (detail) ===')\n",
    "for src, counts in source_dist['source_x_accent'].items():\n",
    "    print(f'  {src}: {dict(sorted(counts.items()))}')\n",
    "\n",
    "# Confound summary for dataset card\n",
    "confound_summary = [\n",
    "    {\n",
    "        'test': r.test_name,\n",
    "        'variables': f'{r.variable_a} x {r.variable_b}',\n",
    "        'statistic': r.statistic,\n",
    "        'p_value': r.p_value,\n",
    "        'effect_size': r.effect_size,\n",
    "        'effect_size_name': r.effect_size_name,\n",
    "        'is_blocking': r.is_blocking,\n",
    "    }\n",
    "    for r in confound_results\n",
    "]\n",
    "\n",
    "total_speakers = len({e.speaker_id for e in combined_entries})\n",
    "total_entries = len(combined_entries)\n",
    "total_duration_h = sum(e.duration_s for e in combined_entries) / 3600\n",
    "print(f'\\nTotal: {total_entries:,} entries, {total_speakers} speakers, {total_duration_h:.1f}h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construção do HuggingFace Dataset\n",
    "\n",
    "Converte as entries do manifest em um `datasets.DatasetDict` com:\n",
    "- `Audio()` feature (decode automático, 16kHz)\n",
    "- Metadados: `speaker_id`, `accent`, `gender`, `duration_s`, `source`, `birth_state`, `utt_id`\n",
    "- Splits: `train`, `validation`, `test` (speaker-disjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Audio, Features, Value, ClassLabel\n",
    "from src.data.hf_utils import entries_to_hf_dict, to_hf_split_entries\n",
    "\n",
    "# Build ordered label lists for ClassLabel features\n",
    "accent_labels = sorted({e.accent for e in combined_entries})\n",
    "gender_labels = sorted({e.gender for e in combined_entries})\n",
    "source_labels = sorted({e.source for e in combined_entries})\n",
    "\n",
    "print(f'Accent classes: {accent_labels}')\n",
    "print(f'Gender classes: {gender_labels}')\n",
    "print(f'Source classes: {source_labels}')\n",
    "\n",
    "# Define features schema\n",
    "features = Features({\n",
    "    'audio': Audio(sampling_rate=16_000),\n",
    "    'utt_id': Value('string'),\n",
    "    'speaker_id': Value('string'),\n",
    "    'accent': ClassLabel(names=accent_labels),\n",
    "    'gender': ClassLabel(names=gender_labels),\n",
    "    'duration_s': Value('float32'),\n",
    "    'source': ClassLabel(names=source_labels),\n",
    "    'birth_state': Value('string'),\n",
    "    'text_id': Value('string'),\n",
    "})\n",
    "\n",
    "# Convert internal split names ('val') to HuggingFace convention ('validation')\n",
    "hf_split_entries = to_hf_split_entries(split_entries)\n",
    "\n",
    "# Build DatasetDict with speaker-disjoint splits\n",
    "dataset_dict = DatasetDict({\n",
    "    name: Dataset.from_dict(\n",
    "        entries_to_hf_dict(entries),\n",
    "        features=features,\n",
    "    )\n",
    "    for name, entries in hf_split_entries.items()\n",
    "})\n",
    "\n",
    "print(f'\\nDatasetDict criado:')\n",
    "print(dataset_dict)\n",
    "for split_name, ds in dataset_dict.items():\n",
    "    print(f'  {split_name}: {len(ds)} rows, columns={ds.column_names}')\n",
    "\n",
    "# Verify a sample\n",
    "sample = dataset_dict['train'][0]\n",
    "print(f'\\nSample (train[0]):')\n",
    "print(f'  utt_id: {sample[\"utt_id\"]}')\n",
    "print(f'  speaker_id: {sample[\"speaker_id\"]}')\n",
    "print(f'  accent: {sample[\"accent\"]}')\n",
    "print(f'  gender: {sample[\"gender\"]}')\n",
    "print(f'  duration_s: {sample[\"duration_s\"]:.2f}')\n",
    "print(f'  source: {sample[\"source\"]}')\n",
    "print(f'  audio sample_rate: {sample[\"audio\"][\"sampling_rate\"]}')\n",
    "print(f'  audio array shape: {np.array(sample[\"audio\"][\"array\"]).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Publicação no HuggingFace Hub\n",
    "\n",
    "Gera dataset card com estatísticas e publica.  \n",
    "Autenticação já foi feita na Seção 1 (Setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auth check — login already happened in Section 1 (Setup).\n",
    "# This cell just verifies the token is available for push_to_hub.\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "user_info = api.whoami()\n",
    "print(f\"Authenticated as: {user_info[\"name\"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset card using src/data/hf_utils (not inline f-string)\n",
    "from src.data.hf_utils import build_dataset_card, to_hf_split_entries\n",
    "\n",
    "commit_hash = get_commit_hash()\n",
    "\n",
    "# Manifest hash\n",
    "COMBINED_MANIFEST_PATH = DRIVE_BASE / 'accents_pt_br' / 'manifest.jsonl'\n",
    "manifest_sha = compute_file_hash(COMBINED_MANIFEST_PATH) if COMBINED_MANIFEST_PATH.exists() else 'N/A'\n",
    "\n",
    "DATASET_CARD = build_dataset_card(\n",
    "    combined_entries=combined_entries,\n",
    "    split_entries=to_hf_split_entries(split_entries),\n",
    "    confound_summary=confound_summary,\n",
    "    accent_labels=accent_labels,\n",
    "    gender_labels=gender_labels,\n",
    "    source_labels=source_labels,\n",
    "    manifest_sha=manifest_sha,\n",
    "    commit_hash=commit_hash,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f'Dataset card generated ({len(DATASET_CARD)} chars)')\n",
    "print(f'Manifest SHA-256: {manifest_sha[:16]}...')\n",
    "print(f'Commit: {commit_hash[:8]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish dataset to HuggingFace Hub\n",
    "# SAFETY: Set PUBLISH = True manually to upload. Prevents accidental publication on Run All.\n",
    "PUBLISH = False\n",
    "\n",
    "HF_REPO_ID = 'paulohenriquevn/accents-pt-br'\n",
    "\n",
    "print(f'Target: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "print(f'Splits: {list(dataset_dict.keys())}')\n",
    "print(f'Total rows: {sum(len(ds) for ds in dataset_dict.values()):,}')\n",
    "print()\n",
    "\n",
    "if not PUBLISH:\n",
    "    print('PUBLISH = False — skipping upload. Set PUBLISH = True in this cell to publish.')\n",
    "else:\n",
    "    dataset_dict.push_to_hub(\n",
    "        HF_REPO_ID,\n",
    "        private=False,\n",
    "    )\n",
    "    print(f'\\nDataset uploaded successfully!')\n",
    "    print(f'URL: https://huggingface.co/datasets/{HF_REPO_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset card\n",
    "import tempfile\n",
    "\n",
    "if not PUBLISH:\n",
    "    print('PUBLISH = False — skipping dataset card upload.')\n",
    "else:\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:\n",
    "        f.write(DATASET_CARD)\n",
    "        card_path = Path(f.name)\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=str(card_path),\n",
    "        path_in_repo='README.md',\n",
    "        repo_id=HF_REPO_ID,\n",
    "        repo_type='dataset',\n",
    "    )\n",
    "    card_path.unlink()  # Clean up temp file\n",
    "\n",
    "    print(f'Dataset card uploaded to {HF_REPO_ID}')\n",
    "    print(f'\\n=== PUBLICATION COMPLETE ===')\n",
    "    print(f'Dataset: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "    print(f'Utterances: {total_entries:,}')\n",
    "    print(f'Speakers: {total_speakers}')\n",
    "    print(f'Duration: {total_duration_h:.1f}h')\n",
    "    print(f'Manifest SHA-256: {manifest_sha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification\n",
    "\n",
    "Load back from HuggingFace Hub to verify row counts and audio decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.data.hf_utils import INTERNAL_TO_HF_SPLITS\n",
    "\n",
    "if not PUBLISH:\n",
    "    print('PUBLISH = False — skipping Hub verification (dataset not uploaded).')\n",
    "    print('Set PUBLISH = True in Section 5 and re-run to verify after publication.')\n",
    "else:\n",
    "    print(f'Verifying: loading {HF_REPO_ID} from Hub...')\n",
    "    ds_verify = load_dataset(HF_REPO_ID)\n",
    "\n",
    "    print(f'\\nLoaded successfully:')\n",
    "    print(ds_verify)\n",
    "\n",
    "    for split_name_hf, split_ds in ds_verify.items():\n",
    "        print(f'  {split_name_hf}: {len(split_ds)} rows')\n",
    "\n",
    "    # Reverse the internal→HF mapping for verification (HF name → internal name)\n",
    "    hf_to_internal = {v: k for k, v in INTERNAL_TO_HF_SPLITS.items()}\n",
    "\n",
    "    # Verify row counts match\n",
    "    for hf_name, internal_name in hf_to_internal.items():\n",
    "        local_count = len(split_entries[internal_name])\n",
    "        remote_count = len(ds_verify[hf_name])\n",
    "        match = 'OK' if local_count == remote_count else 'MISMATCH'\n",
    "        print(f'  {hf_name}: local={local_count}, remote={remote_count} [{match}]')\n",
    "\n",
    "    # Verify a sample decodes correctly\n",
    "    sample = ds_verify['train'][0]\n",
    "    assert sample['audio']['sampling_rate'] == 16000, 'Sampling rate mismatch'\n",
    "    assert len(sample['audio']['array']) > 0, 'Empty audio'\n",
    "    print(f'\\nSample verification PASSED (sr={sample[\"audio\"][\"sampling_rate\"]}, '\n",
    "          f'len={len(sample[\"audio\"][\"array\"])})')\n",
    "\n",
    "    print(f'\\n=== ALL VERIFICATIONS PASSED ===')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
