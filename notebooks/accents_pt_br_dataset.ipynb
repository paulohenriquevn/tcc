{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accents-PT-BR — Dataset Pipeline + HuggingFace Publication\n",
    "\n",
    "**Projeto:** Controle Explícito de Sotaque Regional em pt-BR  \n",
    "**Objetivo:** Construir o dataset derivado **Accents-PT-BR** (CORAA-MUPE + Common Voice PT),  \n",
    "executar toda a pipeline de validação (confounds, splits) e publicar no HuggingFace Hub.  \n",
    "**Config:** `configs/accent_classifier.yaml` (single source of truth).  \n",
    "\n",
    "**Seções:**\n",
    "1. Setup do ambiente\n",
    "2. Dataset pipeline (CORAA-MUPE + Common Voice + confounds + splits)\n",
    "3. Detailed distribution analysis\n",
    "4. Construção do HuggingFace Dataset\n",
    "5. Publicação no HuggingFace Hub\n",
    "6. Verification\n",
    "\n",
    "Este notebook é a **camada de orquestração**. Toda lógica está em `src/` (testável, auditável).  \n",
    "O notebook apenas: instala deps → configura ambiente → chama módulos → publica resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3877/3688750534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This module uses only stdlib — safe to import before pip install.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# On first Colab run, this cell may restart the runtime once (NumPy ABI fix).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_bootstrap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap: clone repo, install deps, check NumPy ABI.\n",
    "# This module uses only stdlib — safe to import before pip install.\n",
    "# On first Colab run, this cell may restart the runtime once (NumPy ABI fix).\n",
    "from src.utils.notebook_bootstrap import bootstrap\n",
    "bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, json, logging\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Platform-aware persistent cache setup\n",
    "from src.utils.platform import detect_platform, setup_environment\n",
    "from src.utils.seed import set_global_seed\n",
    "from src.utils.git import get_commit_hash\n",
    "from src.data.manifest import compute_file_hash\n",
    "\n",
    "# Load config — single source of truth for all experiment parameters\n",
    "with open('configs/accent_classifier.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "platform = detect_platform()\n",
    "setup_environment(platform)\n",
    "\n",
    "SEED = config['seed']['global']\n",
    "generator = set_global_seed(SEED)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Drive cache base directory — platform-aware\n",
    "DRIVE_BASE = platform.cache_base\n",
    "DRIVE_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Platform: {platform.name}')\n",
    "print(f'Config: {config[\"experiment\"][\"name\"]}')\n",
    "print(f'Seed: {SEED}')\n",
    "print(f'Cache: {DRIVE_BASE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Pipeline (CORAA-MUPE + Common Voice + Confounds + Splits)\n",
    "\n",
    "Executa a pipeline completa via `src.data.pipeline.load_or_build_accents_dataset()`:\n",
    "1. Load/build CORAA-MUPE manifest (cache-aware)\n",
    "2. Load/build Common Voice PT manifest (cache-aware)\n",
    "3. Combine manifests (validação de colisões, consistência speaker→accent)\n",
    "4. Análise de confounds (accent × gender, duration, source)\n",
    "5. Speaker-disjoint splits (verificação automática)\n",
    "\n",
    "A mesma função é usada pelo classifier notebook — DRY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.pipeline import load_or_build_accents_dataset\n",
    "\n",
    "bundle = load_or_build_accents_dataset(config, DRIVE_BASE)\n",
    "\n",
    "combined_entries = bundle.combined_entries\n",
    "split_info = bundle.split_info\n",
    "split_entries = bundle.split_entries\n",
    "confound_results = bundle.confound_results\n",
    "combined_sha256 = bundle.combined_sha256\n",
    "\n",
    "train_entries = split_entries['train']\n",
    "val_entries = split_entries['val']\n",
    "test_entries = split_entries['test']\n",
    "\n",
    "# Accent distribution per split\n",
    "for split_name, entries_list in split_entries.items():\n",
    "    accent_dist = Counter(e.accent for e in entries_list)\n",
    "    print(f'  {split_name}: {dict(sorted(accent_dist.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Distribution Analysis\n",
    "\n",
    "Cross-tabulations and source distribution for documentation and dataset card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulations for dataset card\n",
    "gender_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.gender for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('=== ACCENT x GENDER ===')\n",
    "print(gender_table)\n",
    "\n",
    "source_table = pd.crosstab(\n",
    "    [e.accent for e in combined_entries],\n",
    "    [e.source for e in combined_entries],\n",
    "    margins=True,\n",
    ")\n",
    "print('\\n=== ACCENT x SOURCE ===')\n",
    "print(source_table)\n",
    "\n",
    "# Source distribution details\n",
    "source_dist = bundle.source_distribution\n",
    "print('\\n=== SOURCE x ACCENT (detail) ===')\n",
    "for src, counts in source_dist['source_x_accent'].items():\n",
    "    print(f'  {src}: {dict(sorted(counts.items()))}')\n",
    "\n",
    "# Confound summary for dataset card\n",
    "confound_summary = [\n",
    "    {\n",
    "        'test': r.test_name,\n",
    "        'variables': f'{r.variable_a} x {r.variable_b}',\n",
    "        'statistic': r.statistic,\n",
    "        'p_value': r.p_value,\n",
    "        'effect_size': r.effect_size,\n",
    "        'effect_size_name': r.effect_size_name,\n",
    "        'is_blocking': r.is_blocking,\n",
    "    }\n",
    "    for r in confound_results\n",
    "]\n",
    "\n",
    "total_speakers = len({e.speaker_id for e in combined_entries})\n",
    "total_entries = len(combined_entries)\n",
    "total_duration_h = sum(e.duration_s for e in combined_entries) / 3600\n",
    "print(f'\\nTotal: {total_entries:,} entries, {total_speakers} speakers, {total_duration_h:.1f}h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construção do HuggingFace Dataset\n",
    "\n",
    "Converte as entries do manifest em um `datasets.DatasetDict` com:\n",
    "- `Audio()` feature (decode automático, 16kHz)\n",
    "- Metadados: `speaker_id`, `accent`, `gender`, `duration_s`, `source`, `birth_state`, `utt_id`\n",
    "- Splits: `train`, `validation`, `test` (speaker-disjoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Audio, Features, Value, ClassLabel\n",
    "from src.data.hf_utils import entries_to_hf_dict, to_hf_split_entries\n",
    "\n",
    "# Build ordered label lists for ClassLabel features\n",
    "accent_labels = sorted({e.accent for e in combined_entries})\n",
    "gender_labels = sorted({e.gender for e in combined_entries})\n",
    "source_labels = sorted({e.source for e in combined_entries})\n",
    "\n",
    "print(f'Accent classes: {accent_labels}')\n",
    "print(f'Gender classes: {gender_labels}')\n",
    "print(f'Source classes: {source_labels}')\n",
    "\n",
    "# Define features schema\n",
    "features = Features({\n",
    "    'audio': Audio(sampling_rate=16_000),\n",
    "    'utt_id': Value('string'),\n",
    "    'speaker_id': Value('string'),\n",
    "    'accent': ClassLabel(names=accent_labels),\n",
    "    'gender': ClassLabel(names=gender_labels),\n",
    "    'duration_s': Value('float32'),\n",
    "    'source': ClassLabel(names=source_labels),\n",
    "    'birth_state': Value('string'),\n",
    "    'text_id': Value('string'),\n",
    "})\n",
    "\n",
    "# Convert internal split names ('val') to HuggingFace convention ('validation')\n",
    "hf_split_entries = to_hf_split_entries(split_entries)\n",
    "\n",
    "# Build DatasetDict with speaker-disjoint splits\n",
    "dataset_dict = DatasetDict({\n",
    "    name: Dataset.from_dict(\n",
    "        entries_to_hf_dict(entries),\n",
    "        features=features,\n",
    "    )\n",
    "    for name, entries in hf_split_entries.items()\n",
    "})\n",
    "\n",
    "print(f'\\nDatasetDict criado:')\n",
    "print(dataset_dict)\n",
    "for split_name, ds in dataset_dict.items():\n",
    "    print(f'  {split_name}: {len(ds)} rows, columns={ds.column_names}')\n",
    "\n",
    "# Verify a sample\n",
    "sample = dataset_dict['train'][0]\n",
    "print(f'\\nSample (train[0]):')\n",
    "print(f'  utt_id: {sample[\"utt_id\"]}')\n",
    "print(f'  speaker_id: {sample[\"speaker_id\"]}')\n",
    "print(f'  accent: {sample[\"accent\"]}')\n",
    "print(f'  gender: {sample[\"gender\"]}')\n",
    "print(f'  duration_s: {sample[\"duration_s\"]:.2f}')\n",
    "print(f'  source: {sample[\"source\"]}')\n",
    "print(f'  audio sample_rate: {sample[\"audio\"][\"sampling_rate\"]}')\n",
    "print(f'  audio array shape: {np.array(sample[\"audio\"][\"array\"]).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Publicação no HuggingFace Hub\n",
    "\n",
    "Autentica no HuggingFace, gera dataset card com estatísticas e publica.  \n",
    "\n",
    "**IMPORTANTE:** O token precisa de permissão `write` no HuggingFace Hub.  \n",
    "Gere um token em https://huggingface.co/settings/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login — will prompt for token if not already cached\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset card using src/data/hf_utils (not inline f-string)\n",
    "from src.data.hf_utils import build_dataset_card, to_hf_split_entries\n",
    "\n",
    "commit_hash = get_commit_hash()\n",
    "\n",
    "# Manifest hash\n",
    "COMBINED_MANIFEST_PATH = DRIVE_BASE / 'accents_pt_br' / 'manifest.jsonl'\n",
    "manifest_sha = compute_file_hash(COMBINED_MANIFEST_PATH) if COMBINED_MANIFEST_PATH.exists() else 'N/A'\n",
    "\n",
    "DATASET_CARD = build_dataset_card(\n",
    "    combined_entries=combined_entries,\n",
    "    split_entries=to_hf_split_entries(split_entries),\n",
    "    confound_summary=confound_summary,\n",
    "    accent_labels=accent_labels,\n",
    "    gender_labels=gender_labels,\n",
    "    source_labels=source_labels,\n",
    "    manifest_sha=manifest_sha,\n",
    "    commit_hash=commit_hash,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f'Dataset card generated ({len(DATASET_CARD)} chars)')\n",
    "print(f'Manifest SHA-256: {manifest_sha[:16]}...')\n",
    "print(f'Commit: {commit_hash[:8]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish dataset to HuggingFace Hub\n",
    "# SAFETY: Set PUBLISH = True manually to upload. Prevents accidental publication on Run All.\n",
    "PUBLISH = False\n",
    "\n",
    "HF_REPO_ID = 'paulohenriquevn/accents-pt-br'\n",
    "\n",
    "print(f'Target: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "print(f'Splits: {list(dataset_dict.keys())}')\n",
    "print(f'Total rows: {sum(len(ds) for ds in dataset_dict.values()):,}')\n",
    "print()\n",
    "\n",
    "if not PUBLISH:\n",
    "    print('PUBLISH = False — skipping upload. Set PUBLISH = True in this cell to publish.')\n",
    "else:\n",
    "    dataset_dict.push_to_hub(\n",
    "        HF_REPO_ID,\n",
    "        private=False,\n",
    "    )\n",
    "    print(f'\\nDataset uploaded successfully!')\n",
    "    print(f'URL: https://huggingface.co/datasets/{HF_REPO_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset card\n",
    "import tempfile\n",
    "\n",
    "if not PUBLISH:\n",
    "    print('PUBLISH = False — skipping dataset card upload.')\n",
    "else:\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:\n",
    "        f.write(DATASET_CARD)\n",
    "        card_path = Path(f.name)\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=str(card_path),\n",
    "        path_in_repo='README.md',\n",
    "        repo_id=HF_REPO_ID,\n",
    "        repo_type='dataset',\n",
    "    )\n",
    "    card_path.unlink()  # Clean up temp file\n",
    "\n",
    "    print(f'Dataset card uploaded to {HF_REPO_ID}')\n",
    "    print(f'\\n=== PUBLICATION COMPLETE ===')\n",
    "    print(f'Dataset: https://huggingface.co/datasets/{HF_REPO_ID}')\n",
    "    print(f'Utterances: {total_entries:,}')\n",
    "    print(f'Speakers: {total_speakers}')\n",
    "    print(f'Duration: {total_duration_h:.1f}h')\n",
    "    print(f'Manifest SHA-256: {manifest_sha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification\n",
    "\n",
    "Load back from HuggingFace Hub to verify row counts and audio decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.data.hf_utils import INTERNAL_TO_HF_SPLITS\n",
    "\n",
    "print(f'Verifying: loading {HF_REPO_ID} from Hub...')\n",
    "ds_verify = load_dataset(HF_REPO_ID)\n",
    "\n",
    "print(f'\\nLoaded successfully:')\n",
    "print(ds_verify)\n",
    "\n",
    "for split_name_hf, split_ds in ds_verify.items():\n",
    "    print(f'  {split_name_hf}: {len(split_ds)} rows')\n",
    "\n",
    "# Reverse the internal→HF mapping for verification (HF name → internal name)\n",
    "hf_to_internal = {v: k for k, v in INTERNAL_TO_HF_SPLITS.items()}\n",
    "\n",
    "# Verify row counts match\n",
    "for hf_name, internal_name in hf_to_internal.items():\n",
    "    local_count = len(split_entries[internal_name])\n",
    "    remote_count = len(ds_verify[hf_name])\n",
    "    match = 'OK' if local_count == remote_count else 'MISMATCH'\n",
    "    print(f'  {hf_name}: local={local_count}, remote={remote_count} [{match}]')\n",
    "\n",
    "# Verify a sample decodes correctly\n",
    "sample = ds_verify['train'][0]\n",
    "assert sample['audio']['sampling_rate'] == 16000, 'Sampling rate mismatch'\n",
    "assert len(sample['audio']['array']) > 0, 'Empty audio'\n",
    "print(f'\\nSample verification PASSED (sr={sample[\"audio\"][\"sampling_rate\"]}, '\n",
    "      f'len={len(sample[\"audio\"][\"array\"])})')\n",
    "\n",
    "print(f'\\n=== ALL VERIFICATIONS PASSED ===')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
